{"2025-06-18T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2506.03893v2","updated":"2025-06-18T12:32:18Z","published":"2025-06-04T12:42:36Z","title":"An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the\n  Filter-and-Verification Tree and MapReduce","summary":"  Given two different collections of sets, the exact set similarity R-S Join\nfinds all set pairs with similarity no less than a given threshold, which has\nwidespread applications. While existing algorithms accelerate large-scale R-S\nJoins using a two-stage filter-and-verification framework along with the\nparallel and distributed MapReduce framework, they suffer from excessive\ncandidate set pairs, leading to significant I/O, data transfer, and\nverification overhead, and ultimately degrading the performance. This paper\nproposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate\nfiltering and verification into a single stage through filter-and-verification\ntrees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT\n(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that\ncompresses elements and associated sets in memory, enabling single-stage\nprocessing that eliminates the candidate set generation, fast lookups, and\nreduced database scans. Correctness proofs are provided. Second, CF-RS-Join\nwith LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,\nwhich compresses non-branching paths into single nodes and stores them in\nlinear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and\nMR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce\nfor parallel processing. Empirical studies on 7 real-world datasets have been\nconducted to evaluate the performance of the proposed algorithms against\nselected existing algorithms in terms of execution time, scalability, memory\nusage, and disk usage. Experimental results demonstrate that our algorithm\nusing MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.\n","authors":["Yuhong Feng","Fangcao Jian","Yixuan Cao","Xiaobin Jian","Jia Wang","Haiyue Feng","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2506.03893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00031v4","updated":"2025-06-18T08:03:27Z","published":"2025-01-23T15:11:30Z","title":"GNN-based Anchor Embedding for Efficient Exact Subgraph Matching","summary":"  Subgraph matching query is a fundamental problem in graph data management and\nhas a variety of real-world applications. Several recent works utilize deep\nlearning (DL) techniques to process subgraph matching queries. Most of them\nfind approximate subgraph matching results without accuracy guarantees. Unlike\nthese DL-based inexact subgraph matching methods, we propose a learning-based\nexact subgraph matching framework, called \\textit{graph neural network\n(GNN)-based anchor embedding framework} (GNN-AE). In contrast to traditional\nexact subgraph matching methods that rely on creating auxiliary summary\nstructures online for each specific query, our method indexes small feature\nsubgraphs in the data graph offline and uses GNNs to perform graph isomorphism\ntests for these indexed feature subgraphs to efficiently obtain high-quality\ncandidates. To make a tradeoff between query efficiency and index storage cost,\nwe use two types of feature subgraphs, namely anchored subgraphs and anchored\npaths. Based on the proposed techniques, we transform the exact subgraph\nmatching problem into a search problem in the embedding space. Furthermore, to\nefficiently retrieve all matches, we develop a parallel matching growth\nalgorithm and design a cost-based DFS query planning method to further improve\nthe matching growth algorithm. Extensive experiments on 6 real-world and 3\nsynthetic datasets indicate that GNN-AE is more efficient than the baselines,\nespecially outperforming the exploration-based baseline methods by up to 1--2\norders of magnitude.\n","authors":["Bin Yang","Zhaonian Zou","Jianxiong Ye"],"pdf_url":"https://arxiv.org/pdf/2502.00031v4.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2506.15626v1","updated":"2025-06-18T16:56:44Z","published":"2025-06-18T16:56:44Z","title":"Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction","summary":"  $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.\n","authors":["Vincent Roca","Marc Tommasi","Paul Andrey","Aurélien Bellet","Markus D. Schirmer","Hilde Henon","Laurent Puy","Julien Ramon","Grégory Kuchcinski","Martin Bretzner","Renaud Lopes"],"pdf_url":"https://arxiv.org/pdf/2506.15626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15595v1","updated":"2025-06-18T16:10:17Z","published":"2025-06-18T16:10:17Z","title":"LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale\n  Heterogeneous Clusters","summary":"  Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.\n","authors":["Kunming Zhang","Hanlong Liao","Guoming Tang"],"pdf_url":"https://arxiv.org/pdf/2506.15595v1.pdf","comment":"12 pages, 19 figures,7 tables"},{"id":"http://arxiv.org/abs/2506.13998v2","updated":"2025-06-18T15:16:15Z","published":"2025-06-16T20:55:30Z","title":"DAGs for the Masses","summary":"  A recent approach to building consensus protocols on top of Directed Acyclic\nGraphs (DAGs) shows much promise due to its simplicity and stable throughput.\nHowever, as each node in the DAG typically includes a linear number of\nreferences to the nodes in the previous round, prior DAG protocols only scale\nup to a certain point when the overhead of maintaining the graph becomes the\nbottleneck.\n  To enable large-scale deployments of DAG-based protocols, we propose a sparse\nDAG architecture, where each node includes only a constant number of references\nto random nodes in the previous round. We present a sparse version of Bullshark\n-- one of the most prominent DAG-based consensus protocols -- and demonstrate\nits improved scalability.\n  Remarkably, unlike other protocols that use random sampling to reduce\ncommunication complexity, we manage to avoid sacrificing resilience: the\nprotocol can tolerate up to $f<n/3$ Byzantine faults (where $n$ is the number\nof participants), same as its less scalable deterministic counterpart. The\nproposed ``sparse'' methodology can be applied to any protocol that maintains\ndisseminated system updates and causal relations between them in a graph-like\nstructure. Our simulations show that the considerable reduction of transmitted\nmetadata in sparse DAGs results in more efficient network utilization and\nbetter scalability.\n","authors":["Michael Anoprenko","Andrei Tonkikh","Alexander Spiegelman","Petr Kuznetsov"],"pdf_url":"https://arxiv.org/pdf/2506.13998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15537v1","updated":"2025-06-18T15:12:47Z","published":"2025-06-18T15:12:47Z","title":"Automatic Metadata Capture and Processing for High-Performance Workflows","summary":"  Modern workflows run on increasingly heterogeneous computing architectures\nand with this heterogeneity comes additional complexity. We aim to apply the\nFAIR principles for research reproducibility by developing software to collect\nmetadata annotations for workflows run on HPC systems. We experiment with two\npossible formats to uniformly store these metadata, and reorganize the\ncollected metadata to be as easy to use as possible for researchers studying\ntheir workflow performance.\n","authors":["Polina Shpilker","Line Pouchard"],"pdf_url":"https://arxiv.org/pdf/2506.15537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15488v1","updated":"2025-06-18T14:26:55Z","published":"2025-06-18T14:26:55Z","title":"Minimizing Communication for Parallel Symmetric Tensor Times Same Vector\n  Computation","summary":"  In this article, we focus on the parallel communication cost of multiplying\nthe same vector along two modes of a $3$-dimensional symmetric tensor. This is\na key computation in the higher-order power method for determining eigenpairs\nof a $3$-dimensional symmetric tensor and in gradient-based methods for\ncomputing a symmetric CP decomposition. We establish communication lower bounds\nthat determine how much data movement is required to perform the specified\ncomputation in parallel. The core idea of the proof relies on extending a key\ngeometric inequality for $3$-dimensional symmetric computations. We demonstrate\nthat the communication lower bounds are tight by presenting an optimal\nalgorithm where the data distribution is a natural extension of the triangle\nblock partition scheme for symmetric matrices to 3-dimensional symmetric\ntensors.\n","authors":["Hussam Al Daas","Grey Ballard","Laura Grigori","Suraj Kumar","Kathryn Rouse","Mathieu Vérité"],"pdf_url":"https://arxiv.org/pdf/2506.15488v1.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.15461v1","updated":"2025-06-18T13:48:33Z","published":"2025-06-18T13:48:33Z","title":"All is Not Lost: LLM Recovery without Checkpoints","summary":"  Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.\n","authors":["Nikolay Blagoev","Oğuzhan Ersoy","Lydia Yiyu Chen"],"pdf_url":"https://arxiv.org/pdf/2506.15461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15454v1","updated":"2025-06-18T13:35:43Z","published":"2025-06-18T13:35:43Z","title":"Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP,\n  and CUDA","summary":"  This paper presents a comprehensive comparison of three dominant parallel\nprogramming models in High Performance Computing (HPC): Message Passing\nInterface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device\nArchitecture (CUDA). Selecting optimal programming approaches for modern\nheterogeneous HPC architectures has become increasingly critical. We\nsystematically analyze these models across multiple dimensions: architectural\nfoundations, performance characteristics, domain-specific suitability,\nprogramming complexity, and recent advancements. We examine each model's\nstrengths, weaknesses, and optimization techniques. Our investigation\ndemonstrates that MPI excels in distributed memory environments with\nnear-linear scalability for communication-intensive applications, but faces\ncommunication overhead challenges. OpenMP provides strong performance and\nusability in shared-memory systems and loop-centric tasks, though it is limited\nby shared memory contention. CUDA offers substantial performance gains for\ndata-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires\nspecialized expertise. Performance evaluations across scientific simulations,\nmachine learning, and data analytics reveal that hybrid approaches combining\ntwo or more models often yield optimal results in heterogeneous environments.\nThe paper also discusses implementation challenges, optimization best\npractices, and emerging trends such as performance portability frameworks,\ntask-based programming, and the convergence of HPC and Big Data. This research\nhelps developers and researchers make informed decisions when selecting\nprogramming models for modern HPC applications, emphasizing that the best\nchoice depends on application requirements, hardware, and development\nconstraints.\n","authors":["Nizar ALHafez","Ahmad Kurdi"],"pdf_url":"https://arxiv.org/pdf/2506.15454v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2506.15437v1","updated":"2025-06-18T13:12:34Z","published":"2025-06-18T13:12:34Z","title":"Exploring Fast Fourier Transforms on the Tenstorrent Wormhole","summary":"  Whilst numerous areas of computing have adopted the RISC-V Instruction Set\nArchitecture (ISA) wholesale in recent years, it is yet to become widespread in\nHPC. RISC-V accelerators offer a compelling option where the HPC community can\nbenefit from the specialisation offered by the open nature of the standard but\nwithout the extensive ecosystem changes required when adopting RISC-V CPUs. In\nthis paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)\nalgorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon\nTenstorrent's Tensix architecture, this technology decouples the movement of\ndata from compute, potentially offering increased control to the programmer.\nExploring different optimisation techniques to address the bottlenecks inherent\nin data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is\nslower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around\n8 times less power and consumes around 2.8 times less energy than the CPU when\ncomputing the Fourier transform.\n","authors":["Nick Brown","Jake Davies","Felix LeClair"],"pdf_url":"https://arxiv.org/pdf/2506.15437v1.pdf","comment":"Author accepted version of paper submitted to RISC-V for HPC ISC\n  workshop 2025"},{"id":"http://arxiv.org/abs/2505.12976v2","updated":"2025-06-18T12:54:07Z","published":"2025-05-19T11:15:03Z","title":"Computing the Schulze Method for Large-Scale Preference Data Sets","summary":"  The Schulze method is a voting rule widely used in practice and enjoys many\npositive axiomatic properties. While it is computable in polynomial time, its\nstraight-forward implementation does not scale well for large elections. In\nthis paper, we develop a highly optimised algorithm for computing the Schulze\nmethod with Pregel, a framework for massively parallel computation of graph\nproblems, and demonstrate its applicability for large preference data sets. In\naddition, our theoretic analysis shows that the Schulze method is indeed\nparticularly well-suited for parallel computation, in stark contrast to the\nrelated ranked pairs method. More precisely we show that winner determination\nsubject to the Schulze method is NL-complete, whereas this problem is\nP-complete for the ranked pairs method.\n","authors":["Theresa Csar","Martin Lackner","Reinhard Pichler"],"pdf_url":"https://arxiv.org/pdf/2505.12976v2.pdf","comment":"This is an updated version of the original 2018 IJCAI conference\n  publication. It corrects the P-completeness proof for the ranked pairs method"},{"id":"http://arxiv.org/abs/2506.15418v1","updated":"2025-06-18T12:37:30Z","published":"2025-06-18T12:37:30Z","title":"RISC-V for HPC: An update of where we are and main action points","summary":"  This extended abstract is submitted on behalf of the RISC-V HPC SIG who have\nbeen undertaking an analysis to explore the current state and limitations of\nthe RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has\nbeen great progress made in recent years, we also highlight limitations and\nwhere effort should be focussed.\n","authors":["Nick Brown"],"pdf_url":"https://arxiv.org/pdf/2506.15418v1.pdf","comment":"Extended abstract accepted to the RISC-V Summit Europe 2025"},{"id":"http://arxiv.org/abs/2506.03893v2","updated":"2025-06-18T12:32:18Z","published":"2025-06-04T12:42:36Z","title":"An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the\n  Filter-and-Verification Tree and MapReduce","summary":"  Given two different collections of sets, the exact set similarity R-S Join\nfinds all set pairs with similarity no less than a given threshold, which has\nwidespread applications. While existing algorithms accelerate large-scale R-S\nJoins using a two-stage filter-and-verification framework along with the\nparallel and distributed MapReduce framework, they suffer from excessive\ncandidate set pairs, leading to significant I/O, data transfer, and\nverification overhead, and ultimately degrading the performance. This paper\nproposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate\nfiltering and verification into a single stage through filter-and-verification\ntrees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT\n(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that\ncompresses elements and associated sets in memory, enabling single-stage\nprocessing that eliminates the candidate set generation, fast lookups, and\nreduced database scans. Correctness proofs are provided. Second, CF-RS-Join\nwith LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,\nwhich compresses non-branching paths into single nodes and stores them in\nlinear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and\nMR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce\nfor parallel processing. Empirical studies on 7 real-world datasets have been\nconducted to evaluate the performance of the proposed algorithms against\nselected existing algorithms in terms of execution time, scalability, memory\nusage, and disk usage. Experimental results demonstrate that our algorithm\nusing MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.\n","authors":["Yuhong Feng","Fangcao Jian","Yixuan Cao","Xiaobin Jian","Jia Wang","Haiyue Feng","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2506.03893v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13123v2","updated":"2025-06-18T12:15:24Z","published":"2024-07-18T03:18:59Z","title":"Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint\n  Phase-shift Optimization and Multi-User Power Allocation","summary":"  Vehicular edge computing (VEC) is an emerging technology with significant\npotential in the field of internet of vehicles (IoV), enabling vehicles to\nperform intensive computational tasks locally or offload them to nearby edge\ndevices. However, the quality of communication links may be severely\ndeteriorated due to obstacles such as buildings, impeding the offloading\nprocess. To address this challenge, we introduce the use of Reconfigurable\nIntelligent Surfaces (RIS), which provide alternative communication pathways to\nassist vehicular communication. By dynamically adjusting the phase-shift of the\nRIS, the performance of VEC systems can be substantially improved. In this\nwork, we consider a RIS-assisted VEC system, and design an optimal scheme for\nlocal execution power, offloading power, and RIS phase-shift, where random task\narrivals and channel variations are taken into account. To address the scheme,\nwe propose an innovative deep reinforcement learning (DRL) framework that\ncombines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing\nRIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm for optimizing the power allocation of vehicle user\n(VU). Simulation results show that our proposed scheme outperforms the\ntraditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient\n(TD3) and some typical stochastic schemes.\n","authors":["Kangwei Qi","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.13123v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at\n  https://github.com/qiongwu86/DDPG-RIS-MADDPG-POWER. arXiv admin note: text\n  overlap with arXiv:2406.11318"},{"id":"http://arxiv.org/abs/2506.12708v2","updated":"2025-06-18T10:04:59Z","published":"2025-06-15T03:41:34Z","title":"Serving Large Language Models on Huawei CloudMatrix384","summary":"  The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.\n","authors":["Pengfei Zuo","Huimin Lin","Junbo Deng","Nan Zou","Xingkun Yang","Yingyu Diao","Weifeng Gao","Ke Xu","Zhangyu Chen","Shirui Lu","Zhao Qiu","Peiyang Li","Xianyu Chang","Zhengzhong Yu","Fangzheng Miao","Jia Zheng","Ying Li","Yuan Feng","Bei Wang","Zaijian Zong","Mosong Zhou","Wenli Zhou","Houjiang Chen","Xingyu Liao","Yipeng Li","Wenxiao Zhang","Ping Zhu","Yinggang Wang","Chuanjie Xiao","Depeng Liang","Dong Cao","Juncheng Liu","Yongqiang Yang","Xiaolong Bai","Yi Li","Huaguo Xie","Huatao Wu","Zhibin Yu","Lv Chen","Hu Liu","Yujun Ding","Haipei Zhu","Jing Xia","Yi Xiong","Zhou Yu","Heng Liao"],"pdf_url":"https://arxiv.org/pdf/2506.12708v2.pdf","comment":"59 pages, 24 figures"},{"id":"http://arxiv.org/abs/2506.15264v1","updated":"2025-06-18T08:40:49Z","published":"2025-06-18T08:40:49Z","title":"Centroid Approximation for Byzantine-Tolerant Federated Learning","summary":"  Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.\n","authors":["Mélanie Cambus","Darya Melnyk","Tijana Milentijević","Stefan Schmid"],"pdf_url":"https://arxiv.org/pdf/2506.15264v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.15155v1","updated":"2025-06-18T05:56:01Z","published":"2025-06-18T05:56:01Z","title":"eLLM: Elastic Memory Management Framework for Efficient LLM Serving","summary":"  Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.\n","authors":["Jiale Xu","Rui Zhang","Yi Xiong","Cong Guo","Zihan Liu","Yangjie Zhou","Weiming Hu","Hao Wu","Changxu Shao","Ziqing Wang","Yongjie Yuan","Junping Zhao","Minyi Guo","Jingwen Leng"],"pdf_url":"https://arxiv.org/pdf/2506.15155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15114v1","updated":"2025-06-18T03:33:47Z","published":"2025-06-18T03:33:47Z","title":"Parallel Data Object Creation: Towards Scalable Metadata Management in\n  High-Performance I/O Library","summary":"  High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by\nlarge-scale scientific applications to perform I/O tasks in parallel. These I/O\nlibraries store the metadata such as data types and dimensionality along with\nthe raw data in the same files. While these libraries are well-optimized for\nconcurrent access to the raw data, they are designed neither to handle a large\nnumber of data objects efficiently nor to create different data objects\nindependently by multiple processes, as they require applications to call data\nobject creation APIs collectively with consistent metadata among all processes.\nApplications that process data gathered from remote sensors, such as particle\ncollision experiments in high-energy physics, may generate data of different\nsizes from different sensors and desire to store them as separate data objects.\nFor such applications, the I/O library's requirement on collective data object\ncreation can become very expensive, as the cost of metadata consistency check\nincreases with the metadata volume as well as the number of processes. To\naddress this limitation, using PnetCDF as an experimental platform, we\ninvestigate solutions in this paper that abide the netCDF file format, as well\nas propose a new file header format that enables independent data object\ncreation. The proposed file header consists of two sections, an index table and\na list of metadata blocks. The index table contains the reference to the\nmetadata blocks and each block stores metadata of objects that can be created\ncollectively or independently. The new design achieves a scalable performance,\ncutting data object creation times by up to 582x when running on 4096 MPI\nprocesses to create 5,684,800 data objects in parallel. Additionally, the new\nmethod reduces the memory footprints, with each process requiring an amount of\nmemory space inversely proportional to the number of processes.\n","authors":["Youjia Li","Robert Latham","Robert Ross","Ankit Agrawal","Alok Choudhary","Wei-Keng Liao"],"pdf_url":"https://arxiv.org/pdf/2506.15114v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2506.15684v1","updated":"2025-06-18T17:59:59Z","published":"2025-06-18T17:59:59Z","title":"Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D\n  Rewards","summary":"  Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.\n","authors":["Qingming Liu","Zhen Liu","Dinghuai Zhang","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2506.15684v1.pdf","comment":"Technical Report (21 pages, 21 figures)"},{"id":"http://arxiv.org/abs/2506.15680v1","updated":"2025-06-18T17:59:38Z","published":"2025-06-18T17:59:38Z","title":"Particle-Grid Neural Dynamics for Learning Deformable Object Models from\n  RGB-D Videos","summary":"  Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .\n","authors":["Kaifeng Zhang","Baoyu Li","Kris Hauser","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2506.15680v1.pdf","comment":"Project page: https://kywind.github.io/pgnd"},{"id":"http://arxiv.org/abs/2506.15679v1","updated":"2025-06-18T17:59:35Z","published":"2025-06-18T17:59:35Z","title":"Dense SAE Latents Are Features, Not Bugs","summary":"  Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.\n","authors":["Xiaoqing Sun","Alessandro Stolfo","Joshua Engels","Ben Wu","Senthooran Rajamanoharan","Mrinmaya Sachan","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2506.15679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15665v1","updated":"2025-06-18T17:42:45Z","published":"2025-06-18T17:42:45Z","title":"A Data-Integrated Framework for Learning Fractional-Order Nonlinear\n  Dynamical Systems","summary":"  This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.\n","authors":["Bahram Yaghooti","Chengyu Li","Bruno Sinopoli"],"pdf_url":"https://arxiv.org/pdf/2506.15665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15660v1","updated":"2025-06-18T17:39:03Z","published":"2025-06-18T17:39:03Z","title":"On the Upper Bounds for the Matrix Spectral Norm","summary":"  We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.\n","authors":["Alexey Naumov","Maxim Rakhuba","Denis Ryapolov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2506.15660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15654v1","updated":"2025-06-18T17:31:26Z","published":"2025-06-18T17:31:26Z","title":"CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy\n  Optimization","summary":"  Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.\n","authors":["Ranting Hu"],"pdf_url":"https://arxiv.org/pdf/2506.15654v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2506.15651v1","updated":"2025-06-18T17:29:19Z","published":"2025-06-18T17:29:19Z","title":"AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards\n  Improve Preference Learning","summary":"  Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.\n","authors":["Tevin Wang","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2506.15651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15649v1","updated":"2025-06-18T17:23:36Z","published":"2025-06-18T17:23:36Z","title":"Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment\n  for Fast and Faithful VLM Captioning","summary":"  Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.\n","authors":["Ankan Deria","Adinath Madhavrao Dukre","Feilong Tang","Sara Atito","Sudipta Roy","Muhammad Awais","Muhammad Haris Khan","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2506.15649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15648v1","updated":"2025-06-18T17:18:23Z","published":"2025-06-18T17:18:23Z","title":"deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through\n  Fuzzing LLM-Augmented Harnesses","summary":"  Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.\n","authors":["Georgios Androutsopoulos","Antonio Bianchi"],"pdf_url":"https://arxiv.org/pdf/2506.15648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15643v1","updated":"2025-06-18T17:13:53Z","published":"2025-06-18T17:13:53Z","title":"Revisiting Randomization in Greedy Model Search","summary":"  Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.\n","authors":["Xin Chen","Jason M. Klusowski","Yan Shuo Tan","Chang Yu"],"pdf_url":"https://arxiv.org/pdf/2506.15643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14036v2","updated":"2025-06-18T17:08:53Z","published":"2025-06-16T22:20:44Z","title":"Robust Physics-Informed Neural Network Approach for Estimating\n  Heterogeneous Elastic Properties from Noisy Displacement Data","summary":"  Accurately estimating spatially heterogeneous elasticity parameters,\nparticularly Young's modulus and Poisson's ratio, from noisy displacement\nmeasurements remains significantly challenging in inverse elasticity problems.\nExisting inverse estimation techniques are often limited by instability,\npronounced sensitivity to measurement noise, and difficulty in recovering\nabsolute-scale Young's modulus. This work presents a novel Inverse Elasticity\nPhysics-Informed Neural Network (IE-PINN) specifically designed to robustly\nreconstruct heterogeneous distributions of elasticity parameters from noisy\ndisplacement data based on linear elasticity physics. IE-PINN integrates three\ndistinct neural network architectures dedicated to separately modeling\ndisplacement fields, strain fields, and elasticity distributions, thereby\nsignificantly enhancing stability and accuracy against measurement noise.\nAdditionally, a two-phase estimation strategy is introduced: the first phase\nrecovers relative spatial distributions of Young's modulus and Poisson's ratio,\nand the second phase calibrates the absolute scale of Young's modulus using\nimposed loading boundary conditions. Additional methodological innovations,\nincluding positional encoding, sine activation functions, and a sequential\npretraining protocol, further enhance the model's performance and robustness.\nExtensive numerical experiments demonstrate that IE-PINN effectively overcomes\ncritical limitations encountered by existing methods, delivering accurate\nabsolute-scale elasticity estimations even under severe noise conditions. This\nadvancement holds substantial potential for clinical imaging diagnostics and\nmechanical characterization, where measurements typically encounter substantial\nnoise.\n","authors":["Tatthapong Srikitrungruang","Matthew Lemon","Sina Aghaee Dabaghan Fard","Jaesung Lee","Yuxiao Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.14036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21794v2","updated":"2025-06-18T17:03:35Z","published":"2024-07-31T17:59:58Z","title":"Generalized Out-of-Distribution Detection and Beyond in Vision Language\n  Model Era: A Survey","summary":"  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of these fields in\nthe VLM era. Our framework reveals that, with some field inactivity and\nintegration, the demanding challenges have become OOD detection and AD. Then,\nwe highlight the significant shift in the definition, problem settings, and\nbenchmarks; we thus feature a comprehensive review of the methodology for OOD\ndetection and related tasks to clarify their relationship to OOD detection.\nFinally, we explore the advancements in the emerging Large Vision Language\nModel (LVLM) era, such as GPT-4V. We conclude with open challenges and future\ndirections. The resource is available at\nhttps://github.com/AtsuMiyai/Awesome-OOD-VLM.\n","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Yueqian Lin","Qing Yu","Go Irie","Shafiq Joty","Yixuan Li","Hai Li","Ziwei Liu","Toshihiko Yamasaki","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2407.21794v2.pdf","comment":"Accepted at TMLR2025. Survey paper. We welcome questions, issues, and\n  paper requests via https://github.com/AtsuMiyai/Awesome-OOD-VLM"},{"id":"http://arxiv.org/abs/2506.15626v1","updated":"2025-06-18T16:56:44Z","published":"2025-06-18T16:56:44Z","title":"Federated Learning for MRI-based BrainAGE: a multicenter study on\n  post-stroke functional outcome prediction","summary":"  $\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.\n","authors":["Vincent Roca","Marc Tommasi","Paul Andrey","Aurélien Bellet","Markus D. Schirmer","Hilde Henon","Laurent Puy","Julien Ramon","Grégory Kuchcinski","Martin Bretzner","Renaud Lopes"],"pdf_url":"https://arxiv.org/pdf/2506.15626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15620v1","updated":"2025-06-18T16:51:26Z","published":"2025-06-18T16:51:26Z","title":"GFLC: Graph-based Fairness-aware Label Correction for Fair\n  Classification","summary":"  Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.\n","authors":["Modar Sulaiman","Kallol Roy"],"pdf_url":"https://arxiv.org/pdf/2506.15620v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.15617v1","updated":"2025-06-18T16:50:34Z","published":"2025-06-18T16:50:34Z","title":"The Compositional Architecture of Regret in Large Language Models","summary":"  Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.\n","authors":["Xiangxiang Cui","Shu Yang","Tianjin Huang","Wanyu Lin","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15617v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.04840v2","updated":"2025-06-18T16:50:32Z","published":"2025-02-07T11:18:04Z","title":"Coherent Local Explanations for Mathematical Optimization","summary":"  The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.\n","authors":["Daan Otto","Jannis Kurtz","S. Ilker Birbil"],"pdf_url":"https://arxiv.org/pdf/2502.04840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09033v2","updated":"2025-06-18T16:49:26Z","published":"2025-06-10T17:56:45Z","title":"Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning","summary":"  The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To facilitate learning, we employ a lightweight\nrule-based reward comprising format rewards, final outcome rewards, and a novel\ncost reward for optimizing the balance between performance and cost, opening a\npathway toward enhancing performance-cost trade-offs via RL. Router-R1 also\nconditions only on simple model descriptors such as pricing, latency, and\nexample performance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.\n","authors":["Haozhen Zhang","Tao Feng","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2506.09033v2.pdf","comment":"Code is available at https://github.com/ulab-uiuc/Router-R1. Models\n  and Datasets are available at\n  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03"},{"id":"http://arxiv.org/abs/2409.07448v4","updated":"2025-06-18T16:35:21Z","published":"2024-09-11T17:52:37Z","title":"A Novel Perturb-ability Score to Mitigate Evasion Adversarial Attacks on\n  Flow-Based ML-NIDS","summary":"  As network security threats evolve, safeguarding flow-based Machine Learning\n(ML)-based Network Intrusion Detection Systems (NIDS) from evasion adversarial\nattacks is crucial. This paper introduces the notion of feature perturb-ability\nand presents a novel Perturb-ability Score (PS), which quantifies how\nsusceptible NIDS features are to manipulation in the problem-space by an\nattacker. PS thereby identifies features structurally resistant to evasion\nattacks in flow-based ML-NIDS due to the semantics of network traffic fields,\nas these features are constrained by domain-specific limitations and\ncorrelations. Consequently, attempts to manipulate such features would likely\neither compromise the attack's malicious functionality, render the traffic\ninvalid for processing, or potentially both outcomes simultaneously.\n  We introduce and demonstrate the effectiveness of our PS-enabled defenses,\nPS-guided feature selection and PS-guided feature masking, in enhancing\nflow-based NIDS resilience. Experimental results across various ML-based NIDS\nmodels and public datasets show that discarding or masking highly manipulatable\nfeatures (high-PS features) can maintain solid detection performance while\nsignificantly reducing vulnerability to evasion adversarial attacks. Our\nfindings confirm that PS effectively identifies flow-based NIDS features\nsusceptible to problem-space perturbations. This novel approach leverages\nproblem-space NIDS domain constraints as lightweight universal defense\nmechanisms against evasion adversarial attacks targeting flow-based ML-NIDS.\n","authors":["Mohamed elShehaby","Ashraf Matrawy"],"pdf_url":"https://arxiv.org/pdf/2409.07448v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15606v1","updated":"2025-06-18T16:30:02Z","published":"2025-06-18T16:30:02Z","title":"LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning","summary":"  Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.\n","authors":["Gabrel J. Perin","Runjin Chen","Xuxi Chen","Nina S. T. Hirata","Zhangyang Wang","Junyuan Hong"],"pdf_url":"https://arxiv.org/pdf/2506.15606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17244v4","updated":"2025-06-18T16:28:29Z","published":"2025-02-24T15:21:02Z","title":"A dataset of high-resolution plantar pressures for gait analysis across\n  varying footwear and walking speeds","summary":"  Gait refers to the patterns of limb movement generated during walking, which\nare unique to each individual due to both physical and behavioral traits.\nWalking patterns have been widely studied in biometrics, biomechanics, sports,\nand rehabilitation. While traditional methods rely on video and motion capture,\nadvances in plantar pressure sensing technology now offer deeper insights into\ngait. However, underfoot pressures during walking remain underexplored due to\nthe lack of large, publicly accessible datasets. To address this, we introduce\nthe UNB StepUP-P150 dataset: a footStep database for gait analysis and\nrecognition using Underfoot Pressure, including data from 150 individuals. This\ndataset comprises high-resolution plantar pressure data (4 sensors per\ncm-squared) collected using a 1.2m by 3.6m pressure-sensing walkway. It\ncontains over 200,000 footsteps from participants walking with various speeds\n(preferred, slow-to-stop, fast, and slow) and footwear conditions (barefoot,\nstandard shoes, and two personal shoes), supporting advancements in biometric\ngait recognition and presenting new research opportunities in biomechanics and\ndeep learning. UNB StepUP-P150 establishes a new benchmark for plantar\npressure-based gait analysis and recognition.\n","authors":["Robyn Larracy","Angkoon Phinyomark","Ala Salehi","Eve MacDonald","Saeed Kazemi","Shikder Shafiul Bashar","Aaron Tabor","Erik Scheme"],"pdf_url":"https://arxiv.org/pdf/2502.17244v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01481v3","updated":"2025-06-18T16:21:42Z","published":"2025-05-02T15:58:38Z","title":"VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on\n  Synthetic Video Understanding","summary":"  Synthetic video generation has gained significant attention for its realism\nand broad applications, but remains prone to violations of common sense and\nphysical laws. This highlights the need for reliable abnormality detectors that\nunderstand such principles and are robust to hallucinations. To address this,\nwe introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from\nsynthetic videos generated by models like Veo2, Sora, and Kling, paired with\nexpert-crafted counterintuitive QA to evaluate the critical thinking abilities\nof Multi-modal Large Language Models (MLLMs) on abnormalities that are\nperceptually obvious to humans but often hallucinated due to language priors.\nVideoHallu evaluates MLLMs' abnormality detection abilities with examples\nacross alignment, consistency, commonsense, and physics. We benchmark SOTA\nMLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and\nVideoChat-R1. We observe that these models perform well on many real-world\nbenchmarks like MVBench and MovieChat, but still struggle with basic\nphysics-based and commonsense reasoning in synthetic videos. We further show\nthat post-training with Group Relative Policy Optimization (GRPO), using\ncurriculum learning on datasets combining video QA with counterintuitive\ncommonsense and physics reasoning over real and synthetic videos, improves\nMLLMs' abnormality detection and critical thinking, demonstrating the value of\ntargeted training for improving their understanding of commonsense and physical\nlaws. Our code is available at https://github.com/zli12321/VideoHallu.git.\n","authors":["Zongxia Li","Xiyang Wu","Guangyao Shi","Yubin Qin","Hongyang Du","Tianyi Zhou","Dinesh Manocha","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2505.01481v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12529v2","updated":"2025-06-18T16:11:36Z","published":"2025-02-18T04:32:52Z","title":"Alternating Regret for Online Convex Optimization","summary":"  Motivated by alternating learning dynamics in two-player games, a recent work\nby Cevher et al.(2024) shows that $o(\\sqrt{T})$ alternating regret is possible\nfor any $T$-round adversarial Online Linear Optimization (OLO) problem, and\nleft as an open question whether the same is true for general Online Convex\nOptimization (OCO). We answer this question in the affirmative by showing that\nthe continuous Hedge algorithm achieves\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}T^{\\frac{1}{3}})$ alternating regret for\nany adversarial $d$-dimensional OCO problems. We show that this implies an\nalternating learning dynamic that finds a Nash equilibrium for any\nconvex-concave zero-sum games or a coarse correlated equilibrium for any convex\ntwo-player general-sum games at a rate of\n$\\tilde{\\mathcal{O}}(d^{\\frac{2}{3}}/T^{\\frac{2}{3}})$. To further improve the\ntime complexity and/or the dimension dependence, we propose another simple\nalgorithm, Follow-the-Regularized-Leader with a regularizer whose convex\nconjugate is 3rd-order smooth, for OCO with smooth and self-concordant loss\nfunctions (such as linear or quadratic losses). We instantiate our algorithm\nwith different regularizers and show that, for example, when the decision set\nis the $\\ell_2$ ball, our algorithm achieves\n$\\tilde{\\mathcal{O}}(T^{\\frac{2}{5}})$ alternating regret with no dimension\ndependence (and a better $\\tilde{\\mathcal{O}}(T^{\\frac{1}{3}})$ bound for\nquadratic losses). We complement our results by showing some algorithm-specific\nalternating regret lower bounds, including a somewhat surprising\n$\\Omega(\\sqrt{T})$ lower bound for a Regret Matching variant that is widely\nused in alternating learning dynamics.\n","authors":["Soumita Hait","Ping Li","Haipeng Luo","Mengxiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12529v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15594v1","updated":"2025-06-18T16:09:18Z","published":"2025-06-18T16:09:18Z","title":"WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and\n  Charts","summary":"  Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.\n","authors":["Negar Foroutan","Angelika Romanou","Matin Ansaripour","Julian Martin Eisenschlos","Karl Aberer","Rémi Lebret"],"pdf_url":"https://arxiv.org/pdf/2506.15594v1.pdf","comment":"ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2506.15588v1","updated":"2025-06-18T16:05:09Z","published":"2025-06-18T16:05:09Z","title":"Memory-Efficient Differentially Private Training with Gradient Random\n  Projection","summary":"  Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.\n","authors":["Alex Mulrooney","Devansh Gupta","James Flemings","Huanyu Zhang","Murali Annavaram","Meisam Razaviyayn","Xinwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13678v2","updated":"2025-06-18T16:04:08Z","published":"2025-06-16T16:32:51Z","title":"A Gravity-informed Spatiotemporal Transformer for Human Activity\n  Intensity Prediction","summary":"  Human activity intensity prediction is a crucial to many location-based\nservices. Although tremendous progress has been made to model dynamic\nspatiotemporal patterns of human activity, most existing methods, including\nspatiotemporal graph neural networks (ST-GNNs), overlook physical constraints\nof spatial interactions and the over-smoothing phenomenon in spatial\ncorrelation modeling. To address these limitations, this work proposes a\nphysics-informed deep learning framework, namely Gravity-informed\nSpatiotemporal Transformer (Gravityformer) by refining transformer attention to\nintegrate the universal law of gravitation and explicitly incorporating\nconstraints from spatial interactions. Specifically, it (1) estimates two\nspatially explicit mass parameters based on inflow and outflow, (2) models the\nlikelihood of cross-unit interaction using closed-form solutions of spatial\ninteractions to constrain spatial modeling randomness, and (3) utilizes the\nlearned spatial interaction to guide and mitigate the over-smoothing phenomenon\nin transformer attention matrices. The underlying law of human activity can be\nexplicitly modeled by the proposed adaptive gravity model. Moreover, a parallel\nspatiotemporal graph convolution transformer structure is proposed for\nachieving a balance between coupled spatial and temporal learning. Systematic\nexperiments on six real-world large-scale activity datasets demonstrate the\nquantitative and qualitative superiority of our approach over state-of-the-art\nbenchmarks. Additionally, the learned gravity attention matrix can be\ndisentangled and interpreted based on geographical laws. This work provides a\nnovel insight into integrating physical laws with deep learning for\nspatiotemporal predictive learning.\n","authors":["Yi Wang","Zhenghong Wang","Fan Zhang","Chengling Tang","Chaogui Kang","Di Zhu","Zhongfu Ma","Sijie Ruan","Weiyu Zhang","Yu Zheng","Philip S. Yu","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13678v2.pdf","comment":"18 pages, 13 figures, under review"},{"id":"http://arxiv.org/abs/2506.01324v2","updated":"2025-06-18T15:49:32Z","published":"2025-06-02T05:10:40Z","title":"Near-Optimal Clustering in Mixture of Markov Chains","summary":"  We study the problem of clustering $T$ trajectories of length $H$, each\ngenerated by one of $K$ unknown ergodic Markov chains over a finite state space\nof size $S$. The goal is to accurately group trajectories according to their\nunderlying generative model. We begin by deriving an instance-dependent,\nhigh-probability lower bound on the clustering error rate, governed by the\nweighted KL divergence between the transition kernels of the chains. We then\npresent a novel two-stage clustering algorithm. In Stage~I, we apply spectral\nclustering using a new injective Euclidean embedding for ergodic Markov chains\n-- a contribution of independent interest that enables sharp concentration\nresults. Stage~II refines the initial clusters via a single step of\nlikelihood-based reassignment. Our method achieves a near-optimal clustering\nerror with high probability, under the conditions $H =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} (S^2 \\vee \\pi_{\\min}^{-1}))$ and $TH =\n\\tilde{\\Omega}(\\gamma_{\\mathrm{ps}}^{-1} S^2 )$, where $\\pi_{\\min}$ is the\nminimum stationary probability of a state across the $K$ chains and\n$\\gamma_{\\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements\nprovide significant improvements, if not at least comparable, to the\nstate-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm\noffers a key practical advantage: unlike existing approach, it requires no\nprior knowledge of model-specific quantities (e.g., separation between kernels\nor visitation probabilities). We conclude by discussing the inherent gap\nbetween our upper and lower bounds, providing insights into the unique\nstructure of this clustering problem.\n","authors":["Junghyun Lee","Yassir Jedra","Alexandre Proutière","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2506.01324v2.pdf","comment":"36 pages. Minor corrections in v2"},{"id":"http://arxiv.org/abs/2506.15571v1","updated":"2025-06-18T15:48:30Z","published":"2025-06-18T15:48:30Z","title":"MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh\n  Smoothing","summary":"  Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.\n","authors":["Le Vu Anh","Nguyen Viet Anh","Mehmet Dik","Tu Nguyen Thi Ngoc"],"pdf_url":"https://arxiv.org/pdf/2506.15571v1.pdf","comment":"9 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.15567v1","updated":"2025-06-18T15:43:10Z","published":"2025-06-18T15:43:10Z","title":"Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents","summary":"  Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.\n","authors":["Aline Dobrovsky","Konstantin Schekotihin","Christian Burmer"],"pdf_url":"https://arxiv.org/pdf/2506.15567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15566v1","updated":"2025-06-18T15:43:08Z","published":"2025-06-18T15:43:08Z","title":"Task-Agnostic Experts Composition for Continual Learning","summary":"  Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.\n","authors":["Luigi Quarantiello","Andrea Cossu","Vincenzo Lomonaco"],"pdf_url":"https://arxiv.org/pdf/2506.15566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03074v3","updated":"2025-06-18T15:42:12Z","published":"2025-06-03T16:52:24Z","title":"GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for\n  Generalized Low-Rank Trace Regression","summary":"  We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized\nlow-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it\nemploys a two-stage approach: nuclear norm regularization followed by matrix\nCatoni estimation. We establish state-of-the-art estimation error bounds,\nsurpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and\nreveal a novel experimental design objective, $\\mathrm{GL}(\\pi)$. The key\ntechnical challenge is controlling bias from the nonlinear inverse link\nfunction, which we address by our two-stage approach. We prove a *local*\nminimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise\noptimality up to the condition number of the ground-truth Hessian. Applications\ninclude generalized linear matrix completion, where `GL-LowPopArt` achieves a\nstate-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a\nnovel setting inspired by general preference learning (Zhang et al., 2024). Our\nanalysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,\npotentially interesting problem-dependent quantity, along with improved Borda\nregret bound than vectorization (Wu et al., 2024).\n","authors":["Junghyun Lee","Kyoungseok Jang","Kwang-Sung Jun","Milan Vojnović","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2506.03074v3.pdf","comment":"53 pages, 2 figures, 3 tables; Accepted as a Spotlight Poster to the\n  42nd International Conference on Machine Learning (ICML 2025). Minor\n  correction to the arXiv title in v2 ;). Added ToC in v3"},{"id":"http://arxiv.org/abs/2505.12992v3","updated":"2025-06-18T15:41:14Z","published":"2025-05-19T11:30:41Z","title":"Fractured Chain-of-Thought Reasoning","summary":"  Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning. Code is available at\nhttps://github.com/BaohaoLiao/frac-cot.\n","authors":["Baohao Liao","Hanze Dong","Yuhui Xu","Doyen Sahoo","Christof Monz","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.12992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15559v1","updated":"2025-06-18T15:34:41Z","published":"2025-06-18T15:34:41Z","title":"Towards Explainable Indoor Localization: Interpreting Neural Network\n  Learning on Wi-Fi Fingerprints Using Logic Gates","summary":"  Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.\n","authors":["Danish Gufran","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2506.15559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13023v4","updated":"2025-06-18T15:33:09Z","published":"2024-06-18T19:30:46Z","title":"$k$-Submodular Interdiction Problems under Distributional\n  Risk-Receptiveness and Robustness: Application to Machine Learning","summary":"  We study submodular optimization in adversarial context, applicable to\nmachine learning problems such as feature selection using data susceptible to\nuncertainties and attacks. We focus on Stackelberg games between an attacker\n(or interdictor) and a defender where the attacker aims to minimize the\ndefender's objective of maximizing a $k$-submodular function. We allow\nuncertainties arising from the success of attacks and inherent data noise, and\naddress challenges due to incomplete knowledge of the probability distribution\nof random parameters. Specifically, we introduce Distributionally Robust\n$k$-Submodular Interdiction Problem (DRO $k$-SIP) and Distributionally\nRisk-Receptive $k$-Submodular Interdiction Problem (DRR $k$-SIP) along with\nfinitely convergent exact algorithms for solving them. When solving the DRO\n$k$-SIP, the attacker optimizes their expected payoff with respect to the\nworst-case probability distribution within the ambiguity set, and thereby have\nrobust attack strategies despite distributional ambiguity. In contrast, the DRR\n$k$-SIP identifies attacker strategies with the best-case probability\ndistribution, and identifies critical vulnerabilities for the defender. The\noptimal values derived from both DRO $k$-SIP and DRR $k$-SIP offer a confidence\ninterval-like range for the expected value of the defender's objective\nfunction, capturing distributional ambiguity. We conduct computational\nexperiments on instances of feature selection and sensor placement problems,\nusing Wisconsin breast cancer data and synthetic data, respectively.\n","authors":["Seonghun Park","Manish Bansal"],"pdf_url":"https://arxiv.org/pdf/2406.13023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15554v1","updated":"2025-06-18T15:27:40Z","published":"2025-06-18T15:27:40Z","title":"DAILOC: Domain-Incremental Learning for Indoor Localization using\n  Smartphones","summary":"  Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.\n","authors":["Akhil Singampalli","Danish Gufran","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2506.15554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15544v1","updated":"2025-06-18T15:17:21Z","published":"2025-06-18T15:17:21Z","title":"Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning","summary":"  Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.\n","authors":["Roger Creus Castanyer","Johan Obando-Ceron","Lu Li","Pierre-Luc Bacon","Glen Berseth","Aaron Courville","Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2506.15544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15543v1","updated":"2025-06-18T15:17:03Z","published":"2025-06-18T15:17:03Z","title":"Learning Algorithms in the Limit","summary":"  This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.\n","authors":["Hristo Papazov","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2506.15543v1.pdf","comment":"Accepted at COLT 2025. This version matches the proceedings version"},{"id":"http://arxiv.org/abs/2506.15538v1","updated":"2025-06-18T15:13:07Z","published":"2025-06-18T15:13:07Z","title":"Capturing Polysemanticity with PRISM: A Multi-Concept Feature\n  Description Framework","summary":"  Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).\n","authors":["Laura Kopf","Nils Feldhus","Kirill Bykov","Philine Lou Bommer","Anna Hedström","Marina M. -C. Höhne","Oliver Eberle"],"pdf_url":"https://arxiv.org/pdf/2506.15538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15535v1","updated":"2025-06-18T15:10:38Z","published":"2025-06-18T15:10:38Z","title":"A Simplified Analysis of SGD for Linear Regression with Weight Averaging","summary":"  Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.\n","authors":["Alexandru Meterez","Depen Morwani","Costin-Andrei Oncescu","Jingfeng Wu","Cengiz Pehlevan","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2506.15535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13045v2","updated":"2025-06-18T15:06:34Z","published":"2025-06-16T02:27:25Z","title":"A Comprehensive Survey on Continual Learning in Generative Models","summary":"  The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.\n","authors":["Haiyang Guo","Fanhu Zeng","Fei Zhu","Jiayi Wang","Xukai Wang","Jingang Zhou","Hongbo Zhao","Wenzhuo Liu","Shijie Ma","Da-Han Wang","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2506.13045v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2506.15530v1","updated":"2025-06-18T15:01:25Z","published":"2025-06-18T15:01:25Z","title":"Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music\n  Diffusion Models","summary":"  Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.\n","authors":["Teysir Baoueb","Xiaoyu Bie","Xi Wang","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2506.15530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11759v5","updated":"2025-06-18T14:58:26Z","published":"2024-10-15T16:28:55Z","title":"LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery","summary":"  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing\nsample-efficient ANM methods often rely on restrictive assumptions on the data\ngenerating process, limiting their applicability to real-world settings. We\npropose local search in additive noise models, LoSAM, a topological ordering\nmethod for learning a unique DAG in ANMs with mixed causal mechanisms and\ngeneral noise distributions. We introduce new causal substructures and criteria\nfor identifying roots and leaves, enabling efficient top-down learning. We\nprove asymptotic consistency and polynomial runtime, ensuring scalability and\nsample efficiency. We test LoSAM on synthetic and real-world data,\ndemonstrating state-of-the-art performance across all mixed mechanism settings.\n","authors":["Sujai Hiremath","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2410.11759v5.pdf","comment":"To appear at the Forty-First Annual Conference on Uncertainty in\n  Artificial Intelligence (UAI 2025)"},{"id":"http://arxiv.org/abs/2407.15621v3","updated":"2025-06-18T14:52:47Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Online Retrieval-augmented Generation for Radiology Question\n  Answering","summary":"  Large language models (LLMs) often generate outdated or inaccurate\ninformation based on static training datasets. Retrieval-augmented generation\n(RAG) mitigates this by integrating outside data sources. While previous RAG\nsystems used pre-assembled, fixed databases with limited flexibility, we have\ndeveloped Radiology RAG (RadioRAG), an end-to-end framework that retrieves data\nfrom authoritative radiologic online sources in real-time. We evaluate the\ndiagnostic accuracy of various LLMs when answering radiology-specific questions\nwith and without access to additional online information via RAG. Using 80\nquestions from the RSNA Case Collection across radiologic subspecialties and 24\nadditional expert-curated questions with reference standard answers, LLMs\n(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were\nprompted with and without RadioRAG in a zero-shot inference scenario RadioRAG\nretrieved context-specific information from Radiopaedia in real-time. Accuracy\nwas investigated. Statistical analyses were performed using bootstrapping. The\nresults were further compared with human performance. RadioRAG improved\ndiagnostic accuracy across most LLMs, with relative accuracy increases ranging\nup to 54% for different LLMs. It matched or exceeded non-RAG models and the\nhuman radiologist in question answering across radiologic subspecialties,\nparticularly in breast imaging and emergency radiology. However, the degree of\nimprovement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1\nsaw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,\nhighlighting variability in RadioRAG's effectiveness. LLMs benefit when\nprovided access to domain-specific data beyond their training data. RadioRAG\nshows potential to improve LLM accuracy and factuality in radiology question\nanswering by integrating real-time domain-specific data.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Lisa Adams","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v3.pdf","comment":"Published in Radiology: Artificial Intelligence"},{"id":"http://arxiv.org/abs/2205.14651v3","updated":"2025-06-18T14:49:00Z","published":"2022-05-29T13:14:53Z","title":"Contributions to Representation Learning with Graph Autoencoders and\n  Applications to Music Recommendation","summary":"  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as\ntwo powerful groups of unsupervised node embedding methods, with various\napplications to graph-based machine learning problems such as link prediction\nand community detection. Nonetheless, at the beginning of this Ph.D. project,\nGAE and VGAE models were also suffering from key limitations, preventing them\nfrom being adopted in the industry. In this thesis, we present several\ncontributions to improve these models, with the general aim of facilitating\ntheir use to address industrial-level problems involving graph representations.\nFirstly, we propose two strategies to overcome the scalability issues of\nprevious GAE and VGAE models, permitting to effectively train these models on\nlarge graphs with millions of nodes and edges. These strategies leverage graph\ndegeneracy and stochastic subgraph decoding techniques, respectively. Besides,\nwe introduce Gravity-Inspired GAE and VGAE, providing the first extensions of\nthese models for directed graphs, that are ubiquitous in industrial\napplications. We also consider extensions of GAE and VGAE models for dynamic\ngraphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily\ncomplex, and we propose to simplify them by leveraging linear encoders. Lastly,\nwe introduce Modularity-Aware GAE and VGAE to improve community detection on\ngraphs, while jointly preserving good performances on link prediction. In the\nlast part of this thesis, we evaluate our methods on several graphs extracted\nfrom the music streaming service Deezer. We put the emphasis on graph-based\nmusic recommendation problems. In particular, we show that our methods can\nimprove the detection of communities of similar musical items to recommend to\nusers, that they can effectively rank similar artists in a cold start setting,\nand that they permit modeling the music genre perception across cultures.\n","authors":["Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2205.14651v3.pdf","comment":"Ph.D. thesis defended at \\'Ecole Polytechnique (IPP) in March 2022.\n  As mentioned in this thesis, several chapters present results also published\n  in scientific articles written with co-authors"},{"id":"http://arxiv.org/abs/2506.15513v1","updated":"2025-06-18T14:48:19Z","published":"2025-06-18T14:48:19Z","title":"RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented\n  Generation","summary":"  Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.\n","authors":["Le Vu Anh","Nguyen Viet Anh","Mehmet Dik","Luong Van Nghia"],"pdf_url":"https://arxiv.org/pdf/2506.15513v1.pdf","comment":"11 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.05929v6","updated":"2025-06-18T14:45:27Z","published":"2024-09-09T10:40:50Z","title":"M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the\n  Joint-Embedding Predictive Architecture","summary":"  Current multimodal learning strategies primarily optimize in the original\ntoken space. Such a framework is easy to incorporate with the backbone of\npretrained language model, but might result in modality collapse. To alleviate\nsuch issues, we leverage the Joint-Embedding Predictive Architecture (JEPA) on\nthe multimodal tasks, which converts the input embedding into the output\nembedding space by a predictor and then conducts the cross-modal alignment on\nthe latent space. We implement this predictor by a Multi-Gate Mixture of\nExperts (MMoE) and name the framework as M3-JEPA, accordingly. The gating\nfunction disentangles the modality-specific and shared information and derives\ninformation-theoretic optimality. The framework is implemented with both\ncontrastive and regularization loss, and solved by alternative gradient descent\n(AGD) between different multimodal tasks. By thoroughly designed experiments,\nwe show that M3-JEPA can obtain state-of-the-art performance on different\nmodalities and tasks, generalize to unseen datasets and domains, and is\ncomputationally efficient in both training and inference. Our observation\nsuggests that M3-JEPA might become a new basis to self-supervised learning in\nthe open world.\n","authors":["Hongyang Lei","Xiaolong Cheng","Qi Qin","Dan Wang","Kun Fan","Huazhen Huang","Qingqing Gu","Yetao Wu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.05929v6.pdf","comment":"16 pages, 5 figures. ICML 2025"},{"id":"http://arxiv.org/abs/2506.15507v1","updated":"2025-06-18T14:45:06Z","published":"2025-06-18T14:45:06Z","title":"Over-squashing in Spatiotemporal Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.\n","authors":["Ivan Marisca","Jacob Bamberger","Cesare Alippi","Michael M. Bronstein"],"pdf_url":"https://arxiv.org/pdf/2506.15507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13912v2","updated":"2025-06-18T14:44:41Z","published":"2025-03-18T05:16:36Z","title":"KANITE: Kolmogorov-Arnold Networks for ITE estimation","summary":"  We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)\nfor Individual Treatment Effect (ITE) estimation under multiple treatments\nsetting in causal inference. By utilizing KAN's unique abilities to learn\nunivariate activation functions as opposed to learning linear weights by\nMulti-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE\nframework comprises two key architectures: 1.Integral Probability Metric (IPM)\narchitecture: This employs an IPM loss in a specialized manner to effectively\nalign towards ITE estimation across multiple treatments. 2. Entropy Balancing\n(EB) architecture: This uses weights for samples that are learned by optimizing\nentropy subject to balancing the covariates across treatment groups. Extensive\nevaluations on benchmark datasets demonstrate that KANITE outperforms\nstate-of-the-art algorithms in both $\\epsilon_{\\text{PEHE}}$ and\n$\\epsilon_{\\text{ATE}}$ metrics. Our experiments highlight the advantages of\nKANITE in achieving improved causal estimates, emphasizing the potential of\nKANs to advance causal inference methodologies across diverse application\nareas.\n","authors":["Eshan Mehendale","Abhinav Thorat","Ravi Kolla","Niranjan Pedanekar"],"pdf_url":"https://arxiv.org/pdf/2503.13912v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.15506v1","updated":"2025-06-18T14:43:26Z","published":"2025-06-18T14:43:26Z","title":"Insights on Adversarial Attacks for Tabular Machine Learning via a\n  Systematic Literature Review","summary":"  Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.\n","authors":["Salijona Dyrmishi","Mohamed Djilani","Thibault Simonetto","Salah Ghamizi","Maxime Cordy"],"pdf_url":"https://arxiv.org/pdf/2506.15506v1.pdf","comment":"This paper is currently under review at ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2506.15505v1","updated":"2025-06-18T14:43:04Z","published":"2025-06-18T14:43:04Z","title":"Time-dependent density estimation using binary classifiers","summary":"  We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.\n","authors":["Agnimitra Dasgupta","Javier Murgoitio-Esandi","Ali Fardisi","Assad A Oberai"],"pdf_url":"https://arxiv.org/pdf/2506.15505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15504v1","updated":"2025-06-18T14:42:34Z","published":"2025-06-18T14:42:34Z","title":"Enhancing Hyperbole and Metaphor Detection with Their Bidirectional\n  Dynamic Interaction and Emotion Knowledge","summary":"  Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.\n","authors":["Li Zheng","Sihang Wang","Hao Fei","Zuquan Peng","Fei Li","Jianming Fu","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2506.15504v1.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2410.17161v3","updated":"2025-06-18T14:42:07Z","published":"2024-10-22T16:34:36Z","title":"Interchangeable Token Embeddings for Extendable Vocabulary and\n  Alpha-Equivalence","summary":"  Language models lack the notion of interchangeable tokens: symbols that are\nsemantically equivalent yet distinct, such as bound variables in formal logic.\nThis limitation prevents generalization to larger vocabularies and hinders the\nmodel's ability to recognize alpha-equivalence, where renaming bound variables\npreserves meaning. We formalize this machine learning problem and introduce\nalpha-covariance, a metric for evaluating robustness to such transformations.\nTo tackle this task, we propose a dual-part token embedding strategy: a shared\ncomponent ensures semantic consistency, while a randomized component maintains\ntoken distinguishability. Compared to a baseline that relies on alpha-renaming\nfor data augmentation, our approach demonstrates improved generalization to\nunseen tokens in linear temporal logic solving, propositional logic assignment\nprediction, and copying with an extendable vocabulary, while introducing a\nfavorable inductive bias for alpha-equivalence. Our findings establish a\nfoundation for designing language models that can learn interchangeable token\nrepresentations, a crucial step toward more flexible and systematic reasoning\nin formal domains. Our code and project page are available at\nhttps://necrashter.github.io/interchangeable-token-embeddings\n","authors":["İlker Işık","Ramazan Gokberk Cinbis","Ebru Aydin Gol"],"pdf_url":"https://arxiv.org/pdf/2410.17161v3.pdf","comment":"ICML 2025 Poster Paper, Camera Ready Version"},{"id":"http://arxiv.org/abs/2506.15499v1","updated":"2025-06-18T14:41:24Z","published":"2025-06-18T14:41:24Z","title":"Pixel-level Certified Explanations via Randomized Smoothing","summary":"  Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.\n","authors":["Alaa Anani","Tobias Lorenz","Mario Fritz","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2506.15499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03727v2","updated":"2025-06-18T14:38:47Z","published":"2025-01-07T12:16:26Z","title":"Detecting Neurocognitive Disorders through Analyses of Topic Evolution\n  and Cross-modal Consistency in Visual-Stimulated Narratives","summary":"  Early detection of neurocognitive disorders (NCDs) is crucial for timely\nintervention and disease management. Given that language impairments manifest\nearly in NCD progression, visual-stimulated narrative (VSN)-based analysis\noffers a promising avenue for NCD detection. Current VSN-based NCD detection\nmethods primarily focus on linguistic microstructures (e.g., pauses, lexical\ndiversity), which are potentially linked to bottom-up (stimulus-driven)\ncognitive processing. While these features illuminate basic language abilities,\nthe higher-order linguistic macrostructures (e.g., thematic or logical\ndevelopment), which may reflect top-down (concept-driven) cognitive abilities,\nremain underexplored. These patterns are crucial for NCD detection yet\nchallenging to quantify due to their abstract and complex nature. To bridge\nthis gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic\nTopic Model (DTM) to track topic evolution over time, and (2) Text-Image\nTemporal Alignment Network (TITAN) to measure cross-modal consistency between\nspeech and visual stimuli. Experimental results validated the efficiency of\nproposed approaches in NCD detection, with TITAN achieving superior performance\nboth on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 =\n0.8889). The feature contribution analysis revealed that macrostructural\nfeatures (e.g., topic variability, topic change rate, and topic consistency)\nconstituted the most significant contributors in the model's decision pathways,\noutperforming investigated microstructural features. These findings underscore\nthe critical role of macrostructural patterns in understanding cognitive\nimpairment mechanisms in NCDs.\n","authors":["Jinchao Li","Yuejiao Wang","Junan Li","Jiawen Kang","Bo Zheng","Simon Wong","Brian Mak","Helene Fung","Jean Woo","Man-Wai Mak","Timothy Kwok","Vincent Mok","Xianmin Gong","Xixin Wu","Xunying Liu","Patrick Wong","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2501.03727v2.pdf","comment":"13 pages, 7 figures, submitted to JSTSP"},{"id":"http://arxiv.org/abs/2506.15498v1","updated":"2025-06-18T14:37:59Z","published":"2025-06-18T14:37:59Z","title":"SPARE: Single-Pass Annotation with Reference-Guided Evaluation for\n  Automatic Process Supervision and Reward Modelling","summary":"  Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.\n","authors":["Md Imbesat Hassan Rizvi","Xiaodan Zhu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2506.15498v1.pdf","comment":"8 pages main content, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.15492v1","updated":"2025-06-18T14:30:04Z","published":"2025-06-18T14:30:04Z","title":"LIT-LVM: Structured Regularization for Interaction Terms in Linear\n  Predictors using Latent Variable Models","summary":"  Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.\n","authors":["Mohammadreza Nemati","Zhipeng Huang","Kevin S. Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13061v2","updated":"2025-06-18T14:18:09Z","published":"2025-06-16T03:09:25Z","title":"Fast Convergence for High-Order ODE Solvers in Diffusion Probabilistic\n  Models","summary":"  Diffusion probabilistic models generate samples by learning to reverse a\nnoise-injection process that transforms data into noise. Reformulating this\nreverse process as a deterministic probability flow ordinary differential\nequation (ODE) enables efficient sampling using high-order solvers, often\nrequiring only $\\mathcal{O}(10)$ steps. Since the score function is typically\napproximated by a neural network, analyzing the interaction between its\nregularity, approximation error, and numerical integration error is key to\nunderstanding the overall sampling accuracy. In this work, we continue our\nanalysis of the convergence properties of the deterministic sampling methods\nderived from probability flow ODEs [25], focusing on $p$-th order (exponential)\nRunge-Kutta schemes for any integer $p \\geq 1$. Under the assumption that the\nfirst and second derivatives of the approximate score function are bounded, we\ndevelop $p$-th order (exponential) Runge-Kutta schemes and demonstrate that the\ntotal variation distance between the target distribution and the generated data\ndistribution can be bounded above by \\begin{align*}\n  O\\bigl(d^{\\frac{7}{4}}\\varepsilon_{\\text{score}}^{\\frac{1}{2}}\n+d(dH_{\\max})^p\\bigr), \\end{align*} where $\\varepsilon^2_{\\text{score}}$\ndenotes the $L^2$ error in the score function approximation, $d$ is the data\ndimension and $H_{\\max}$ represents the maximum step size used in the solver.\nWe numerically verify the regularity assumption on benchmark datasets,\nconfirming that the first and second derivatives of the approximate score\nfunction remain bounded in practice. Our theoretical guarantees hold for\ngeneral forward processes with arbitrary variance schedules.\n","authors":["Daniel Zhengyu Huang","Jiaoyang Huang","Zhengjiang Lin"],"pdf_url":"https://arxiv.org/pdf/2506.13061v2.pdf","comment":"64 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.01953v2","updated":"2025-06-18T14:15:09Z","published":"2025-02-04T03:02:24Z","title":"Local minima of the empirical risk in high dimension: General theorems\n  and convex examples","summary":"  We consider a general model for high-dimensional empirical risk minimization\nwhereby the data $\\mathbf{x}_i$ are $d$-dimensional isotropic Gaussian vectors,\nthe model is parametrized by $\\mathbf{\\Theta}\\in\\mathbb{R}^{d\\times k}$, and\nthe loss depends on the data via the projection\n$\\mathbf{\\Theta}^\\mathsf{T}\\mathbf{x}_i$. This setting covers as special cases\nclassical statistics methods (e.g. multinomial regression and other generalized\nlinear models), but also two-layer fully connected neural networks with $k$\nhidden neurons. We use the Kac-Rice formula from Gaussian process theory to\nderive a bound on the expected number of local minima of this empirical risk,\nunder the proportional asymptotics in which $n,d\\to\\infty$, with $n\\asymp d$.\nVia Markov's inequality, this bound allows to determine the positions of these\nminimizers (with exponential deviation bounds) and hence derive sharp\nasymptotics on the estimation and prediction error. In this paper, we apply our\ncharacterization to convex losses, where high-dimensional asymptotics were not\n(in general) rigorously established for $k\\ge 2$. We show that our approach is\ntight and allows to prove previously conjectured results. In addition, we\ncharacterize the spectrum of the Hessian at the minimizer. A companion paper\napplies our general result to non-convex examples.\n","authors":["Kiana Asgari","Andrea Montanari","Basil Saeed"],"pdf_url":"https://arxiv.org/pdf/2502.01953v2.pdf","comment":"101 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.11511v2","updated":"2025-06-18T14:10:39Z","published":"2025-04-15T10:45:55Z","title":"Position Paper: Rethinking Privacy in RL for Sequential Decision-making\n  in the Age of LLMs","summary":"  The rise of reinforcement learning (RL) in critical real-world applications\ndemands a fundamental rethinking of privacy in AI systems. Traditional privacy\nframeworks, designed to protect isolated data points, fall short for sequential\ndecision-making systems where sensitive information emerges from temporal\npatterns, behavioral strategies, and collaborative dynamics. Modern RL\nparadigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in\nlarge language models (LLMs), exacerbate these challenges by introducing\ncomplex, interactive, and context-dependent learning environments that\ntraditional methods do not address. In this position paper, we argue for a new\nprivacy paradigm built on four core principles: multi-scale protection,\nbehavioral pattern protection, collaborative privacy preservation, and\ncontext-aware adaptation. These principles expose inherent tensions between\nprivacy, utility, and interpretability that must be navigated as RL systems\nbecome more pervasive in high-stakes domains like healthcare, autonomous\nvehicles, and decision support systems powered by LLMs. To tackle these\nchallenges, we call for the development of new theoretical frameworks,\npractical mechanisms, and rigorous evaluation methodologies that collectively\nenable effective privacy protection in sequential decision-making systems.\n","authors":["Flint Xiaofeng Fan","Cheston Tan","Roger Wattenhofer","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2504.11511v2.pdf","comment":"IJCNN 2025 Position Paper Track"},{"id":"http://arxiv.org/abs/2506.15479v1","updated":"2025-06-18T14:10:38Z","published":"2025-06-18T14:10:38Z","title":"Creating User-steerable Projections with Interactive Semantic Mapping","summary":"  Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.\n","authors":["Artur André Oliveira","Mateus Espadoto","Roberto Hirata Jr.","Roberto M. Cesar Jr.","Alex C. Telea"],"pdf_url":"https://arxiv.org/pdf/2506.15479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10244v2","updated":"2025-06-18T14:06:53Z","published":"2025-06-11T23:57:26Z","title":"A new type of federated clustering: A non-model-sharing approach","summary":"  In recent years, the growing need to leverage sensitive data across\ninstitutions has led to increased attention on federated learning (FL), a\ndecentralized machine learning paradigm that enables model training without\nsharing raw data. However, existing FL-based clustering methods, known as\nfederated clustering, typically assume simple data partitioning scenarios such\nas horizontal or vertical splits, and cannot handle more complex distributed\nstructures. This study proposes data collaboration clustering (DC-Clustering),\na novel federated clustering method that supports clustering over complex data\npartitioning scenarios where horizontal and vertical splits coexist. In\nDC-Clustering, each institution shares only intermediate representations\ninstead of raw data, ensuring privacy preservation while enabling collaborative\nclustering. The method allows flexible selection between k-means and spectral\nclustering, and achieves final results with a single round of communication\nwith the central server. We conducted extensive experiments using synthetic and\nopen benchmark datasets. The results show that our method achieves clustering\nperformance comparable to centralized clustering where all data are pooled.\nDC-Clustering addresses an important gap in current FL research by enabling\neffective knowledge discovery from distributed heterogeneous data. Its\npractical properties -- privacy preservation, communication efficiency, and\nflexibility -- make it a promising tool for privacy-sensitive domains such as\nhealthcare and finance.\n","authors":["Yuji Kawamata","Kaoru Kamijo","Masateru Kihira","Akihiro Toyoda","Tomoru Nakayama","Akira Imakura","Tetsuya Sakurai","Yukihiko Okada"],"pdf_url":"https://arxiv.org/pdf/2506.10244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11982v2","updated":"2025-06-18T14:06:33Z","published":"2025-06-13T17:39:41Z","title":"Interpretable representation learning of quantum data enabled by\n  probabilistic variational autoencoders","summary":"  Interpretable machine learning is rapidly becoming a crucial tool for\nscientific discovery. Among existing approaches, variational autoencoders\n(VAEs) have shown promise in extracting the hidden physical features of some\ninput data, with no supervision nor prior knowledge of the system at study.\nYet, the ability of VAEs to create meaningful, interpretable representations\nrelies on their accurate approximation of the underlying probability\ndistribution of their input. When dealing with quantum data, VAEs must hence\naccount for its intrinsic randomness and complex correlations. While VAEs have\nbeen previously applied to quantum data, they have often neglected its\nprobabilistic nature, hindering the extraction of meaningful physical\ndescriptors. Here, we demonstrate that two key modifications enable VAEs to\nlearn physically meaningful latent representations: a decoder capable of\nfaithfully reproduce quantum states and a probabilistic loss tailored to this\ntask. Using benchmark quantum spin models, we identify regimes where standard\nmethods fail while the representations learned by our approach remain\nmeaningful and interpretable. Applied to experimental data from Rydberg atom\narrays, the model autonomously uncovers the phase structure without access to\nprior labels, Hamiltonian details, or knowledge of relevant order parameters,\nhighlighting its potential as an unsupervised and interpretable tool for the\nstudy of quantum systems.\n","authors":["Paulin de Schoulepnikoff","Gorka Muñoz-Gil","Hendrik Poulsen Nautrup","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2506.11982v2.pdf","comment":"Main text 10 pages, total document 16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.13244v3","updated":"2025-06-18T14:04:08Z","published":"2025-06-16T08:42:31Z","title":"No-Regret Learning Under Adversarial Resource Constraints: A Spending\n  Plan Is All You Need!","summary":"  We study online decision making problems under resource constraints, where\nboth reward and cost functions are drawn from distributions that may change\nadversarially over time. We focus on two canonical settings: $(i)$ online\nresource allocation where rewards and costs are observed before action\nselection, and $(ii)$ online learning with resource constraints where they are\nobserved after action selection, under full feedback or bandit feedback. It is\nwell known that achieving sublinear regret in these settings is impossible when\nreward and cost distributions may change arbitrarily over time. To address this\nchallenge, we analyze a framework in which the learner is guided by a spending\nplan--a sequence prescribing expected resource usage across rounds. We design\ngeneral (primal-)dual methods that achieve sublinear regret with respect to\nbaselines that follow the spending plan. Crucially, the performance of our\nalgorithms improves when the spending plan ensures a well-balanced distribution\nof the budget across rounds. We additionally provide a robust variant of our\nmethods to handle worst-case scenarios where the spending plan is highly\nimbalanced. To conclude, we study the regret of our algorithms when competing\nagainst benchmarks that deviate from the prescribed spending plan.\n","authors":["Francesco Emanuele Stradi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti","Christian Kroer"],"pdf_url":"https://arxiv.org/pdf/2506.13244v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15751v4","updated":"2025-06-18T14:02:28Z","published":"2025-01-27T03:35:25Z","title":"Adversarially Robust Bloom Filters: Privacy, Reductions, and Open\n  Problems","summary":"  A Bloom filter is a space-efficient probabilistic data structure that\nrepresents a set $S$ of elements from a larger universe $U$. This efficiency\ncomes with a trade-off, namely, it allows for a small chance of false\npositives. When you query the Bloom filter about an element x, the filter will\nrespond 'Yes' if $x \\in S$. If $x \\notin S$, it may still respond 'Yes' with\nprobability at most $\\varepsilon$. We investigate the adversarial robustness\nand privacy of Bloom filters, addressing open problems across three prominent\nframeworks: the game-based model of Naor-Oved-Yogev (NOY), the simulator-based\nmodel of Filic et. al., and learning-augmented variants. We prove the first\nformal connection between the Filic and NOY models, showing that Filic\ncorrectness implies AB-test resilience. We resolve a longstanding open question\nby proving that PRF-backed Bloom filters fail the NOY model's stronger BP-test.\nFinally, we introduce the first private Bloom filters with differential privacy\nguarantees, including constructions applicable to learned Bloom filters. Our\ntaxonomy organizes the space of robustness and privacy guarantees, clarifying\nrelationships between models and constructions.\n","authors":["Hayder Tirmazi"],"pdf_url":"https://arxiv.org/pdf/2501.15751v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15468v1","updated":"2025-06-18T13:58:45Z","published":"2025-06-18T13:58:45Z","title":"Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI","summary":"  We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.\n","authors":["Ryota Okumura","Tadahiro Taniguchi","Akira Taniguchi","Yoshinobu Hagiwara"],"pdf_url":"https://arxiv.org/pdf/2506.15468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15464v1","updated":"2025-06-18T13:55:30Z","published":"2025-06-18T13:55:30Z","title":"Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic\n  Graphs","summary":"  Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.\n","authors":["Le Vu Anh","Mehmet Dik","Nguyen Viet Anh"],"pdf_url":"https://arxiv.org/pdf/2506.15464v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.15461v1","updated":"2025-06-18T13:48:33Z","published":"2025-06-18T13:48:33Z","title":"All is Not Lost: LLM Recovery without Checkpoints","summary":"  Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.\n","authors":["Nikolay Blagoev","Oğuzhan Ersoy","Lydia Yiyu Chen"],"pdf_url":"https://arxiv.org/pdf/2506.15461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09957v2","updated":"2025-06-18T13:42:20Z","published":"2024-09-16T03:05:11Z","title":"Deep Graph Anomaly Detection: A Survey and New Perspectives","summary":"  Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.\n","authors":["Hezhe Qiao","Hanghang Tong","Bo An","Irwin King","Charu Aggarwal","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2409.09957v2.pdf","comment":"Accepted by TKDE"},{"id":"http://arxiv.org/abs/2506.15452v1","updated":"2025-06-18T13:25:48Z","published":"2025-06-18T13:25:48Z","title":"Warping and Matching Subsequences Between Time Series","summary":"  Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.\n","authors":["Simiao Lin","Wannes Meert","Pieter Robberechts","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2506.15452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15448v1","updated":"2025-06-18T13:19:22Z","published":"2025-06-18T13:19:22Z","title":"Semi-supervised Graph Anomaly Detection via Robust Homophily Learning","summary":"  Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.\n","authors":["Guoguo Ai","Hezhe Qiao","Hui Yan","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2506.15448v1.pdf","comment":"18 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2506.15446v1","updated":"2025-06-18T13:18:36Z","published":"2025-06-18T13:18:36Z","title":"Zero-Shot Reinforcement Learning Under Partial Observability","summary":"  Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.\n","authors":["Scott Jeen","Tom Bewley","Jonathan M. Cullen"],"pdf_url":"https://arxiv.org/pdf/2506.15446v1.pdf","comment":"Reinforcement Learning Conference 2025"},{"id":"http://arxiv.org/abs/2503.01630v2","updated":"2025-06-18T13:02:48Z","published":"2025-03-03T15:05:48Z","title":"Machine Learners Should Acknowledge the Legal Implications of Large\n  Language Models as Personal Data","summary":"  Does GPT know you? The answer depends on your level of public recognition;\nhowever, if your information was available on a website, the answer could be\nyes. Most Large Language Models (LLMs) memorize training data to some extent.\nThus, even when an LLM memorizes only a small amount of personal data, it\ntypically falls within the scope of data protection laws. If a person is\nidentified or identifiable, the implications are far-reaching. The LLM is\nsubject to EU General Data Protection Regulation requirements even after the\ntraining phase is concluded. To back our arguments: (1.) We reiterate that LLMs\noutput training data at inference time, be it verbatim or in generalized form.\n(2.) We show that some LLMs can thus be considered personal data on their own.\nThis triggers a cascade of data protection implications such as data subject\nrights, including rights to access, rectification, or erasure. These rights\nextend to the information embedded within the AI model. (3.) This paper argues\nthat machine learning researchers must acknowledge the legal implications of\nLLMs as personal data throughout the full ML development lifecycle, from data\ncollection and curation to model provision on e.g., GitHub or Hugging Face.\n(4.) We propose different ways for the ML research community to deal with these\nlegal implications. Our paper serves as a starting point for improving the\nalignment between data protection law and the technical capabilities of LLMs.\nOur findings underscore the need for more interaction between the legal domain\nand the ML community.\n","authors":["Henrik Nolte","Michèle Finck","Kristof Meding"],"pdf_url":"https://arxiv.org/pdf/2503.01630v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14095v2","updated":"2025-06-18T12:59:54Z","published":"2025-06-17T01:19:28Z","title":"Transformers Learn Faster with Semantic Focus","summary":"  Various forms of sparse attention have been explored to mitigate the\nquadratic computational and memory cost of the attention mechanism in\ntransformers. We study sparse transformers not through a lens of efficiency but\nrather in terms of learnability and generalization. Empirically studying a\nrange of attention mechanisms, we find that input-dependent sparse attention\nmodels appear to converge faster and generalize better than standard attention\nmodels, while input-agnostic sparse attention models show no such benefits -- a\nphenomenon that is robust across architectural and optimization hyperparameter\nchoices. This can be interpreted as demonstrating that concentrating a model's\n\"semantic focus\" with respect to the tokens currently being considered (in the\nform of input-dependent sparse attention) accelerates learning. We develop a\ntheoretical characterization of the conditions that explain this behavior. We\nestablish a connection between the stability of the standard softmax and the\nloss function's Lipschitz properties, then show how sparsity affects the\nstability of the softmax and the subsequent convergence and generalization\nguarantees resulting from the attention mechanism. This allows us to\ntheoretically establish that input-agnostic sparse attention does not provide\nany benefits. We also characterize conditions when semantic focus\n(input-dependent sparse attention) can provide improved guarantees, and we\nvalidate that these conditions are in fact met in our empirical evaluations.\n","authors":["Parikshit Ram","Kenneth L. Clarkson","Tim Klinger","Shashanka Ubaru","Alexander G. Gray"],"pdf_url":"https://arxiv.org/pdf/2506.14095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15421v1","updated":"2025-06-18T12:46:39Z","published":"2025-06-18T12:46:39Z","title":"Reward Models in Deep Reinforcement Learning: A Survey","summary":"  In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.\n","authors":["Rui Yu","Shenghua Wan","Yucen Wang","Chen-Xiao Gao","Le Gan","Zongzhang Zhang","De-Chuan Zhan"],"pdf_url":"https://arxiv.org/pdf/2506.15421v1.pdf","comment":"IJCAI 2025 Survey Track (To Appear)"},{"id":"http://arxiv.org/abs/2409.17287v2","updated":"2025-06-18T12:37:28Z","published":"2024-09-20T17:30:19Z","title":"Blockchain-Enabled Variational Information Bottleneck for Data\n  Extraction Based on Mutual Information in Internet of Vehicles","summary":"  The Internet of Vehicles (IoV) network can address the issue of limited\ncomputing resources and data processing capabilities of individual vehicles,\nbut it also brings the risk of privacy leakage to vehicle users. Applying\nblockchain technology can establish secure data links within the IoV, solving\nthe problems of insufficient computing resources for each vehicle and the\nsecurity of data transmission over the network. However, with the development\nof the IoV, the amount of data interaction between multiple vehicles and\nbetween vehicles and base stations, roadside units, etc., is continuously\nincreasing. There is a need to further reduce the interaction volume, and\nintelligent data compression is key to solving this problem. The VIB technique\nfacilitates the training of encoding and decoding models, substantially\ndiminishing the volume of data that needs to be transmitted. This paper\nintroduces an innovative approach that integrates blockchain with VIB, referred\nto as BVIB, designed to lighten computational workloads and reinforce the\nsecurity of the network. We first construct a new network framework by\nseparating the encoding and decoding networks to address the computational\nburden issue, and then propose a new algorithm to enhance the security of IoV\nnetworks. We also discuss the impact of the data extraction rate on system\nlatency to determine the most suitable data extraction rate. An experimental\nframework combining Python and C++ has been established to substantiate the\nefficacy of our BVIB approach. Comprehensive simulation studies indicate that\nthe BVIB consistently excels in comparison to alternative foundational\nmethodologies.\n","authors":["Cui Zhang","Wenjun Zhang","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2409.17287v2.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/BVIB-for-Data-Extraction-Based-on\n  Mutual-Information-in-the-IoV"},{"id":"http://arxiv.org/abs/2411.18043v2","updated":"2025-06-18T12:35:32Z","published":"2024-11-27T04:25:13Z","title":"Heterogeneous Relationships of Subjects and Shapelets for\n  Semi-supervised Multivariate Series Classification","summary":"  Multivariate time series (MTS) classification is widely applied in fields\nsuch as industry, healthcare, and finance, aiming to extract key features from\ncomplex time series data for accurate decision-making and prediction. However,\nexisting methods for MTS often struggle due to the challenges of effectively\nmodeling high-dimensional data and the lack of labeled data, resulting in poor\nclassification performance. To address this issue, we propose a heterogeneous\nrelationships of subjects and shapelets method for semi-supervised MTS\nclassification. This method offers a novel perspective by integrating various\ntypes of additional information while capturing the relationships between them.\nSpecifically, we first utilize a contrast temporal self-attention module to\nobtain sparse MTS representations, and then model the similarities between\nthese representations using soft dynamic time warping to construct a similarity\ngraph. Secondly, we learn the shapelets for different subject types,\nincorporating both the subject features and their shapelets as additional\ninformation to further refine the similarity graph, ultimately generating a\nheterogeneous graph. Finally, we use a dual level graph attention network to\nget prediction. Through this method, we successfully transform dataset into a\nheterogeneous graph, integrating multiple additional information and achieving\nprecise semi-supervised node classification. Experiments on the Human Activity\nRecognition, sleep stage classification and University of East Anglia datasets\ndemonstrate that our method outperforms current state-of-the-art methods in MTS\nclassification tasks, validating its superiority.\n","authors":["Mingsen Du","Meng Chen","Yongjian Li","Cun Ji","Shoushui Wei"],"pdf_url":"https://arxiv.org/pdf/2411.18043v2.pdf","comment":"We would like to request the withdrawal of our manuscript due to\n  logical errors in the paper"},{"id":"http://arxiv.org/abs/2411.12222v2","updated":"2025-06-18T12:34:55Z","published":"2024-11-19T04:32:41Z","title":"Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time\n  Series Node Classification","summary":"  Multivariate time series (MTS) data is generated through multiple sensors\nacross various domains such as engineering application, health monitoring, and\nthe internet of things, characterized by its temporal changes and high\ndimensional characteristics. Over the past few years, many studies have\nexplored the long-range dependencies and similarities in MTS. However,\nlong-range dependencies are difficult to model due to their temporal changes\nand high dimensionality makes it difficult to obtain similarities effectively\nand efficiently. Thus, to address these issues, we propose contrast\nsimilarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).\nFirstly, to obtain the dynamic similarity of each sample, we initially use\ntemporal contrast learning module to acquire MTS representations. And then we\nconstruct a similarity matrix between MTS representations using Fast Dynamic\nTime Warping (FastDTW). Secondly, we apply the DPMamba to consider the\nbidirectional nature of MTS, allowing us to better capture long-range and\nshort-range dependencies within the data. Finally, we utilize the\nKolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the\ninformation interaction in the matrix and MTS node classification task. By\ncomprehensively considering the long-range dependencies and dynamic similarity\nfeatures, we achieved precise MTS node classification. We conducted experiments\non multiple University of East Anglia (UEA) MTS datasets, which encompass\ndiverse application scenarios. Our results demonstrate the superiority of our\nmethod through both supervised and semi-supervised experiments on the MTS\nclassification task.\n","authors":["Mingsen Du","Meng Chen","Yongjian Li","Xiuxin Zhang","Jiahui Gao","Cun Ji","Shoushui Wei"],"pdf_url":"https://arxiv.org/pdf/2411.12222v2.pdf","comment":"We would like to request the withdrawal of our manuscript due to\n  logical errors in the paper"},{"id":"http://arxiv.org/abs/2410.16500v2","updated":"2025-06-18T12:34:17Z","published":"2024-10-21T20:45:13Z","title":"Implementation and Assessment of Machine Learning Models for Forecasting\n  Suspected Opioid Overdoses in Emergency Medical Services Data","summary":"  We present efforts in the fields of machine learning and time series\nforecasting to accurately predict counts of future suspected opioid overdoses\nrecorded by Emergency Medical Services (EMS) in the state of Kentucky.\nForecasts help government agencies properly prepare and distribute resources\nrelated to opioid overdoses. Our approach uses county and district level\naggregations of suspected opioid overdose encounters and forecasts future\ncounts for different time intervals. Models with different levels of complexity\nwere evaluated to minimize forecasting error. A variety of additional\ncovariates relevant to opioid overdoses and public health were tested to\ndetermine their impact on model performance. Our evaluation shows that useful\npredictions can be generated with limited error for different types of regions,\nand high performance can be achieved using commonly available covariates and\nrelatively simple forecasting models.\n","authors":["Aaron D. Mullen","Daniel R. Harris","Peter Rock","Katherine Thompson","Svetla Slavova","Jeffery Talbert","V. K. Cody Bumgardner"],"pdf_url":"https://arxiv.org/pdf/2410.16500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08462v2","updated":"2025-06-18T12:32:53Z","published":"2024-07-11T12:58:47Z","title":"Distributed Deep Reinforcement Learning Based Gradient Quantization for\n  Federated Learning Enabled Vehicle Edge Computing","summary":"  Federated Learning (FL) can protect the privacy of the vehicles in vehicle\nedge computing (VEC) to a certain extent through sharing the gradients of\nvehicles' local models instead of local data. The gradients of vehicles' local\nmodels are usually large for the vehicular artificial intelligence (AI)\napplications, thus transmitting such large gradients would cause large\nper-round latency. Gradient quantization has been proposed as one effective\napproach to reduce the per-round latency in FL enabled VEC through compressing\ngradients and reducing the number of bits, i.e., the quantization level, to\ntransmit gradients. The selection of quantization level and thresholds\ndetermines the quantization error, which further affects the model accuracy and\ntraining time. To do so, the total training time and quantization error (QE)\nbecome two key metrics for the FL enabled VEC. It is critical to jointly\noptimize the total training time and QE for the FL enabled VEC. However, the\ntime-varying channel condition causes more challenges to solve this problem. In\nthis paper, we propose a distributed deep reinforcement learning (DRL)-based\nquantization level allocation scheme to optimize the long-term reward in terms\nof the total training time and QE. Extensive simulations identify the optimal\nweighted factors between the total training time and QE, and demonstrate the\nfeasibility and effectiveness of the proposed scheme.\n","authors":["Cui Zhang","Wenjun Zhang","Qiong Wu","Pingyi Fan","Qiang Fan","Jiangzhou Wang","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.08462v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at:\n  https://github.com/qiongwu86/Distributed-Deep-Reinforcement-Learning-Based-Gradient\n  Quantization-for-Federated-Learning-Enabled-Vehicle-Edge-Computing"},{"id":"http://arxiv.org/abs/2503.15576v2","updated":"2025-06-18T12:27:58Z","published":"2025-03-19T13:19:06Z","title":"A Bird Song Detector for improving bird identification through Deep\n  Learning: a case study from Doñana","summary":"  Passive Acoustic Monitoring is a key tool for biodiversity conservation, but\nthe large volumes of unsupervised audio it generates present major challenges\nfor extracting meaningful information. Deep Learning offers promising\nsolutions. BirdNET, a widely used bird identification model, has shown success\nin many study systems but is limited at local scale due to biases in its\ntraining data, which focus on specific locations and target sounds rather than\nentire soundscapes. A key challenge in bird species identification is that many\nrecordings either lack target species or contain overlapping vocalizations,\ncomplicating automatic identification. To address these problems, we developed\na multi-stage pipeline for automatic bird vocalization identification in\nDo\\~nana National Park (SW Spain), a wetland of high conservation concern. We\ndeployed AudioMoth recorders in three main habitats across nine locations and\nmanually annotated 461 minutes of audio, resulting in 3749 labeled segments\nspanning 34 classes. We first applied a Bird Song Detector to isolate bird\nvocalizations using spectrogram-based image processing. Then, species were\nclassified using custom models trained at the local scale. Applying the Bird\nSong Detector before classification improved species identification, as all\nmodels performed better when analyzing only the segments where birds were\ndetected. Specifically, the combination of detector and fine-tuned BirdNET\noutperformed the baseline without detection. This approach demonstrates the\neffectiveness of integrating a Bird Song Detector with local classification\nmodels. These findings highlight the need to adapt general-purpose tools to\nspecific ecological challenges. Automatically detecting bird species helps\ntrack the health of this threatened ecosystem, given birds sensitivity to\nenvironmental change, and supports conservation planning to reduce biodiversity\nloss.\n","authors":["Alba Márquez-Rodríguez","Miguel Ángel Mohedano-Munoz","Manuel J. Marín-Jiménez","Eduardo Santamaría-García","Giulia Bastianelli","Pedro Jordano","Irene Mendoza"],"pdf_url":"https://arxiv.org/pdf/2503.15576v2.pdf","comment":"23 pages, 14 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector"},{"id":"http://arxiv.org/abs/2506.15408v1","updated":"2025-06-18T12:25:37Z","published":"2025-06-18T12:25:37Z","title":"Unifying VXAI: A Systematic Review and Framework for the Evaluation of\n  Explainable AI","summary":"  Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.\n","authors":["David Dembinsky","Adriano Lucieri","Stanislav Frolov","Hiba Najjar","Ko Watanabe","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2506.15408v1.pdf","comment":"Submitted to TMLR, under review"},{"id":"http://arxiv.org/abs/2502.03029v3","updated":"2025-06-18T12:24:26Z","published":"2025-02-05T09:31:27Z","title":"On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation","summary":"  The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention.\n","authors":["Nghiem T. Diep","Huy Nguyen","Chau Nguyen","Minh Le","Duy M. H. Nguyen","Daniel Sonntag","Mathias Niepert","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2502.03029v3.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15404v1","updated":"2025-06-18T12:22:17Z","published":"2025-06-18T12:22:17Z","title":"NERO: Explainable Out-of-Distribution Detection with Neuron-level\n  Relevance","summary":"  Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.\n","authors":["Anju Chhetri","Jari Korhonen","Prashnna Gyawali","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2506.15404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14670v3","updated":"2025-06-18T12:18:24Z","published":"2023-11-24T18:59:04Z","title":"Differentiable and accelerated spherical harmonic and Wigner transforms","summary":"  Many areas of science and engineering encounter data defined on spherical\nmanifolds. Modelling and analysis of spherical data often necessitates\nspherical harmonic transforms, at high degrees, and increasingly requires\nefficient computation of gradients for machine learning or other differentiable\nprogramming tasks. We develop novel algorithmic structures for accelerated and\ndifferentiable computation of generalised Fourier transforms on the sphere\n$\\mathbb{S}^2$ and rotation group $\\text{SO}(3)$, i.e. spherical harmonic and\nWigner transforms, respectively. We present a recursive algorithm for the\ncalculation of Wigner $d$-functions that is both stable to high harmonic\ndegrees and extremely parallelisable. By tightly coupling this with separable\nspherical transforms, we obtain algorithms that exhibit an extremely\nparallelisable structure that is well-suited for the high throughput computing\nof modern hardware accelerators (e.g. GPUs). We also develop a hybrid automatic\nand manual differentiation approach so that gradients can be computed\nefficiently. Our algorithms are implemented within the JAX differentiable\nprogramming framework in the S2FFT software code. Numerous samplings of the\nsphere are supported, including equiangular and HEALPix sampling. Computational\nerrors are at the order of machine precision for spherical samplings that admit\na sampling theorem. When benchmarked against alternative C codes we observe up\nto a 400-fold acceleration. Furthermore, when distributing over multiple GPUs\nwe achieve very close to optimal linear scaling with increasing number of GPUs\ndue to the highly parallelised and balanced nature of our algorithms. Provided\naccess to sufficiently many GPUs our transforms thus exhibit an unprecedented\neffective linear time complexity.\n","authors":["Matthew A. Price","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2311.14670v3.pdf","comment":"31 pages, 7 figures, accepted by Journal of Computational Physics,\n  code available at https://github.com/astro-informatics/s2fft"},{"id":"http://arxiv.org/abs/2407.13123v2","updated":"2025-06-18T12:15:24Z","published":"2024-07-18T03:18:59Z","title":"Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint\n  Phase-shift Optimization and Multi-User Power Allocation","summary":"  Vehicular edge computing (VEC) is an emerging technology with significant\npotential in the field of internet of vehicles (IoV), enabling vehicles to\nperform intensive computational tasks locally or offload them to nearby edge\ndevices. However, the quality of communication links may be severely\ndeteriorated due to obstacles such as buildings, impeding the offloading\nprocess. To address this challenge, we introduce the use of Reconfigurable\nIntelligent Surfaces (RIS), which provide alternative communication pathways to\nassist vehicular communication. By dynamically adjusting the phase-shift of the\nRIS, the performance of VEC systems can be substantially improved. In this\nwork, we consider a RIS-assisted VEC system, and design an optimal scheme for\nlocal execution power, offloading power, and RIS phase-shift, where random task\narrivals and channel variations are taken into account. To address the scheme,\nwe propose an innovative deep reinforcement learning (DRL) framework that\ncombines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing\nRIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm for optimizing the power allocation of vehicle user\n(VU). Simulation results show that our proposed scheme outperforms the\ntraditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient\n(TD3) and some typical stochastic schemes.\n","authors":["Kangwei Qi","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2407.13123v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at\n  https://github.com/qiongwu86/DDPG-RIS-MADDPG-POWER. arXiv admin note: text\n  overlap with arXiv:2406.11318"},{"id":"http://arxiv.org/abs/2506.15397v1","updated":"2025-06-18T12:15:13Z","published":"2025-06-18T12:15:13Z","title":"Learn to Vaccinate: Combining Structure Learning and Effective\n  Vaccination for Epidemic and Outbreak Control","summary":"  The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.\n","authors":["Sepehr Elahi","Paula Mürmann","Patrick Thiran"],"pdf_url":"https://arxiv.org/pdf/2506.15397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11310v2","updated":"2025-06-18T12:07:56Z","published":"2024-07-16T01:51:32Z","title":"Digital Twin Vehicular Edge Computing Network: Task Offloading and\n  Resource Allocation","summary":"  With the increasing demand for multiple applications on internet of vehicles.\nIt requires vehicles to carry out multiple computing tasks in real time.\nHowever, due to the insufficient computing capability of vehicles themselves,\noffloading tasks to vehicular edge computing (VEC) servers and allocating\ncomputing resources to tasks becomes a challenge. In this paper, a multi task\ndigital twin (DT) VEC network is established. By using DT to develop offloading\nstrategies and resource allocation strategies for multiple tasks of each\nvehicle in a single slot, an optimization problem is constructed. To solve it,\nwe propose a multi-agent reinforcement learning method on the task offloading\nand resource allocation. Numerous experiments demonstrate that our method is\neffective compared to other benchmark algorithms.\n","authors":["Yu Xie","Qiong Wu","Pingyi Fan"],"pdf_url":"https://arxiv.org/pdf/2407.11310v2.pdf","comment":"This paper has been accepted by ICICSP 2024. The source code has been\n  released\n  at:https://github.com/qiongwu86/Digital-Twin-Vehicular-Edge-Computing-Network_Task-Offloading-and-Resource-Allocation"},{"id":"http://arxiv.org/abs/2506.15387v1","updated":"2025-06-18T12:03:34Z","published":"2025-06-18T12:03:34Z","title":"Multi-Timescale Gradient Sliding for Distributed Optimization","summary":"  We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).\n","authors":["Junhui Zhang","Patrick Jaillet"],"pdf_url":"https://arxiv.org/pdf/2506.15387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03767v2","updated":"2025-06-18T12:02:14Z","published":"2025-03-03T11:21:48Z","title":"A Survey on Semantic Communications in Internet of Vehicles","summary":"  Internet of Vehicles (IoV), as the core of intelligent transportation system,\nenables comprehensive interconnection between vehicles and their surroundings\nthrough multiple communication modes, which is significant for autonomous\ndriving and intelligent traffic management. However, with the emergence of new\napplications, traditional communication technologies face the problems of\nscarce spectrum resources and high latency. Semantic communication, which\nfocuses on extracting, transmitting, and recovering some useful semantic\ninformation from messages, can reduce redundant data transmission, improve\nspectrum utilization, and provide innovative solutions to communication\nchallenges in the IoV. This paper systematically reviews state of art of\nsemantic communications in the IoV, elaborates the technical background of IoV\nand semantic communications, and deeply discusses key technologies of semantic\ncommunications in IoV, including semantic information extraction, semantic\ncommunication architecture, resource allocation and management, and so on.\nThrough specific case studies, it demonstrates that semantic communications can\nbe effectively employed in the scenarios of traffic environment perception and\nunderstanding, intelligent driving decision support, IoV service optimization,\nand intelligent traffic management. Additionally, it analyzes the current\nchallenges and future research directions. This survey reveals that semantic\ncommunications has broad application prospects in IoV, but it is necessary to\nsolve the real existing problems by combining advanced technologies to promote\nits wide application in IoV and contributing to the development of intelligent\ntransportation system.\n","authors":["Sha Ye","Qiong Wu","Pingyi Fan","Qiang Fan"],"pdf_url":"https://arxiv.org/pdf/2503.03767v2.pdf","comment":"This paper has been accepted to Entropy"},{"id":"http://arxiv.org/abs/2506.15385v1","updated":"2025-06-18T11:59:15Z","published":"2025-06-18T11:59:15Z","title":"Provable Maximum Entropy Manifold Exploration via Diffusion Models","summary":"  Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.\n","authors":["Riccardo De Santi","Marin Vlastelica","Ya-Ping Hsieh","Zebang Shen","Niao He","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2506.15385v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2506.15383v1","updated":"2025-06-18T11:53:13Z","published":"2025-06-18T11:53:13Z","title":"Global Ground Metric Learning with Applications to scRNA data","summary":"  Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.\n","authors":["Damin Kühn","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2506.15383v1.pdf","comment":"This method is provided as a Python package on PyPI, see\n  https://github.com/DaminK/ggml-ot"},{"id":"http://arxiv.org/abs/2506.04265v2","updated":"2025-06-18T11:50:18Z","published":"2025-06-03T08:04:43Z","title":"CORA: Coalitional Rational Advantage Decomposition for Multi-Agent\n  Policy Gradients","summary":"  This work focuses on the credit assignment problem in cooperative multi-agent\nreinforcement learning (MARL). Sharing the global advantage among agents often\nleads to suboptimal policy updates as it fails to account for the distinct\ncontributions of agents. Although numerous methods consider global or\nindividual contributions for credit assignment, a detailed analysis at the\ncoalition level remains lacking in many approaches. This work analyzes the\nover-updating problem during multi-agent policy updates from a coalition-level\nperspective. To address this issue, we propose a credit assignment method\ncalled Coalitional Rational Advantage Decomposition (CORA). CORA evaluates\ncoalitional advantages via marginal contributions from all possible coalitions\nand decomposes advantages using the core solution from cooperative game theory,\nensuring coalitional rationality. To reduce computational overhead, CORA\nemploys random coalition sampling. Experiments on matrix games, differential\ngames, and multi-agent collaboration benchmarks demonstrate that CORA\noutperforms strong baselines, particularly in tasks with multiple local optima.\nThese findings highlight the importance of coalition-aware credit assignment\nfor improving MARL performance.\n","authors":["Mengda Ji","Genjiu Xu","Liying Wang"],"pdf_url":"https://arxiv.org/pdf/2506.04265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15378v1","updated":"2025-06-18T11:47:59Z","published":"2025-06-18T11:47:59Z","title":"Sampling 3D Molecular Conformers with Diffusion Transformers","summary":"  Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.\n","authors":["J. Thorben Frank","Winfried Ripken","Gregor Lied","Klaus-Robert Müller","Oliver T. Unke","Stefan Chmiela"],"pdf_url":"https://arxiv.org/pdf/2506.15378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03196v2","updated":"2025-06-18T11:36:11Z","published":"2025-06-01T14:29:25Z","title":"Graph Neural Networks for Jamming Source Localization","summary":"  Graph-based learning provides a powerful framework for modeling complex\nrelational structures; however, its application within the domain of wireless\nsecurity remains significantly underexplored. In this work, we introduce the\nfirst application of graph-based learning for jamming source localization,\naddressing the imminent threat of jamming attacks in wireless networks. Unlike\ngeometric optimization techniques that struggle under environmental\nuncertainties and dense interference, we reformulate the localization as an\ninductive graph regression task. Our approach integrates structured node\nrepresentations that encode local and global signal aggregation, ensuring\nspatial coherence and adaptive signal fusion. To enhance robustness, we\nincorporate an attention-based \\ac{GNN} that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex \\ac{RF} environments with various sampling\ndensities, network topologies, jammer characteristics, and signal propagation\nconditions, conducting comprehensive ablation studies on graph construction,\nfeature selection, and pooling strategies. Results demonstrate that our novel\ngraph-based learning framework significantly outperforms established\nlocalization baselines, particularly in challenging scenarios with sparse and\nobfuscated signal information. Our code is available at\nhttps://github.com/tiiuae/gnn-jamming-source-localization.\n","authors":["Dania Herzalla","Willian T. Lunardi","Martin Andreoni"],"pdf_url":"https://arxiv.org/pdf/2506.03196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15366v1","updated":"2025-06-18T11:34:15Z","published":"2025-06-18T11:34:15Z","title":"Performative Validity of Recourse Explanations","summary":"  When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.\n","authors":["Gunnar König","Hidde Fokkema","Timo Freiesleben","Celestine Mendler-Dünner","Ulrike Von Luxburg"],"pdf_url":"https://arxiv.org/pdf/2506.15366v1.pdf","comment":"34 pages, 3 figures, 1 table, Preprint"},{"id":"http://arxiv.org/abs/2503.06253v2","updated":"2025-06-18T11:04:21Z","published":"2025-03-08T15:28:26Z","title":"MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM\n  Red Teaming","summary":"  With LLM usage rapidly increasing, their vulnerability to jailbreaks that\ncreate harmful outputs are a major security risk. As new jailbreaking\nstrategies emerge and models are changed by fine-tuning, continuous testing for\nsecurity vulnerabilities is necessary. Existing Red Teaming methods fall short\nin cost efficiency, attack success rate, attack diversity, or extensibility as\nnew attack types emerge. We address these challenges with Modular And Diverse\nMalicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses\nautomatic assignment of attack strategies into relevant attack clusters,\nchooses the most relevant clusters for a malicious goal, and then combines\nstrategies from the selected clusters to achieve diverse novel attacks with\nhigh attack success rates. MAD-MAX further merges promising attacks together at\neach iteration of Red Teaming to boost performance and introduces a similarity\nfilter to prune out similar attacks for increased cost efficiency. The MAD-MAX\napproach is designed to be easily extensible with newly discovered attack\nstrategies and outperforms the prominent Red Teaming method Tree of Attacks\nwith Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and\nqueries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals\nin our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX\ndoes so with only 10.9 average queries to the target LLM compared to TAP with\n23.3.\n  WARNING: This paper contains contents which are offensive in nature.\n","authors":["Stefan Schoepf","Muhammad Zaid Hameed","Ambrish Rawat","Kieran Fraser","Giulio Zizzo","Giandomenico Cornacchia","Mark Purcell"],"pdf_url":"https://arxiv.org/pdf/2503.06253v2.pdf","comment":"Data in Generative Models Workshop: The Bad, the Ugly, and the Greats\n  (DIG-BUGS) at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15349v1","updated":"2025-06-18T11:03:39Z","published":"2025-06-18T11:03:39Z","title":"Enhancing One-run Privacy Auditing with Quantile Regression-Based\n  Membership Inference","summary":"  Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.\n","authors":["Terrance Liu","Matteo Boglioni","Yiwei Fu","Shengyuan Hu","Pratiksha Thaker","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07179v2","updated":"2025-06-18T11:00:36Z","published":"2023-06-12T15:21:02Z","title":"Benchmarking Neural Network Training Algorithms","summary":"  Training algorithms, broadly construed, are an essential part of every deep\nlearning pipeline. Training algorithm improvements that speed up training\nacross a wide variety of workloads (e.g., better update rules, tuning\nprotocols, learning rate schedules, or data selection schemes) could save time,\nsave computational resources, and lead to better, more accurate, models.\nUnfortunately, as a community, we are currently unable to reliably identify\ntraining algorithm improvements, or even determine the state-of-the-art\ntraining algorithm. In this work, using concrete experiments, we argue that\nreal progress in speeding up training requires new benchmarks that resolve\nthree basic challenges faced by empirical comparisons of training algorithms:\n(1) how to decide when training is complete and precisely measure training\ntime, (2) how to handle the sensitivity of measurements to exact workload\ndetails, and (3) how to fairly compare algorithms that require hyperparameter\ntuning. In order to address these challenges, we introduce a new, competitive,\ntime-to-result benchmark using multiple workloads running on fixed hardware,\nthe AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of\nworkload variants that make it possible to detect benchmark submissions that\nare more robust to workload changes than current widely-used methods. Finally,\nwe evaluate baseline submissions constructed using various optimizers that\nrepresent current practice, as well as other optimizers that have recently\nreceived attention in the literature. These baseline results collectively\ndemonstrate the feasibility of our benchmark, show that non-trivial gaps\nbetween methods exist, and set a provisional state-of-the-art for future\nbenchmark submissions to try and surpass.\n","authors":["George E. Dahl","Frank Schneider","Zachary Nado","Naman Agarwal","Chandramouli Shama Sastry","Philipp Hennig","Sourabh Medapati","Runa Eschenhagen","Priya Kasimbeg","Daniel Suo","Juhan Bae","Justin Gilmer","Abel L. Peirson","Bilal Khan","Rohan Anil","Mike Rabbat","Shankar Krishnan","Daniel Snider","Ehsan Amid","Kongtao Chen","Chris J. Maddison","Rakshith Vasudev","Michal Badura","Ankush Garg","Peter Mattson"],"pdf_url":"https://arxiv.org/pdf/2306.07179v2.pdf","comment":"102 pages, 8 figures, 41 tables"},{"id":"http://arxiv.org/abs/2506.15346v1","updated":"2025-06-18T10:55:26Z","published":"2025-06-18T10:55:26Z","title":"Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges","summary":"  Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.\n","authors":["A. S. Stankevich","I. B. Petrov"],"pdf_url":"https://arxiv.org/pdf/2506.15346v1.pdf","comment":"Submitted to \"Computational Mathematics And Mathematical Physics\",\n  ISSN 1555-6662, issue 8, August 2025"},{"id":"http://arxiv.org/abs/2505.17830v2","updated":"2025-06-18T10:55:09Z","published":"2025-05-23T12:43:55Z","title":"Imagine Beyond! Distributionally Robust Auto-Encoding for State Space\n  Coverage in Online Reinforcement Learning","summary":"  Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.\n","authors":["Nicolas Castanet","Olivier Sigaud","Sylvain Lamprier"],"pdf_url":"https://arxiv.org/pdf/2505.17830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22963v2","updated":"2025-06-18T10:53:35Z","published":"2025-05-29T01:05:02Z","title":"Agile Orchestration at Will: An Entire Smart Service-Based Security\n  Architecture Towards 6G","summary":"  The upcoming 6G will fundamentally reshape mobile networks beyond\ncommunications, unlocking a multitude of applications that were once considered\nunimaginable. Meanwhile, security and resilience are especially highlighted in\nthe 6G design principles. However, safeguarding 6G networks will be quite\nchallenging due to various known and unknown threats from highly heterogeneous\nnetworks and diversified security requirements of distinct use cases, calling\nfor a comprehensive re-design of security architecture. This motivates us to\npropose ES3A (Entire Smart Service-based Security Architecture), a novel\nsecurity architecture for 6G networks. Specifically, we first discuss six\nhigh-level principles of our ES3A that include hierarchy, flexibility,\nscalability, resilience, endogeny, and trust and privacy. With these goals in\nmind, we then introduce three guidelines from a deployment perspective,\nenvisioning our ES3A that offers service-based security, end-to-end protection,\nand smart security automation for 6G networks. Our architecture consists of\nthree layers and three domains. It relies on a two-stage orchestration\nmechanism to tailor smart security strategies for customized protection in\nhigh-dynamic 6G networks, thereby addressing the aforementioned challenges.\nFinally, we prototype the proposed ES3A on a real-world radio system based on\nSoftware-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.\nWe also provide a case to show the superiority of our architecture.\n","authors":["Zhuoran Duan","Guoshun Nan","Rushan Li","Zijun Wang","Lihua Xiong","Chaoying Yuan","Guorong Liu","Hui Xu","Qimei Cui","Xiaofeng Tao","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2505.22963v2.pdf","comment":"Accepted by IEEE Wireless Communications Magazine"},{"id":"http://arxiv.org/abs/2403.05754v7","updated":"2025-06-18T10:42:23Z","published":"2024-03-09T01:34:26Z","title":"Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition","summary":"  In this paper, we propose two hybrid quantum-inspired neural networks with\nadaptive residual and dense connections respectively for pattern recognition.\nWe explain the frameworks of the symmetrical circuit models in the\nquantum-inspired layers in our hybrid models. We also illustrate the potential\nsuperiority of our hybrid models to prevent gradient explosion owing to the\nsine and cosine functions in the quantum-inspired layers. Groups of numerical\nexperiments on generalization power showcase that our hybrid models are\ncomparable to the pure classical models with different noisy datasets utilized.\nFurthermore, the comparison between our hybrid models and a state-of-the-art\nhybrid quantum-classical convolutional network demonstrates 3%-4% higher\naccuracy of our hybrid densely-connected model than the hybrid\nquantum-classical network. Additionally, compared with other two hybrid\nquantum-inspired residual networks, our hybrid models showcase a little higher\naccuracy on image datasets with asymmetrical noises. Simultaneously, in terms\nof groups of robustness experiments, the outcomes demonstrate that our two\nhybrid models outperform pure classical models notably in resistance to\nadversarial parameter attacks with various asymmetrical noises. They also\nindicate the slight superiority of our densely-connected hybrid model over the\nhybrid quantum-classical network to both symmetrical and asymmetrical attacks.\nMeanwhile, the accuracy of our two hybrid models is a little bit higher than\nthat of the two hybrid quantum-inspired residual networks. In addition, an\nablation study indicate that the recognition accuracy of our two hybrid models\nis 2%-3% higher than that of the traditional quantum-inspired neural network\nwithout residual or dense connection. Eventually, we discuss the application\nscenarios of our hybrid models by analyzing their computational complexity.\n","authors":["Andi Chen","Hua-Lei Yin","Zeng-Bing Chen","Shengjun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05754v7.pdf","comment":"21 pages of main paper with two links of a 20-page supplementary\n  material and the program codes below the acknowledgement in the main paper"},{"id":"http://arxiv.org/abs/2405.00074v2","updated":"2025-06-18T10:36:17Z","published":"2024-04-30T07:24:41Z","title":"PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating\n  Pre-trained Neural Networks","summary":"  We present PAODING, a toolkit to debloat pretrained neural network models\nthrough the lens of data-free pruning. To preserve the model fidelity, PAODING\nadopts an iterative process, which dynamically measures the effect of deleting\na neuron to identify candidates that have the least impact to the output layer.\nOur evaluation shows that PAODING can significantly reduce the model size,\ngeneralize on different datasets and models, and meanwhile preserve the model\nfidelity in terms of test accuracy and adversarial robustness. PAODING is\npublicly available on PyPI via https://pypi.org/project/paoding-dl.\n","authors":["Mark Huasong Meng","Hao Guan","Liuhuo Wan","Sin Gee Teo","Guangdong Bai","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2405.00074v2.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2506.06761v2","updated":"2025-06-18T10:34:41Z","published":"2025-06-07T11:05:33Z","title":"The OCR Quest for Generalization: Learning to recognize low-resource\n  alphabets with model editing","summary":"  Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.\n","authors":["Adrià Molina Rodríguez","Oriol Ramos Terrades","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2506.06761v2.pdf","comment":"Preprint (under review) For Journal"},{"id":"http://arxiv.org/abs/2506.15337v1","updated":"2025-06-18T10:32:26Z","published":"2025-06-18T10:32:26Z","title":"Knowledge Distillation Framework for Accelerating High-Accuracy Neural\n  Network-Based Molecular Dynamics Simulations","summary":"  Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.\n","authors":["Naoki Matsumura","Yuta Yoshimoto","Yuto Iwasaki","Meguru Yamazaki","Yasufumi Sakai"],"pdf_url":"https://arxiv.org/pdf/2506.15337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.00783v3","updated":"2025-06-18T10:26:20Z","published":"2022-04-02T07:09:17Z","title":"Supervised Robustness-preserving Data-free Neural Network Pruning","summary":"  When deploying pre-trained neural network models in real-world applications,\nmodel consumers often encounter resource-constraint platforms such as mobile\nand smart devices. They typically use the pruning technique to reduce the size\nand complexity of the model, generating a lighter one with less resource\nconsumption. Nonetheless, most existing pruning methods are proposed with the\npremise that the model after being pruned has a chance to be fine-tuned or even\nretrained based on the original training data. This may be unrealistic in\npractice, as the data controllers are often reluctant to provide their model\nconsumers with the original data. In this work, we study the neural network\npruning in the data-free context, aiming to yield lightweight models that are\nnot only accurate in prediction but also robust against undesired inputs in\nopen-world deployments. Considering the absence of the fine-tuning and\nretraining that can fix the mis-pruned units, we replace the traditional\naggressive one-shot strategy with a conservative one that treats the pruning as\na progressive process. We propose a pruning method based on stochastic\noptimization that uses robustness-related metrics to guide the pruning process.\nOur method is implemented as a Python program and evaluated with a series of\nexperiments on diverse neural network models. The experimental results show\nthat it significantly outperforms existing one-shot data-free pruning\napproaches in terms of robustness preservation and accuracy.\n","authors":["Mark Huasong Meng","Guangdong Bai","Sin Gee Teo","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2204.00783v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2501.10885v3","updated":"2025-06-18T10:13:51Z","published":"2025-01-18T21:44:38Z","title":"CEReBrO: Compact Encoder for Representations of Brain Oscillations Using\n  Efficient Alternating Attention","summary":"  Electroencephalograph (EEG) is a crucial tool for studying brain activity.\nRecently, self-supervised learning methods leveraging large unlabeled datasets\nhave emerged as a potential solution to the scarcity of widely available\nannotated EEG data. However, current methods suffer from at least one of the\nfollowing limitations: i) sub-optimal EEG signal modeling, ii) model sizes in\nthe hundreds of millions of trainable parameters, and iii) reliance on private\ndatasets and/or inconsistent public benchmarks, hindering reproducibility. To\naddress these challenges, we introduce a Compact Encoder for Representations of\nBrain Oscillations using alternating attention (CEReBrO), a new small EEG\nfoundation model. Our tokenization scheme represents EEG signals at a\nper-channel patch granularity. We propose an alternating attention mechanism\nthat jointly models intra-channel temporal dynamics and inter-channel spatial\ncorrelations, achieving 2x speed improvement with 6x less memory required\ncompared to standard self-attention. We present several model sizes ranging\nfrom 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of\npublicly available scalp EEG recordings with diverse channel configurations,\nour models set new benchmarks in emotion detection and seizure detection tasks,\nwith competitive performance in anomaly classification and gait prediction.\nThis validates our models' effectiveness and efficiency.\n","authors":["Alexandru Dimofte","Glenn Anta Bucagu","Thorir Mar Ingolfsson","Xiaying Wang","Andrea Cossettini","Luca Benini","Yawei Li"],"pdf_url":"https://arxiv.org/pdf/2501.10885v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15330v1","updated":"2025-06-18T10:10:02Z","published":"2025-06-18T10:10:02Z","title":"Universal Laboratory Model: prognosis of abnormal clinical outcomes\n  based on routine tests","summary":"  Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.\n","authors":["Pavel Karpov","Ilya Petrenkov","Ruslan Raiman"],"pdf_url":"https://arxiv.org/pdf/2506.15330v1.pdf","comment":"7 pages, 2 figues"},{"id":"http://arxiv.org/abs/2506.12708v2","updated":"2025-06-18T10:04:59Z","published":"2025-06-15T03:41:34Z","title":"Serving Large Language Models on Huawei CloudMatrix384","summary":"  The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.\n","authors":["Pengfei Zuo","Huimin Lin","Junbo Deng","Nan Zou","Xingkun Yang","Yingyu Diao","Weifeng Gao","Ke Xu","Zhangyu Chen","Shirui Lu","Zhao Qiu","Peiyang Li","Xianyu Chang","Zhengzhong Yu","Fangzheng Miao","Jia Zheng","Ying Li","Yuan Feng","Bei Wang","Zaijian Zong","Mosong Zhou","Wenli Zhou","Houjiang Chen","Xingyu Liao","Yipeng Li","Wenxiao Zhang","Ping Zhu","Yinggang Wang","Chuanjie Xiao","Depeng Liang","Dong Cao","Juncheng Liu","Yongqiang Yang","Xiaolong Bai","Yi Li","Huaguo Xie","Huatao Wu","Zhibin Yu","Lv Chen","Hu Liu","Yujun Ding","Haipei Zhu","Jing Xia","Yi Xiong","Zhou Yu","Heng Liao"],"pdf_url":"https://arxiv.org/pdf/2506.12708v2.pdf","comment":"59 pages, 24 figures"},{"id":"http://arxiv.org/abs/2506.15329v1","updated":"2025-06-18T10:01:17Z","published":"2025-06-18T10:01:17Z","title":"When and How Unlabeled Data Provably Improve In-Context Learning","summary":"  Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.\n","authors":["Yingcong Li","Xiangyu Chang","Muti Kara","Xiaofeng Liu","Amit Roy-Chowdhury","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2506.15329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15315v1","updated":"2025-06-18T09:44:13Z","published":"2025-06-18T09:44:13Z","title":"Proximal Operators of Sorted Nonconvex Penalties","summary":"  This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.\n","authors":["Anne Gagneux","Mathurin Massias","Emmanuel Soubies"],"pdf_url":"https://arxiv.org/pdf/2506.15315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15309v1","updated":"2025-06-18T09:39:51Z","published":"2025-06-18T09:39:51Z","title":"Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target\n  Inhibitor Generation","summary":"  Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.\n","authors":["Júlia Vilalta-Mor","Alexis Molina","Laura Ortega Varga","Isaac Filella-Merce","Victor Guallar"],"pdf_url":"https://arxiv.org/pdf/2506.15309v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.15307v1","updated":"2025-06-18T09:36:57Z","published":"2025-06-18T09:36:57Z","title":"SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language\n  Models Using Forward-Only Passes","summary":"  Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.\n","authors":["Jinglong Luo","Zhuo Zhang","Yehong Zhang","Shiyu Liu","Ye Dong","Xun Zhou","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2506.15307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15305v1","updated":"2025-06-18T09:35:50Z","published":"2025-06-18T09:35:50Z","title":"Conditional Generative Modeling for Enhanced Credit Risk Management in\n  Supply Chain Finance","summary":"  The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.\n","authors":["Qingkai Zhang","L. Jeff Hong","Houmin Yan"],"pdf_url":"https://arxiv.org/pdf/2506.15305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15304v1","updated":"2025-06-18T09:35:33Z","published":"2025-06-18T09:35:33Z","title":"ConLID: Supervised Contrastive Learning for Low-Resource Language\n  Identification","summary":"  Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.\n","authors":["Negar Foroutan","Jakhongir Saydaliev","Ye Eun Kim","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2506.15304v1.pdf","comment":"Submitted to EMNLP"},{"id":"http://arxiv.org/abs/2506.12749v2","updated":"2025-06-18T09:25:11Z","published":"2025-06-15T07:13:52Z","title":"Free Privacy Protection for Wireless Federated Learning: Enjoy It or\n  Suffer from It?","summary":"  Inherent communication noises have the potential to preserve privacy for\nwireless federated learning (WFL) but have been overlooked in digital\ncommunication systems predominantly using floating-point number standards,\ne.g., IEEE 754, for data storage and transmission. This is due to the\npotentially catastrophic consequences of bit errors in floating-point numbers,\ne.g., on the sign or exponent bits. This paper presents a novel channel-native\nbit-flipping differential privacy (DP) mechanism tailored for WFL, where\ntransmit bits are randomly flipped and communication noises are leveraged, to\ncollectively preserve the privacy of WFL in digital communication systems. The\nkey idea is to interpret the bit perturbation at the transmitter and bit errors\ncaused by communication noises as a bit-flipping DP process. This is achieved\nby designing a new floating-point-to-fixed-point conversion method that only\ntransmits the bits in the fraction part of model parameters, hence eliminating\nthe need for transmitting the sign and exponent bits and preventing the\ncatastrophic consequence of bit errors. We analyze a new metric to measure the\nbit-level distance of the model parameters and prove that the proposed\nmechanism satisfies (\\lambda,\\epsilon)-R\\'enyi DP and does not violate the WFL\nconvergence. Experiments validate privacy and convergence analysis of the\nproposed mechanism and demonstrate its superiority to the state-of-the-art\nGaussian mechanisms that are channel-agnostic and add Gaussian noise for\nprivacy protection.\n","authors":["Weicai Li","Tiejun Lv","Xiyu Zhao","Xin Yuan","Wei Ni"],"pdf_url":"https://arxiv.org/pdf/2506.12749v2.pdf","comment":"16 pages, 8 figures, accepted by IEEE Transactions on Information\n  Forensics and Security"},{"id":"http://arxiv.org/abs/2407.13538v3","updated":"2025-06-18T09:24:12Z","published":"2024-07-18T14:10:50Z","title":"EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion\n  Models","summary":"  High-resolution time series data are crucial for the operation and planning\nof energy systems such as electrical power systems and heating systems. Such\ndata often cannot be shared due to privacy concerns, necessitating the use of\nsynthetic data. However, high-resolution time series data is difficult to model\ndue to its inherent high dimensionality and complex temporal dependencies.\nLeveraging the recent development of generative AI, especially diffusion\nmodels, we propose EnergyDiff, a universal data generation framework for energy\ntime series data. EnergyDiff builds on state-of-the-art denoising diffusion\nprobabilistic models, utilizing a proposed denoising network dedicated to\nhigh-resolution time series data and introducing a novel Marginal Calibration\ntechnique. Our extensive experimental results demonstrate that EnergyDiff\nachieves significant improvement in capturing the temporal dependencies and\nmarginal distributions compared to baselines, particularly at the 1-minute\nresolution. EnergyDiff's universality is validated across diverse energy\ndomains (e.g., electricity demand, heat pump, PV, multiple time resolutions (1\nminute, 15 minutes, 30 minutes and 1 hour), and at both customer and\ntransformer levels.\n","authors":["Nan Lin","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2407.13538v3.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2506.15289v1","updated":"2025-06-18T09:15:18Z","published":"2025-06-18T09:15:18Z","title":"DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for\n  Optimizing Large-Scale EV Charging Infrastructure","summary":"  The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.\n","authors":["Chuan Li","Shunyu Zhao","Vincent Gauthier","Hassine Moungla"],"pdf_url":"https://arxiv.org/pdf/2506.15289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11093v2","updated":"2025-06-18T09:01:09Z","published":"2024-04-17T06:17:08Z","title":"Simulating Non-Markovian Open Quantum Dynamics with Neural Quantum\n  States","summary":"  Reducing computational scaling for simulating non-Markovian dissipative\ndynamics using artificial neural networks is both a major focus and formidable\nchallenge in open quantum systems. To enable neural quantum states (NQSs), we\nencode environmental memory in dissipatons (quasiparticles with characteristic\nlifetimes), yielding the dissipaton-embedded quantum master equation (DQME).\nThe resulting NQS-DQME framework achieves compact representation of many-body\ncorrelations and non-Markovian memory. Benchmarking against numerically exact\nhierarchical equations of motion confirms NQS-DQME maintains comparable\naccuracy while enhancing scalability and interpretability. This methodology\nopens new paths to explore non-Markovian open quantum dynamics in previously\nintractable systems.\n","authors":["Long Cao","Liwei Ge","Daochi Zhang","Xiang Li","Yao Wang","Rui-Xue Xu","YiJing Yan","Xiao Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.11093v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.03943v3","updated":"2025-06-18T08:58:45Z","published":"2024-10-04T22:00:13Z","title":"Oscillatory State-Space Models","summary":"  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba and LRU by nearly 2x on a sequence modeling task with\nsequences of length 50k.\n","authors":["T. Konstantin Rusch","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.03943v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2409.06525v3","updated":"2025-06-18T08:58:10Z","published":"2024-09-10T14:02:34Z","title":"MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based\n  Likelihood Estimation","summary":"  We introduce MENSA, a novel deep learning model for multi-event survival\nanalysis, which predicts the time until an instance experiences multiple\ndistinct events based on its features. MENSA learns a shared representation of\nthe input features while capturing the complex dependence structures between\nevents. In practice, it optimizes the sum of the traditional negative\nlog-likelihood across events and a novel trajectory-based likelihood, which\nencourages the model to learn the temporal order in which events occur.\nExperiments on real-world clinical datasets demonstrate that MENSA improves\nrisk and time-to-event prediction compared to state-of-the-art models across\nsingle-event, competing-risk, and multi-event settings. Moreover, MENSA\nachieves this with fewer parameters and lower computational cost (FLOPs) than\nseveral deep learning baselines, particularly in high-dimensional feature\nspaces (more than 100 features).\n","authors":["Christian Marius Lillelund","Ali Hossein Gharari Foomani","Weijie Sun","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2409.06525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05185v3","updated":"2025-06-18T08:55:23Z","published":"2024-04-08T04:22:55Z","title":"Convergence analysis of controlled particle systems arising in deep\n  learning: from finite to infinite sample size","summary":"  This paper deals with a class of neural SDEs and studies the limiting\nbehavior of the associated sampled optimal control problems as the sample size\ngrows to infinity. The neural SDEs with $N$ samples can be linked to the\n$N$-particle systems with centralized control. We analyze the\nHamilton-Jacobi-Bellman equation corresponding to the $N$-particle system and\nestablish regularity results which are uniform in $N$. The uniform regularity\nestimates are obtained by the stochastic maximum principle and the analysis of\na backward stochastic Riccati equation. Using these uniform regularity results,\nwe show the convergence of the minima of the objective functionals and optimal\nparameters of the neural SDEs as the sample size $N$ tends to infinity. The\nlimiting objects can be identified with suitable functions defined on the\nWasserstein space of Borel probability measures. Furthermore, quantitative\nconvergence rates are also obtained.\n","authors":["Huafu Liao","Alpár R. Mészáros","Chenchen Mou","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.05185v3.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2408.09194v2","updated":"2025-06-18T08:49:21Z","published":"2024-08-17T13:12:04Z","title":"DRL-Based Resource Allocation for Motion Blur Resistant Federated\n  Self-Supervised Learning in IoV","summary":"  In the Internet of Vehicles (IoV), Federated Learning (FL) provides a\nprivacy-preserving solution by aggregating local models without sharing data.\nTraditional supervised learning requires image data with labels, but data\nlabeling involves significant manual effort. Federated Self-Supervised Learning\n(FSSL) utilizes Self-Supervised Learning (SSL) for local training in FL,\neliminating the need for labels while protecting privacy. Compared to other SSL\nmethods, Momentum Contrast (MoCo) reduces the demand for computing resources\nand storage space by creating a dictionary. However, using MoCo in FSSL\nrequires uploading the local dictionary from vehicles to Base Station (BS),\nwhich poses a risk of privacy leakage. Simplified Contrast (SimCo) addresses\nthe privacy leakage issue in MoCo-based FSSL by using dual temperature instead\nof a dictionary to control sample distribution. Additionally, considering the\nnegative impact of motion blur on model aggregation, and based on SimCo, we\npropose a motion blur-resistant FSSL method, referred to as BFSSL. Furthermore,\nwe address energy consumption and delay in the BFSSL process by proposing a\nDeep Reinforcement Learning (DRL)-based resource allocation scheme, called\nDRL-BFSSL. In this scheme, BS allocates the Central Processing Unit (CPU)\nfrequency and transmission power of vehicles to minimize energy consumption and\nlatency, while aggregating received models based on the motion blur level.\nSimulation results validate the effectiveness of our proposed aggregation and\nresource allocation methods.\n","authors":["Xueying Gu","Qiong Wu","Pingyi Fan","Qiang Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2408.09194v2.pdf","comment":"This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released at: https://github.com/qiongwu86/DRL-BFSSL"},{"id":"http://arxiv.org/abs/2506.15271v1","updated":"2025-06-18T08:46:59Z","published":"2025-06-18T08:46:59Z","title":"Unlocking Post-hoc Dataset Inference with Synthetic Data","summary":"  The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.\n","authors":["Bihe Zhao","Pratyush Maini","Franziska Boenisch","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2506.15271v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2506.15264v1","updated":"2025-06-18T08:40:49Z","published":"2025-06-18T08:40:49Z","title":"Centroid Approximation for Byzantine-Tolerant Federated Learning","summary":"  Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.\n","authors":["Mélanie Cambus","Darya Melnyk","Tijana Milentijević","Stefan Schmid"],"pdf_url":"https://arxiv.org/pdf/2506.15264v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.13104v2","updated":"2025-06-18T08:40:30Z","published":"2024-11-20T07:59:35Z","title":"DRL-Based Optimization for AoI and Energy Consumption in C-V2X Enabled\n  IoV","summary":"  To address communication latency issues, the Third Generation Partnership\nProject (3GPP) has defined Cellular-Vehicle to Everything (C-V2X) technology,\nwhich includes Vehicle-to-Vehicle (V2V) communication for direct\nvehicle-to-vehicle communication. However, this method requires vehicles to\nautonomously select communication resources based on the Semi-Persistent\nScheduling (SPS) protocol, which may lead to collisions due to different\nvehicles sharing the same communication resources, thereby affecting\ncommunication effectiveness. Non-Orthogonal Multiple Access (NOMA) is\nconsidered a potential solution for handling large-scale vehicle communication,\nas it can enhance the Signal-to-Interference-plus-Noise Ratio (SINR) by\nemploying Successive Interference Cancellation (SIC), thereby reducing the\nnegative impact of communication collisions. When evaluating vehicle\ncommunication performance, traditional metrics such as reliability and\ntransmission delay present certain contradictions. Introducing the new metric\nAge of Information (AoI) provides a more comprehensive evaluation of\ncommunication system. Additionally, to ensure service quality, user terminals\nneed to possess high computational capabilities, which may lead to increased\nenergy consumption, necessitating a trade-off between communication energy\nconsumption and effectiveness. Given the complexity and dynamics of\ncommunication systems, Deep Reinforcement Learning (DRL) serves as an\nintelligent learning method capable of learning optimal strategies in dynamic\nenvironments. Therefore, this paper analyzes the effects of multi-priority\nqueues and NOMA on AoI in the C-V2X vehicular communication system and proposes\nan energy consumption and AoI optimization method based on DRL. Finally,\nthrough comparative simulations with baseline methods, the proposed approach\ndemonstrates its advances in terms of energy consumption and AoI.\n","authors":["Zheng Zhang","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2411.13104v2.pdf","comment":"This paper has been accepted by IEEE Transactions on Green\n  Communications and Networking. The source code has been released at:\n  https://github.com/qiongwu86/DRL-Based-Optimization-for-Information-of-Age-and-Energy-Consumption-in-C-V2X-Enabled-IoV"},{"id":"http://arxiv.org/abs/2506.15263v1","updated":"2025-06-18T08:40:22Z","published":"2025-06-18T08:40:22Z","title":"Minimizing Structural Vibrations via Guided Flow Matching Design\n  Optimization","summary":"  Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.\n","authors":["Jan van Delden","Julius Schultz","Sebastian Rothe","Christian Libner","Sabine C. Langer","Timo Lüddecke"],"pdf_url":"https://arxiv.org/pdf/2506.15263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04524v2","updated":"2025-06-18T08:39:46Z","published":"2025-04-06T15:48:26Z","title":"Trust Region Preference Approximation: A simple and stable reinforcement\n  learning algorithm for LLM reasoning","summary":"  Recently, Large Language Models (LLMs) have rapidly evolved, approaching\nArtificial General Intelligence (AGI) while benefiting from large-scale\nreinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent\nreward-based optimization algorithms, such as Proximal Policy Optimization\n(PPO) and Group Relative Policy Optimization (GRPO) have achieved significant\nperformance on reasoning tasks, whereas preference-based optimization\nalgorithms such as Direct Preference Optimization (DPO) significantly improve\nthe performance of LLMs on human alignment. However, despite the strong\nperformance of reward-based optimization methods in alignment tasks , they\nremain vulnerable to reward hacking. Furthermore, preference-based algorithms\n(such as Online DPO) haven't yet matched the performance of reward-based\noptimization algorithms (like PPO) on reasoning tasks, making their exploration\nin this specific area still a worthwhile pursuit. Motivated by these\nchallenges, we propose the Trust Region Preference Approximation (TRPA)\nalgorithm, which integrates rule-based optimization with preference-based\noptimization for reasoning tasks. As a preference-based algorithm, TRPA\nnaturally eliminates the reward hacking issue. TRPA constructs preference\nlevels using predefined rules, forms corresponding preference pairs, and\nleverages a novel optimization algorithm for RL training with a theoretical\nmonotonic improvement guarantee. Experimental results demonstrate that TRPA not\nonly achieves competitive performance on reasoning tasks but also exhibits\nrobust stability. The code of this paper are released and updating on\nhttps://github.com/XueruiSu/Trust-Region-Preference-Approximation.git.\n","authors":["Xuerui Su","Shufang Xie","Guoqing Liu","Yingce Xia","Renqian Luo","Peiran Jin","Zhiming Ma","Yue Wang","Zun Wang","Yuting Liu"],"pdf_url":"https://arxiv.org/pdf/2504.04524v2.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2506.14665v2","updated":"2025-06-18T08:39:15Z","published":"2025-06-17T15:56:56Z","title":"Accurate and scalable exchange-correlation with deep learning","summary":"  Density Functional Theory (DFT) is the most widely used electronic structure\nmethod for predicting the properties of molecules and materials. Although DFT\nis, in principle, an exact reformulation of the Schr\\\"odinger equation,\npractical applications rely on approximations to the unknown\nexchange-correlation (XC) functional. Most existing XC functionals are\nconstructed using a limited set of increasingly complex, hand-crafted features\nthat improve accuracy at the expense of computational efficiency. Yet, no\ncurrent approximation achieves the accuracy and generality for predictive\nmodeling of laboratory experiments at chemical accuracy -- typically defined as\nerrors below 1 kcal/mol. In this work, we present Skala, a modern deep\nlearning-based XC functional that bypasses expensive hand-designed features by\nlearning representations directly from data. Skala achieves chemical accuracy\nfor atomization energies of small molecules while retaining the computational\nefficiency typical of semi-local DFT. This performance is enabled by training\non an unprecedented volume of high-accuracy reference data generated using\ncomputationally intensive wavefunction-based methods. Notably, Skala\nsystematically improves with additional training data covering diverse\nchemistry. By incorporating a modest amount of additional high-accuracy data\ntailored to chemistry beyond atomization energies, Skala achieves accuracy\ncompetitive with the best-performing hybrid functionals across general main\ngroup chemistry, at the cost of semi-local DFT. As the training dataset\ncontinues to expand, Skala is poised to further enhance the predictive power of\nfirst-principles simulations.\n","authors":["Giulia Luise","Chin-Wei Huang","Thijs Vogels","Derk P. Kooi","Sebastian Ehlert","Stephanie Lanius","Klaas J. H. Giesbertz","Amir Karton","Deniz Gunceler","Megan Stanley","Wessel P. Bruinsma","Lin Huang","Xinran Wei","José Garrido Torres","Abylay Katbashev","Bálint Máté","Sékou-Oumar Kaba","Roberto Sordillo","Yingrong Chen","David B. Williams-Young","Christopher M. Bishop","Jan Hermann","Rianne van den Berg","Paola Gori-Giorgi"],"pdf_url":"https://arxiv.org/pdf/2506.14665v2.pdf","comment":"Main: 13 pages plus references, 11 figures and tables. Supplementary\n  information: 19 pages, 12 figures and tables. v2 update: fix rendering of\n  figure 1 and part of figure 5 in Safari PDF viewer"},{"id":"http://arxiv.org/abs/2411.19479v2","updated":"2025-06-18T08:32:27Z","published":"2024-11-29T05:34:21Z","title":"FLARE: Towards Universal Dataset Purification against Backdoor Attacks","summary":"  Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison datasets with adversary-specified triggers to implant hidden\nbackdoors, enabling malicious manipulation of model predictions. Dataset\npurification serves as a proactive defense by removing malicious training\nsamples to prevent backdoor injection at its source. We first reveal that the\ncurrent advanced purification methods rely on a latent assumption that the\nbackdoor connections between triggers and target labels in backdoor attacks are\nsimpler to learn than the benign features. We demonstrate that this assumption,\nhowever, does not always hold, especially in all-to-all (A2A) and untargeted\n(UT) attacks. As a result, purification methods that analyze the separation\nbetween the poisoned and benign samples in the input-output space or the final\nhidden layer space are less effective. We observe that this separability is not\nconfined to a single layer but varies across different hidden layers. Motivated\nby this understanding, we propose FLARE, a universal purification method to\ncounter various backdoor attacks. FLARE aggregates abnormal activations from\nall hidden layers to construct representations for clustering. To enhance\nseparation, FLARE develops an adaptive subspace selection algorithm to isolate\nthe optimal space for dividing an entire dataset into two clusters. FLARE\nassesses the stability of each cluster and identifies the cluster with higher\nstability as poisoned. Extensive evaluations on benchmark datasets demonstrate\nthe effectiveness of FLARE against 22 representative backdoor attacks,\nincluding all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and\nits robustness to adaptive attacks. Codes are available at\n\\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and\n\\href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.\n","authors":["Linshan Hou","Wei Luo","Zhongyun Hua","Songhua Chen","Leo Yu Zhang","Yiming Li"],"pdf_url":"https://arxiv.org/pdf/2411.19479v2.pdf","comment":"15 pages, This paper is accepted and will appear in TIFS (CCF-A)"},{"id":"http://arxiv.org/abs/2506.15251v1","updated":"2025-06-18T08:28:53Z","published":"2025-06-18T08:28:53Z","title":"Singular Value Decomposition on Kronecker Adaptation for Large Language\n  Model","summary":"  Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.\n","authors":["Yee Hin Chong","Peng Qu"],"pdf_url":"https://arxiv.org/pdf/2506.15251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15249v1","updated":"2025-06-18T08:26:30Z","published":"2025-06-18T08:26:30Z","title":"Context-Aware Deep Lagrangian Networks for Model Predictive Control","summary":"  Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.\n","authors":["Lucas Schulze","Jan Peters","Oleg Arenz"],"pdf_url":"https://arxiv.org/pdf/2506.15249v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2504.08200v2","updated":"2025-06-18T08:25:00Z","published":"2025-04-11T02:05:51Z","title":"Influential Bandits: Pulling an Arm May Change the Environment","summary":"  While classical formulations of multi-armed bandit problems assume that each\narm's reward is independent and stationary, real-world applications often\ninvolve non-stationary environments and interdependencies between arms. In\nparticular, selecting one arm may influence the future rewards of other arms, a\nscenario not adequately captured by existing models such as rotting bandits or\nrestless bandits. To address this limitation, we propose the influential bandit\nproblem, which models inter-arm interactions through an unknown, symmetric,\npositive semi-definite interaction matrix that governs the dynamics of arm\nlosses. We formally define this problem and establish two regret lower bounds,\nincluding a superlinear $\\Omega(T^2 / \\log^2 T)$ bound for the standard LCB\nalgorithm (loss minimization version of UCB) and an algorithm-independent\n$\\Omega(T)$ bound, which highlight the inherent difficulty of the setting. We\nthen introduce a new algorithm based on a lower confidence bound (LCB)\nestimator tailored to the structure of the loss dynamics. Under mild\nassumptions, our algorithm achieves a regret of $O(KT \\log T)$, which is nearly\noptimal in terms of its dependence on the time horizon. The algorithm is simple\nto implement and computationally efficient. Empirical evaluations on both\nsynthetic and real-world datasets demonstrate the presence of inter-arm\ninfluence and confirm the superior performance of our method compared to\nconventional bandit algorithms.\n","authors":["Ryoma Sato","Shinji Ito"],"pdf_url":"https://arxiv.org/pdf/2504.08200v2.pdf","comment":"TMLR"},{"id":"http://arxiv.org/abs/2506.13584v2","updated":"2025-06-18T08:20:25Z","published":"2025-06-16T15:07:44Z","title":"From Data-Driven to Purpose-Driven Artificial Intelligence: Systems\n  Thinking for Data-Analytic Automation of Patient Care","summary":"  In this work, we reflect on the data-driven modeling paradigm that is gaining\nground in AI-driven automation of patient care. We argue that the repurposing\nof existing real-world patient datasets for machine learning may not always\nrepresent an optimal approach to model development as it could lead to\nundesirable outcomes in patient care. We reflect on the history of data\nanalysis to explain how the data-driven paradigm rose to popularity, and we\nenvision ways in which systems thinking and clinical domain theory could\ncomplement the existing model development approaches in reaching human-centric\noutcomes. We call for a purpose-driven machine learning paradigm that is\ngrounded in clinical theory and the sociotechnical realities of real-world\noperational contexts. We argue that understanding the utility of existing\npatient datasets requires looking in two directions: upstream towards the data\ngeneration, and downstream towards the automation objectives. This\npurpose-driven perspective to AI system development opens up new methodological\nopportunities and holds promise for AI automation of patient care.\n","authors":["Daniel Anadria","Roel Dobbe","Anastasia Giachanou","Ruurd Kuiper","Richard Bartels","Wouter van Amsterdam","Íñigo Martínez de Rituerto de Troya","Carmen Zürcher","Daniel Oberski"],"pdf_url":"https://arxiv.org/pdf/2506.13584v2.pdf","comment":"The work is under review at ACM Health"},{"id":"http://arxiv.org/abs/2504.05716v3","updated":"2025-06-18T08:17:16Z","published":"2025-04-08T06:34:15Z","title":"Single-Agent vs. Multi-Agent LLM Strategies for Automated Student\n  Reflection Assessment","summary":"  We explore the use of Large Language Models (LLMs) for automated assessment\nof open-text student reflections and prediction of academic performance.\nTraditional methods for evaluating reflections are time-consuming and may not\nscale effectively in educational settings. In this work, we employ LLMs to\ntransform student reflections into quantitative scores using two assessment\nstrategies (single-agent and multi-agent) and two prompting techniques\n(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278\nreflections from 377 students over three academic terms, demonstrate that the\nsingle-agent with few-shot strategy achieves the highest match rate with human\nevaluations. Furthermore, models utilizing LLM-assessed reflection scores\noutperform baselines in both at-risk student identification and grade\nprediction tasks. These findings suggest that LLMs can effectively automate\nreflection assessment, reduce educators' workload, and enable timely support\nfor students who may need additional assistance. Our work emphasizes the\npotential of integrating advanced generative AI technologies into educational\npractices to enhance student engagement and academic success.\n","authors":["Gen Li","Li Chen","Cheng Tang","Valdemar Švábenský","Daisuke Deguchi","Takayoshi Yamashita","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2504.05716v3.pdf","comment":"Published in Proceedings of the 29th Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2025), see\n  https://doi.org/10.1007/978-981-96-8186-0_24"},{"id":"http://arxiv.org/abs/2408.05249v2","updated":"2025-06-18T07:45:09Z","published":"2024-08-08T14:36:16Z","title":"Advancing oncology with federated learning: transcending boundaries in\n  breast, lung, and prostate cancer. A systematic review","summary":"  Federated Learning (FL) has emerged as a promising solution to address the\nlimitations of centralised machine learning (ML) in oncology, particularly in\novercoming privacy concerns and harnessing the power of diverse, multi-center\ndata. This systematic review synthesises current knowledge on the\nstate-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer.\nDistinct from previous surveys, our comprehensive review critically evaluates\nthe real-world implementation and impact of FL on cancer care, demonstrating\nits effectiveness in enhancing ML generalisability, performance and data\nprivacy in clinical settings and data. We evaluated state-of-the-art advances\nin FL, demonstrating its growing adoption amid tightening data privacy\nregulations. FL outperformed centralised ML in 15 out of the 25 studies\nreviewed, spanning diverse ML models and clinical applications, and\nfacilitating integration of multi-modal information for precision medicine.\nDespite the current challenges identified in reproducibility, standardisation\nand methodology across studies, the demonstrable benefits of FL in harnessing\nreal-world data and addressing clinical needs highlight its significant\npotential for advancing cancer research. We propose that future research should\nfocus on addressing these limitations and investigating further advanced FL\nmethods, to fully harness data diversity and realise the transformative power\nof cutting-edge FL in cancer care.\n","authors":["Anshu Ankolekar","Sebastian Boie","Maryam Abdollahyan","Emanuela Gadaleta","Seyed Alireza Hasheminasab","Guang Yang","Charles Beauville","Nikolaos Dikaios","George Anthony Kastis","Michael Bussmann","Sara Khalid","Hagen Kruger","Philippe Lambin","Giorgos Papanastasiou"],"pdf_url":"https://arxiv.org/pdf/2408.05249v2.pdf","comment":"5 Figures, 3 Tables, 1 Supplementary Table"},{"id":"http://arxiv.org/abs/2111.07243v3","updated":"2025-06-18T07:35:46Z","published":"2021-11-14T05:18:31Z","title":"Simulating Diffusion Bridges with Score Matching","summary":"  We consider the problem of simulating diffusion bridges, which are diffusion\nprocesses that are conditioned to initialize and terminate at two given states.\nThe simulation of diffusion bridges has applications in diverse scientific\nfields and plays a crucial role in the statistical inference of\ndiscretely-observed diffusions. This is known to be a challenging problem that\nhas received much attention in the last two decades. This article contributes\nto this rich body of literature by presenting a new avenue to obtain diffusion\nbridge approximations. Our approach is based on a backward time representation\nof a diffusion bridge, which may be simulated if one can time-reverse the\nunconditioned diffusion. We introduce a variational formulation to learn this\ntime-reversal with function approximation and rely on a score matching method\nto circumvent intractability. Another iteration of our proposed methodology\napproximates the Doob's $h$-transform defining the forward time representation\nof a diffusion bridge. We discuss algorithmic considerations and extensions,\nand present numerical results on an Ornstein--Uhlenbeck process, a model from\nfinancial econometrics for interest rates, and a model from genetics for cell\ndifferentiation and development to illustrate the effectiveness of our\napproach.\n","authors":["Jeremy Heng","Valentin De Bortoli","Arnaud Doucet","James Thornton"],"pdf_url":"https://arxiv.org/pdf/2111.07243v3.pdf","comment":"Revised"},{"id":"http://arxiv.org/abs/2506.15199v1","updated":"2025-06-18T07:25:09Z","published":"2025-06-18T07:25:09Z","title":"Interpretability and Generalization Bounds for Learning Spatial Physics","summary":"  While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.\n","authors":["Alejandro Francisco Queiruga","Theo Gutman-Solo","Shuai Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.15199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20606v3","updated":"2025-06-18T07:21:56Z","published":"2025-02-28T00:10:52Z","title":"Map Space Belief Prediction for Manipulation-Enhanced Mapping","summary":"  Searching for objects in cluttered environments requires selecting efficient\nviewpoints and manipulation actions to remove occlusions and reduce uncertainty\nin object locations, shapes, and categories. In this work, we address the\nproblem of manipulation-enhanced semantic mapping, where a robot has to\nefficiently identify all objects in a cluttered shelf. Although Partially\nObservable Markov Decision Processes~(POMDPs) are standard for decision-making\nunder uncertainty, representing unstructured interactive worlds remains\nchallenging in this formalism. To tackle this, we define a POMDP whose belief\nis summarized by a metric-semantic grid map and propose a novel framework that\nuses neural networks to perform map-space belief updates to reason efficiently\nand simultaneously about object geometries, locations, categories, occlusions,\nand manipulation physics. Further, to enable accurate information gain\nanalysis, the learned belief updates should maintain calibrated estimates of\nuncertainty. Therefore, we propose Calibrated Neural-Accelerated Belief Updates\n(CNABUs) to learn a belief propagation model that generalizes to novel\nscenarios and provides confidence-calibrated predictions for unknown areas. Our\nexperiments show that our novel POMDP planner improves map completeness and\naccuracy over existing methods in challenging simulations and successfully\ntransfers to real-world cluttered shelves in zero-shot fashion.\n","authors":["Joao Marcos Correia Marques","Nils Dengler","Tobias Zaenker","Jesper Mucke","Shenlong Wang","Maren Bennewitz","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2502.20606v3.pdf","comment":"14 pages, 10 figures; Published at RSS 2025 - this version contains a\n  small fix to figure 6 which was missing a plot in the original submission"},{"id":"http://arxiv.org/abs/2303.17992v3","updated":"2025-06-18T07:19:17Z","published":"2023-03-31T12:09:36Z","title":"A Second-Order Majorant Algorithm for Nonnegative Matrix Factorization","summary":"  Nonnegative Matrix Factorization (NMF) is a fundamental tool in unsupervised\nlearning, widely used for tasks such as dimensionality reduction, feature\nextraction, representation learning, and topic modeling. Many algorithms have\nbeen developed for NMF, including the well-known Multiplicative Updates (MU)\nalgorithm, which belongs to a broader class of majorization-minimization\ntechniques. In this work, we introduce a general second-order optimization\nframework for NMF under both quadratic and $\\beta$-divergence loss functions.\nThis approach, called Second-Order Majorant (SOM), constructs a local quadratic\nmajorization of the loss function by majorizing its Hessian matrix. It includes\nMU as a special case, while enabling faster variants. In particular, we propose\nmSOM, a new algorithm within this class that leverages a tighter local\napproximation to accelerate convergence. We provide a convergence analysis,\nshowing linear convergence for individual factor updates and global convergence\nto a stationary point for the alternating version, AmSOM algorithm. Numerical\nexperiments on both synthetic and real data sets demonstrate that mSOM\nconsistently outperforms state-of-the-art algorithms across multiple loss\nfunctions.\n","authors":["Mai-Quyen Pham","Jérémy Cohen","Thierry Chonavel"],"pdf_url":"https://arxiv.org/pdf/2303.17992v3.pdf","comment":"Updated version in JMLR style. This version matches the manuscript\n  currently under review at JMLR and includes substantial improvements over the\n  original arXiv version"},{"id":"http://arxiv.org/abs/2506.15190v1","updated":"2025-06-18T07:11:48Z","published":"2025-06-18T07:11:48Z","title":"Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal\n  Behaviors","summary":"  Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.\n","authors":["Jiyi Wang","Jingyang Ke","Bo Dai","Anqi Wu"],"pdf_url":"https://arxiv.org/pdf/2506.15190v1.pdf","comment":"9 pages and 4 figures for the main text"},{"id":"http://arxiv.org/abs/2506.15182v1","updated":"2025-06-18T06:55:38Z","published":"2025-06-18T06:55:38Z","title":"Classification of Multi-Parametric Body MRI Series Using Deep Learning","summary":"  Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.\n","authors":["Boah Kim","Tejas Sudharshan Mathai","Kimberly Helm","Peter A. Pinto","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2506.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15181v1","updated":"2025-06-18T06:53:52Z","published":"2025-06-18T06:53:52Z","title":"ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed\n  Machine Learning","summary":"  Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.\n","authors":["Bing Liu","Chengcheng Zhao","Li Chai","Peng Cheng","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2506.15181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00712v8","updated":"2025-06-18T06:53:36Z","published":"2023-01-02T15:09:12Z","title":"On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness\n  Results and Improved Analysis","summary":"  Bilevel optimization reveals the inner structure of otherwise oblique\noptimization problems, such as hyperparameter tuning, neural architecture\nsearch, and meta-learning. A common goal in bilevel optimization is to minimize\na hyper-objective that implicitly depends on the solution set of the\nlower-level function. Although this hyper-objective approach is widely used,\nits theoretical properties have not been thoroughly investigated in cases where\nthe lower-level functions lack strong convexity. In this work, we first provide\nhardness results to show that the goal of finding stationary points of the\nhyper-objective for nonconvex-convex bilevel optimization can be intractable\nfor zero-respecting algorithms. Then we study a class of tractable\nnonconvex-nonconvex bilevel problems when the lower-level function satisfies\nthe Polyak-{\\L}ojasiewicz (PL) condition. We show a simple first-order\nalgorithm can achieve better complexity bounds of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$, $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ and\n$\\tilde{\\mathcal{O}}(\\epsilon^{-6})$ in the deterministic, partially\nstochastic, and fully stochastic setting respectively. The complexities in the\nfirst two cases are optimal up to logarithmic factors.\n","authors":["Lesi Chen","Jing Xu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.00712v8.pdf","comment":"Published in COLT 2024. This arXiv version refines Assumption 4.1\n  (d); adds discussions on related works in Appendix A; and corrects the kappa\n  dependency in the upper bounds"},{"id":"http://arxiv.org/abs/2502.02834v3","updated":"2025-06-18T06:52:53Z","published":"2025-02-05T02:31:50Z","title":"Task-Aware Virtual Training: Enhancing Generalization in\n  Meta-Reinforcement Learning for Out-of-Distribution Tasks","summary":"  Meta reinforcement learning aims to develop policies that generalize to\nunseen tasks sampled from a task distribution. While context-based meta-RL\nmethods improve task representation using task latents, they often struggle\nwith out-of-distribution (OOD) tasks. To address this, we propose Task-Aware\nVirtual Training (TAVT), a novel algorithm that accurately captures task\ncharacteristics for both training and OOD scenarios using metric-based\nrepresentation learning. Our method successfully preserves task characteristics\nin virtual tasks and employs a state regularization technique to mitigate\noverestimation errors in state-varying environments. Numerical results\ndemonstrate that TAVT significantly enhances generalization to OOD tasks across\nvarious MuJoCo and MetaWorld environments. Our code is available at\nhttps://github.com/JM-Kim-94/tavt.git.\n","authors":["Jeongmo Kim","Yisak Park","Minung Kim","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02834v3.pdf","comment":"9 pages main paper, 20 pages appendices with reference. Accepted to\n  ICML 2025"},{"id":"http://arxiv.org/abs/2506.14673v2","updated":"2025-06-18T06:49:11Z","published":"2025-06-17T16:07:36Z","title":"Uniform Mean Estimation for Heavy-Tailed Distributions via\n  Median-of-Means","summary":"  The Median of Means (MoM) is a mean estimator that has gained popularity in\nthe context of heavy-tailed data. In this work, we analyze its performance in\nthe task of simultaneously estimating the mean of each function in a class\n$\\mathcal{F}$ when the data distribution possesses only the first $p$ moments\nfor $p \\in (1,2]$. We prove a new sample complexity bound using a novel\nsymmetrization technique that may be of independent interest. Additionally, we\npresent applications of our result to $k$-means clustering with unbounded\ninputs and linear regression with general losses, improving upon existing\nworks.\n","authors":["Mikael Møller Høgsgaard","Andrea Paudice"],"pdf_url":"https://arxiv.org/pdf/2506.14673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02844v3","updated":"2025-06-18T06:49:09Z","published":"2025-02-05T02:59:23Z","title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement\n  Learning","summary":"  Traditional robust methods in multi-agent reinforcement learning (MARL) often\nstruggle against coordinated adversarial attacks in cooperative scenarios. To\naddress this limitation, we propose the Wolfpack Adversarial Attack framework,\ninspired by wolf hunting strategies, which targets an initial agent and its\nassisting agents to disrupt cooperation. Additionally, we introduce the\nWolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust\nMARL policies to defend against the proposed Wolfpack attack by fostering\nsystemwide collaboration. Experimental results underscore the devastating\nimpact of the Wolfpack attack and the significant robustness improvements\nachieved by WALL. Our code is available at\nhttps://github.com/sunwoolee0504/WALL.\n","authors":["Sunwoo Lee","Jaebak Hwang","Yonghyeon Jo","Seungyul Han"],"pdf_url":"https://arxiv.org/pdf/2502.02844v3.pdf","comment":"9 pages main, 23 pages appendix with reference. Accepeted by ICML\n  2025"},{"id":"http://arxiv.org/abs/2506.15176v1","updated":"2025-06-18T06:43:55Z","published":"2025-06-18T06:43:55Z","title":"In-Context Learning for Gradient-Free Receiver Adaptation: Principles,\n  Applications, and Theory","summary":"  In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.\n","authors":["Matteo Zecchin","Tomer Raviv","Dileep Kalathil","Krishna Narayanan","Nir Shlezinger","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2506.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07765v2","updated":"2025-06-18T06:35:16Z","published":"2025-05-12T17:12:53Z","title":"Solving Nonlinear PDEs with Sparse Radial Basis Function Networks","summary":"  We propose a novel framework for solving nonlinear PDEs using sparse radial\nbasis function (RBF) networks. Sparsity-promoting regularization is employed to\nprevent over-parameterization and reduce redundant features. This work is\nmotivated by longstanding challenges in traditional RBF collocation methods,\nalong with the limitations of physics-informed neural networks (PINNs) and\nGaussian process (GP) approaches, aiming to blend their respective strengths in\na unified framework. The theoretical foundation of our approach lies in the\nfunction space of Reproducing Kernel Banach Spaces (RKBS) induced by\none-hidden-layer neural networks of possibly infinite width. We prove a\nrepresenter theorem showing that the sparse optimization problem in the RKBS\nadmits a finite solution and establishes error bounds that offer a foundation\nfor generalizing classical numerical analysis. The algorithmic framework is\nbased on a three-phase algorithm to maintain computational efficiency through\nadaptive feature selection, second-order optimization, and pruning of inactive\nneurons. Numerical experiments demonstrate the effectiveness of our method and\nhighlight cases where it offers notable advantages over GP approaches. This\nwork opens new directions for adaptive PDE solvers grounded in rigorous\nanalysis with efficient, learning-inspired implementation.\n","authors":["Zihan Shao","Konstantin Pieper","Xiaochuan Tian"],"pdf_url":"https://arxiv.org/pdf/2505.07765v2.pdf","comment":"51 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.00396v2","updated":"2025-06-18T06:30:20Z","published":"2024-10-01T04:39:04Z","title":"Dynamic neuron approach to deep neural networks: Decoupling neurons for\n  renormalization group analysis","summary":"  Deep neural network architectures often consist of repetitive structural\nelements. We introduce an approach that reveals these patterns and can be\nbroadly applied to the study of deep learning. Similarly to how a power strip\nhelps untangle and organize complex cable connections, this approach treats\nneurons as additional degrees of freedom in interactions, simplifying the\nstructure and enhancing the intuitive understanding of interactions within deep\nneural networks. Furthermore, it reveals the translational symmetry of deep\nneural networks, which simplifies the application of the renormalization group\ntransformation-a method that effectively analyzes the scaling behavior of the\nsystem. By utilizing translational symmetry and renormalization group\ntransformations, we can analyze critical phenomena. This approach may open new\navenues for studying deep neural networks using statistical physics.\n","authors":["Donghee Lee","Hye-Sung Lee","Jaeok Yi"],"pdf_url":"https://arxiv.org/pdf/2410.00396v2.pdf","comment":"Version matching the publication"},{"id":"http://arxiv.org/abs/2411.11171v5","updated":"2025-06-18T06:29:57Z","published":"2024-11-17T20:44:34Z","title":"LLäMmlein: Transparent, Compact and Competitive German-Only Language\n  Models from Scratch","summary":"  We create two German-only decoder models, LL\\\"aMmlein 120M and 1B,\ntransparently from scratch and publish them, along with the training data, for\nthe German NLP research community to use. The model training involved several\nkey steps, including extensive data preprocessing, the creation of a custom\nGerman tokenizer, the training itself, as well as the evaluation of the final\nmodels on various benchmarks. Throughout the training process, multiple\ncheckpoints were saved and analyzed using the SuperGLEBer benchmark to monitor\nthe models' learning dynamics. Compared to state-of-the-art models on the\nSuperGLEBer benchmark, both LL\\\"aMmlein models performed competitively,\nconsistently matching or surpassing models with similar parameter sizes. The\nresults show that the models' quality scales with size as expected, but\nperformance improvements on some tasks plateaued early, offering valuable\ninsights into resource allocation for future model development.\n","authors":["Jan Pfister","Julia Wunderle","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2411.11171v5.pdf","comment":"camera ready @ACL25;\n  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/"}]},"2025-06-17T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2504.17918v3","updated":"2025-06-17T21:43:18Z","published":"2025-04-24T20:19:07Z","title":"PHast -- Perfect Hashing with fast evaluation","summary":"  Perfect hash functions give unique \"names\" to arbitrary keys requiring only a\nfew bits per key. This is an essential building block in applications like\nstatic hash tables, databases, or bioinformatics. This paper introduces the\nPHast approach that has the currently fastest query time with competitive\nconstruction time and space consumption. PHast improves bucket-placement which\nfirst hashes each key k to a bucket, and then looks for the bucket seed s such\nthat a secondary hash function maps pairs (s,k) in a collision-free way. PHast\ncan use small-range primary hash functions with linear mapping, fixed-width\nencoding of seeds, and parallel construction. This is achieved using small\noverlapping slices of allowed values and bumping to handle unsuccessful seed\nassignment.\n","authors":["Piotr Beling","Peter Sanders"],"pdf_url":"https://arxiv.org/pdf/2504.17918v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14707v1","updated":"2025-06-17T16:45:42Z","published":"2025-06-17T16:45:42Z","title":"HARMONY: A Scalable Distributed Vector Database for High-Throughput\n  Approximate Nearest Neighbor Search","summary":"  Approximate Nearest Neighbor Search (ANNS) is essential for various\ndata-intensive applications, including recommendation systems, image retrieval,\nand machine learning. Scaling ANNS to handle billions of high-dimensional\nvectors on a single machine presents significant challenges in memory capacity\nand processing efficiency. To address these challenges, distributed vector\ndatabases leverage multiple nodes for the parallel storage and processing of\nvectors. However, existing solutions often suffer from load imbalance and high\ncommunication overhead, primarily due to traditional partition strategies that\nfail to effectively distribute the workload. In this paper, we introduce\nHarmony, a distributed ANNS system that employs a novel multi-granularity\npartition strategy, combining dimension-based and vector-based partition. This\nstrategy ensures a balanced distribution of computational load across all nodes\nwhile effectively minimizing communication costs. Furthermore, Harmony\nincorporates an early-stop pruning mechanism that leverages the monotonicity of\ndistance computations in dimension-based partition, resulting in significant\nreductions in both computational and communication overhead. We conducted\nextensive experiments on diverse real-world datasets, demonstrating that\nHarmony outperforms leading distributed vector databases, achieving 4.63 times\nthroughput on average in four nodes and 58% performance improvement over\ntraditional distribution for skewed workloads.\n","authors":["Qian Xu","Feng Zhang","Chengxi Li","Lei Cao","Zheng Chen","Jidong Zhai","Xiaoyong Du"],"pdf_url":"https://arxiv.org/pdf/2506.14707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14661v2","updated":"2025-06-17T15:45:12Z","published":"2025-05-20T17:49:46Z","title":"Abacus: A Cost-Based Optimizer for Semantic Operator Systems","summary":"  LLMs enable an exciting new class of data processing applications over large\ncollections of unstructured documents. Several new programming frameworks have\nenabled developers to build these applications by composing them out of\nsemantic operators: a declarative set of AI-powered data transformations with\nnatural language specifications. These include LLM-powered maps, filters,\njoins, etc. used for document processing tasks such as information extraction,\nsummarization, and more. While systems of semantic operators have achieved\nstrong performance on benchmarks, they can be difficult to optimize. An\noptimizer for this setting must determine how to physically implement each\nsemantic operator in a way that optimizes the system globally. Existing\noptimizers are limited in the number of optimizations they can apply, and most\n(if not all) cannot optimize system quality, cost, or latency subject to\nconstraint(s) on the other dimensions. In this paper we present Abacus, an\nextensible, cost-based optimizer which searches for the best implementation of\na semantic operator system given a (possibly constrained) optimization\nobjective. Abacus estimates operator performance by leveraging a minimal set of\nvalidation examples and, if available, prior beliefs about operator\nperformance. We evaluate Abacus on document processing workloads in the\nbiomedical and legal domains (BioDEX; CUAD) and multi-modal question answering\n(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%\nbetter quality and up to 23.6x lower cost and 4.2x lower latency than the next\nbest system.\n","authors":["Matthew Russo","Sivaprasad Sudhir","Gerardo Vitagliano","Chunwei Liu","Tim Kraska","Samuel Madden","Michael Cafarella"],"pdf_url":"https://arxiv.org/pdf/2505.14661v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14630v1","updated":"2025-06-17T15:25:11Z","published":"2025-06-17T15:25:11Z","title":"Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)","summary":"  We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.\n","authors":["Rúben Adão","Zhongjie Wu","Changjun Zhou","Oana Balmau","João Paulo","Ricardo Macedo"],"pdf_url":"https://arxiv.org/pdf/2506.14630v1.pdf","comment":"This is an extended version of the full paper to appear in VLDB 2025"},{"id":"http://arxiv.org/abs/2412.02448v2","updated":"2025-06-17T09:24:55Z","published":"2024-12-03T13:32:11Z","title":"UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors\n  Search","summary":"  This paper presents an efficient and scalable framework for Range Filtered\nApproximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors\nassociated with attribute values. Given a query vector $q$ and a range $[l,\nh]$, RF-ANNS aims to find the approximate $k$ nearest neighbors of $q$ among\ndata whose attribute values fall within $[l, h]$. Existing methods including\npre-, post-, and hybrid filtering strategies that perform attribute range\nfiltering before, after, or during the ANNS process, all suffer from\nsignificant performance degradation when query ranges shift. Though building\ndedicated indexes for each strategy and selecting the best one based on the\nquery range can address this problem, it leads to index consistency and\nmaintenance issues.\n  Our framework, called UNIFY, constructs a unified Proximity Graph-based\n(PG-based) index that seamlessly supports all three strategies. In UNIFY, we\nintroduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by\nattribute values. It ensures the PG of objects from any segment combinations is\na sub-graph of SIG, thereby enabling efficient hybrid filtering by\nreconstructing and searching a PG from relevant segments. Moreover, we present\nHierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which\nincorporates a hierarchical structure inspired by HNSW to achieve logarithmic\nhybrid filtering complexity. We also implement pre- and post-filtering for HSIG\nby fusing skip list connections and compressed HNSW edges into the hierarchical\ngraph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS\nperformance across small, mid, and large query ranges.\n","authors":["Anqi Liang","Pengcheng Zhang","Bin Yao","Zhongpu Chen","Yitong Song","Guangxu Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.02448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08194v2","updated":"2025-06-17T08:36:16Z","published":"2024-12-11T08:35:56Z","title":"Magneto: Combining Small and Large Language Models for Schema Matching","summary":"  Recent advances in language models opened new opportunities to address\ncomplex schema matching tasks. Schema matching approaches have been proposed\nthat demonstrate the usefulness of language models, but they have also\nuncovered important limitations: Small language models (SLMs) require training\ndata (which can be both expensive and challenging to obtain), and large\nlanguage models (LLMs) often incur high computational costs and must deal with\nconstraints imposed by context windows. We present Magneto, a cost-effective\nand accurate solution for schema matching that combines the advantages of SLMs\nand LLMs to address their limitations. By structuring the schema matching\npipeline in two phases, retrieval and reranking, Magneto can use\ncomputationally efficient SLM-based strategies to derive candidate matches\nwhich can then be reranked by LLMs, thus making it possible to reduce runtime\nwithout compromising matching accuracy. We propose a self-supervised approach\nto fine-tune SLMs which uses LLMs to generate syntactically diverse training\ndata, and prompting strategies that are effective for reranking. We also\nintroduce a new benchmark, developed in collaboration with domain experts,\nwhich includes real biomedical datasets and presents new challenges to schema\nmatching methods. Through a detailed experimental evaluation, using both our\nnew and existing benchmarks, we show that Magneto is scalable and attains high\naccuracy for datasets from different domains.\n","authors":["Yurong Liu","Eduardo Pena","Aecio Santos","Eden Wu","Juliana Freire"],"pdf_url":"https://arxiv.org/pdf/2412.08194v2.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2502.21266v2","updated":"2025-06-17T23:23:33Z","published":"2025-02-28T17:42:58Z","title":"Supporting the development of Machine Learning for fundamental science\n  in a federated Cloud with the AI_INFN platform","summary":"  Machine Learning (ML) is driving a revolution in the way scientists design,\ndevelop, and deploy data-intensive software. However, the adoption of ML\npresents new challenges for the computing infrastructure, particularly in terms\nof provisioning and orchestrating access to hardware accelerators for\ndevelopment, testing, and production. The INFN-funded project AI_INFN\n(\"Artificial Intelligence at INFN\") aims at fostering the adoption of ML\ntechniques within INFN use cases by providing support on multiple aspects,\nincluding the provision of AI-tailored computing resources. It leverages\ncloud-native solutions in the context of INFN Cloud, to share hardware\naccelerators as effectively as possible, ensuring the diversity of the\nInstitute's research activities is not compromised. In this contribution, we\nprovide an update on the commissioning of a Kubernetes platform designed to\nease the development of GPU-powered data analysis workflows and their\nscalability on heterogeneous, distributed computing resources, possibly\nfederated as Virtual Kubelets with the interLink provider.\n","authors":["Lucio Anderlini","Matteo Barbetti","Giulio Bianchini","Diego Ciangottini","Stefano Dal Pra","Diego Michelotto","Carmelo Pellegrino","Rosa Petrini","Alessandro Pascolini","Daniele Spiga"],"pdf_url":"https://arxiv.org/pdf/2502.21266v2.pdf","comment":"To be published in EPJ Web of Conferences (CHEP 2024)"},{"id":"http://arxiv.org/abs/2506.15006v1","updated":"2025-06-17T22:29:37Z","published":"2025-06-17T22:29:37Z","title":"Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models","summary":"  The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.\n","authors":["Jesmin Jahan Tithi","Hanjiang Wu","Avishaii Abuhatzera","Fabrizio Petrini"],"pdf_url":"https://arxiv.org/pdf/2506.15006v1.pdf","comment":"14 pages, submitted to SC25 for review"},{"id":"http://arxiv.org/abs/2506.14981v1","updated":"2025-06-17T21:10:43Z","published":"2025-06-17T21:10:43Z","title":"Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions","summary":"  Data analysis on massive multi-dimensional data, such as high-resolution\nlarge-region time averaging or area averaging for geospatial data, often\ninvolves calculations over a significant number of data points. While\nperforming calculations in scalable and flexible distributed or cloud\nenvironments is a viable option, a full scan of large data volumes still serves\nas a computationally intensive bottleneck, leading to significant cost. This\npaper introduces a generic and comprehensive method to address these\ncomputational challenges. This method generates a small, size-tunable\nsupplementary dataset that stores the cumulative sums along specific subset\ndimensions on top of the raw data. This minor addition unlocks rapid and cheap\nhigh-resolution large-region data analysis, making calculations over large\nnumbers of data points feasible with small instances or even microservices in\nthe cloud. This method is general-purpose, but is particularly well-suited for\ndata stored in chunked, cloud-optimized formats and for services running in\ndistributed or cloud environments. We present a Zarr extension proposal to\nintegrate the specifications of this method and facilitate its straightforward\nimplementation in general-purpose software applications. Benchmark tests\ndemonstrate that this method, implemented in Amazon Web services (AWS),\nsignificantly outperforms the brute-force approach used in on-premises\nservices. With just 5% supplemental storage, this method achieves a performance\nthat is 3-4 orders of magnitude (~10,000 times) faster than the brute-force\napproach, while incurring significantly reduced computational costs.\n","authors":["Hailiang Zhang","Dieu My T. Nguyen","Christine Smit","Mahabal Hegde"],"pdf_url":"https://arxiv.org/pdf/2506.14981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15122v2","updated":"2025-06-17T20:32:49Z","published":"2025-05-21T05:15:14Z","title":"Exploring Dynamic Load Balancing Algorithms for Block-Structured\n  Mesh-and-Particle Simulations in AMReX","summary":"  Load balancing is critical for successful large-scale high-performance\ncomputing (HPC) simulations. With modern supercomputers increasing in\ncomplexity and variability, dynamic load balancing is becoming more critical to\nuse computational resources efficiently. In this study, performed during a\nsummer collaboration at Lawrence Berkeley National Laboratory, we investigate\nvarious standard dynamic load-balancing algorithms. This includes the time\nevaluation of a brute-force solve for application in algorithmic evaluation, as\nwell as quality and time evaluations of the Knapsack algorithm, an SFC\nalgorithm, and two novel algorithms: a painter's partition-based SFC algorithm\nand a combination Knapsack+SFC methodology-based on hardware topology. The\nresults suggest Knapsack and painter's partition-based algorithms should be\namong the first algorithms evaluated by HPC codes for cases with limited weight\ndeviation and will perform at least slightly better than AMReX's\npercentage-tracking partitioning strategy across most simulations, although\neffects diminish as weight variety increases.\n","authors":["Amitash Nanda","Md Kamal Hossain Chowdhury","Hannah Ross","Kevin Gott"],"pdf_url":"https://arxiv.org/pdf/2505.15122v2.pdf","comment":"13 pages, 5 figures, Accepted in the ACM Practice and Experience in\n  Advanced Research Computing (PEARC) Conference Series 2025"},{"id":"http://arxiv.org/abs/2506.14911v1","updated":"2025-06-17T18:41:43Z","published":"2025-06-17T18:41:43Z","title":"Event-Driven Online Vertical Federated Learning","summary":"  Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.\n","authors":["Ganyu Wang","Boyu Wang","Bin Gu","Charles Ling"],"pdf_url":"https://arxiv.org/pdf/2506.14911v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2506.14743v1","updated":"2025-06-17T17:28:31Z","published":"2025-06-17T17:28:31Z","title":"Resource Optimization with MPI Process Malleability for Dynamic\n  Workloads in HPC Clusters","summary":"  Dynamic resource management is essential for optimizing computational\nefficiency in modern high-performance computing (HPC) environments,\nparticularly as systems scale. While research has demonstrated the benefits of\nmalleability in resource management systems (RMS), the adoption of such\ntechniques in production environments remains limited due to challenges in\nstandardization, interoperability, and usability. Addressing these gaps, this\npaper extends our prior work on the Dynamic Management of Resources (DMR)\nframework, which provides a modular and user-friendly approach to dynamic\nresource allocation. Building upon the original DMRlib reconfiguration runtime,\nthis work integrates new methodology from the Malleability Module (MaM) of the\nProteo framework, further enhancing reconfiguration capabilities with new\nspawning strategies and data redistribution methods. In this paper, we explore\nnew malleability strategies in HPC dynamic workloads, such as merging MPI\ncommunicators and asynchronous reconfigurations, which offer new opportunities\nfor dramatically reducing memory overhead. The proposed enhancements are\nrigorously evaluated on a world-class supercomputer, demonstrating improved\nresource utilization and workload efficiency. Results show that dynamic\nresource management can reduce the workload completion time by 40% and increase\nthe resource utilization by over 20%, compared to static resource allocation.\n","authors":["Sergio Iserte","Iker Martín-Álvarez","Krzysztof Rojek","José I. Aliaga","Maribel Castillo","Weronika Folwarska","Antonio J. Peña"],"pdf_url":"https://arxiv.org/pdf/2506.14743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14718v1","updated":"2025-06-17T16:52:48Z","published":"2025-06-17T16:52:48Z","title":"SETI@home: Data Acquisition and Front-End Processing","summary":"  SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project,\nlooking for technosignatures in data recorded at multiple observatories from\n1998 to 2020. Most radio SETI projects analyze data using dedicated processing\nhardware. SETI@home uses a different approach: time-domain data is distributed\nover the Internet to $\\gt 10^{5}$ volunteered home computers, which analyze it.\nThe large amount of computing power this affords ($\\sim 10^{15}$ floating-point\noperations per second (FPOP/s)) allows us to increase the sensitivity and\ngenerality of our search in three ways. We use coherent integration, a\ntechnique in which data is transformed so that the power of drifting signals is\nconfined to a single discrete Fourier transform (DFT) bin. We perform this\ncoherent search over 123 000 Doppler drift rates in the range ($\\pm$100 Hz\ns$^{-1}$). Second, we search for a variety of signal types, such as pulsed\nsignals and arbitrary repeated waveforms. The analysis uses a range of DFT\nsizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front\nend of SETI@home produces a set of detections that exceed thresholds in power\nand goodness of fit. We accumulated $\\sim 1.2\\times 10^{10}$ such detections.\nThe back end of SETI@home takes these detections, identifies and removes radio\nfrequency interference (RFI), and looks for groups of detections that are\nconsistent with extraterrestrial origin and that persist over long timescales.\nThis paper describes the front end of SETI@home and provides parameters for the\nprimary data source, the Arecibo Observatory; the back end and its results are\ndescribed in a companion paper.\n","authors":["Eric J. Korpela","David P. Anderson","Jeff Cobb","Matt Lebofsky","Wei Liu","Dan Werthimer"],"pdf_url":"https://arxiv.org/pdf/2506.14718v1.pdf","comment":"21 pages, 7 figures, 5 tables. Accepted to AJ"},{"id":"http://arxiv.org/abs/2506.14630v1","updated":"2025-06-17T15:25:11Z","published":"2025-06-17T15:25:11Z","title":"Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)","summary":"  We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.\n","authors":["Rúben Adão","Zhongjie Wu","Changjun Zhou","Oana Balmau","João Paulo","Ricardo Macedo"],"pdf_url":"https://arxiv.org/pdf/2506.14630v1.pdf","comment":"This is an extended version of the full paper to appear in VLDB 2025"},{"id":"http://arxiv.org/abs/2506.14610v1","updated":"2025-06-17T15:09:44Z","published":"2025-06-17T15:09:44Z","title":"Concepts for designing modern C++ interfaces for MPI","summary":"  Since the C++ bindings were deleted in 2008, the Message Passing Interface\n(MPI) community has revived efforts in building high-level modern C++\ninterfaces. Such interfaces are either built to serve specific scientific\napplication needs (with limited coverage to the underlying MPI\nfunctionalities), or as an exercise in general-purpose programming model\nbuilding, with the hope that bespoke interfaces can be broadly adopted to\nconstruct a variety of distributed-memory scientific applications. However,\nwith the advent of modern C++-based heterogeneous programming models, GPUs and\nwidespread Machine Learning (ML) usage in contemporary scientific computing,\nthe role of prospective community-standardized high-level C++ interfaces to MPI\nis evolving. The success of such an interface clearly will depend on providing\nrobust abstractions and features adhering to the generic programming principles\nthat underpin the C++ programming language, without compromising on either\nperformance and portability, the core principles upon which MPI was founded.\nHowever, there is a tension between idiomatic C++ handling of types and\nlifetimes, and, MPI's loose interpretation of object lifetimes/ownership and\ninsistence on maintaining global states.\n  Instead of proposing \"yet another\" high-level C++ interface to MPI,\noverlooking or providing partial solutions to work around the key issues\nconcerning the dissonance between MPI semantics and idiomatic C++, this paper\nfocuses on the three fundamental aspects of a high-level interface: type\nsystem, object lifetimes and communication buffers, also identifying\ninconsistencies in the MPI specification. Presumptive solutions can be\nunrefined, and we hope the broader MPI and C++ communities will engage with us\nin productive exchange of ideas and concerns.\n","authors":["C. Nicole Avans","Alfredo A. Correa","Sayan Ghosh","Matthias Schimek","Joseph Schuchart","Anthony Skjellum","Evan D. Suggs","Tim Niklas Uhl"],"pdf_url":"https://arxiv.org/pdf/2506.14610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14393v1","updated":"2025-06-17T10:45:41Z","published":"2025-06-17T10:45:41Z","title":"Consensus Power Inequality: A Comparative Study of Blockchain Networks","summary":"  The distribution of consensus power is a cornerstone of decentralisation,\ninfluencing the security, resilience, and fairness of blockchain networks while\nensuring equitable impact among participants. This study provides a rigorous\nevaluation of consensus power inequality across five prominent blockchain\nnetworks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data\ncollected from January 2022 to July 2024. Leveraging established economic\nmetrics, including the Gini coefficient and Theil index, the research\nquantitatively assesses how power is distributed among blockchain network\nparticipants. A robust dataset, capturing network-specific characteristics such\nas mining pools, staking patterns, and consensus nodes, forms the foundation of\nthe analysis, enabling meaningful comparisons across diverse architectures.\nThrough an in-depth comparative study, the paper identifies key disparities in\nconsensus power distribution. Hedera and Bitcoin demonstrate more balanced\npower distribution, aligning closely with the principles of decentralisation.\nEthereum and Cardano demonstrate moderate levels of inequality. However,\ncontrary to expectations, Ethereum has become more concentrated following its\ntransition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced\ncentralisation of power. Moreover, the findings highlight the structural and\noperational drivers of inequality, including economic barriers, governance\nmodels, and network effects, offering actionable insights for more equitable\nnetwork design. This study establishes a methodological framework for\nevaluating blockchain consensus power inequality, emphasising the importance of\ntargeted strategies to ensure fairer power distribution and enhancing the\nsustainability of decentralised systems. Future research will build on these\nfindings by integrating additional metrics and examining the influence of\nemerging consensus mechanisms.\n","authors":["Kamil Tylinski","Abylay Satybaldy","Paolo Tasca"],"pdf_url":"https://arxiv.org/pdf/2506.14393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05682v2","updated":"2025-06-17T08:28:03Z","published":"2024-08-11T03:29:17Z","title":"Decoupling Generation and Evaluation for Parallel Greedy Best-First\n  Search(extended version)","summary":"  In order to understand and control the search behavior of parallel search,\nrecent work has proposed a class of constrained parallel greedy best-first\nsearch algorithms which only expands states that satisfy some\nconstraint.However, enforcing such constraints can be costly, as threads must\nbe waiting idly until a state that satisfies the expansion constraint is\navailable. We propose an improvement to constrained parallel search which\ndecouples state generation and state evaluation and significantly improves\nstate evaluation rate, resulting in better search performance.\n","authors":["Takumi Shimoda","Alex Fukunaga"],"pdf_url":"https://arxiv.org/pdf/2408.05682v2.pdf","comment":"In Proceedings of SoCS 2025"},{"id":"http://arxiv.org/abs/2506.12417v2","updated":"2025-06-17T08:25:12Z","published":"2025-06-14T09:21:52Z","title":"HarMoEny: Efficient Multi-GPU Inference of MoE Models","summary":"  Mixture-of-Experts (MoE) models offer computational efficiency during\ninference by activating only a subset of specialized experts for a given input.\nThis enables efficient model scaling on multi-GPU systems that use expert\nparallelism without compromising performance. However, load imbalance among\nexperts and GPUs introduces waiting times, which can significantly increase\ninference latency. To address this challenge, we propose HarMoEny, a novel\nsolution to address MoE load imbalance through two simple techniques: (i)\ndynamic token redistribution to underutilized GPUs and (ii) asynchronous\nprefetching of experts from the system to GPU memory. These techniques achieve\na near-perfect load balance among experts and GPUs and mitigate delays caused\nby overloaded GPUs. We implement HarMoEny and compare its latency and\nthroughput with four MoE baselines using real-world and synthetic datasets.\nUnder heavy load imbalance, HarMoEny increases throughput by 37%-70% and\nreduces time-to-first-token by 34%-41%, compared to the next-best baseline.\nMoreover, our ablation study demonstrates that HarMoEny's scheduling policy\nreduces the GPU idling time by up to 84% compared to the baseline policies.\n","authors":["Zachary Doucet","Rishi Sharma","Martijn de Vos","Rafael Pires","Anne-Marie Kermarrec","Oana Balmau"],"pdf_url":"https://arxiv.org/pdf/2506.12417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14251v1","updated":"2025-06-17T07:15:28Z","published":"2025-06-17T07:15:28Z","title":"Convergence-Privacy-Fairness Trade-Off in Personalized Federated\n  Learning","summary":"  Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a\nbalance between personalization and generalization by conducting federated\nlearning (FL) to guide personalized learning (PL). While FL is unaffected by\npersonalized model training, in Ditto, PL depends on the outcome of the FL.\nHowever, the clients' concern about their privacy and consequent perturbation\nof their local models can affect the convergence and (performance) fairness of\nPL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension\nof Ditto under the protection of differential privacy (DP), and analyzes the\ntrade-off among its privacy guarantee, model convergence, and performance\ndistribution fairness. We also analyze the convergence upper bound of the\npersonalized models under DP-Ditto and derive the optimal number of global\naggregations given a privacy budget. Further, we analyze the performance\nfairness of the personalized models, and reveal the feasibility of optimizing\nDP-Ditto jointly for convergence and fairness. Experiments validate our\nanalysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of\nthe state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by\nover 32.71% in fairness and 9.66% in accuracy.\n","authors":["Xiyu Zhao","Qimei Cui","Weicai Li","Wei Ni","Ekram Hossain","Quan Z. Sheng","Xiaofeng Tao","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14237v1","updated":"2025-06-17T06:51:01Z","published":"2025-06-17T06:51:01Z","title":"A Novel Indicator for Quantifying and Minimizing Information Utility\n  Loss of Robot Teams","summary":"  The timely exchange of information among robots within a team is vital, but\nit can be constrained by limited wireless capacity. The inability to deliver\ninformation promptly can result in estimation errors that impact collaborative\nefforts among robots. In this paper, we propose a new metric termed Loss of\nInformation Utility (LoIU) to quantify the freshness and utility of information\ncritical for cooperation. The metric enables robots to prioritize information\ntransmissions within bandwidth constraints. We also propose the estimation of\nLoIU using belief distributions and accordingly optimize both transmission\nschedule and resource allocation strategy for device-to-device transmissions to\nminimize the time-average LoIU within a robot team. A semi-decentralized\nMulti-Agent Deep Deterministic Policy Gradient framework is developed, where\neach robot functions as an actor responsible for scheduling transmissions among\nits collaborators while a central critic periodically evaluates and refines the\nactors in response to mobility and interference. Simulations validate the\neffectiveness of our approach, demonstrating an enhancement of information\nfreshness and utility by 98%, compared to alternative methods.\n","authors":["Xiyu Zhao","Qimei Cui","Wei Ni","Quan Z. Sheng","Abbas Jamalipour","Guoshun Nan","Xiaofeng Tao","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.14237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14197v1","updated":"2025-06-17T05:26:40Z","published":"2025-06-17T05:26:40Z","title":"The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic\n  Demonstration of Miner-Centric Propagation Topologies","summary":"  This paper formally examines the network structure of Bitcoin CORE (BTC) and\nBitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that\nhome-hosted full nodes are incapable of participating in or influencing the\npropagation topology. Leveraging established models such as scale-free networks\nand small-world connectivity, we demonstrate that the propagation graph is\ndominated by a densely interconnected miner clique, while full nodes reside on\nthe periphery, excluded from all transaction-to-block inclusion paths. Using\nsimulation-backed metrics and eigenvalue centrality analysis, we confirm that\nfull nodes are neither critical nor operationally relevant for consensus\npropagation.\n","authors":["Dr Craig S Wright"],"pdf_url":"https://arxiv.org/pdf/2506.14197v1.pdf","comment":"34 pages, 1 figures. Comprehensive technical treatment of Bitcoin\n  propagation topology. Submitted to arXiv for public dissemination and\n  archival reference"},{"id":"http://arxiv.org/abs/2506.14852v1","updated":"2025-06-17T04:42:30Z","published":"2025-06-17T04:42:30Z","title":"Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching","summary":"  LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.\n","authors":["Qizheng Zhang","Michael Wornow","Kunle Olukotun"],"pdf_url":"https://arxiv.org/pdf/2506.14852v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2506.14851v1","updated":"2025-06-17T03:49:25Z","published":"2025-06-17T03:49:25Z","title":"Efficient Serving of LLM Applications with Probabilistic Demand Modeling","summary":"  Applications based on Large Language Models (LLMs) contains a series of tasks\nto address real-world problems with boosted capability, which have dynamic\ndemand volumes on diverse backends. Existing serving systems treat the resource\ndemands of LLM applications as a blackbox, compromising end-to-end efficiency\ndue to improper queuing order and backend warm up latency. We find that the\nresource demands of LLM applications can be modeled in a general and accurate\nmanner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which\nleverages PDGraph for efficient serving of LLM applications. Confronting\nprobabilistic demand description, Hermes applies the Gittins policy to\ndetermine the scheduling order that can minimize the average application\ncompletion time. It also uses the PDGraph model to help prewarm cold backends\nat proper moments. Experiments with diverse LLM applications confirm that\nHermes can effectively improve the application serving efficiency, reducing the\naverage completion time by over 70% and the P95 completion time by over 80%.\n","authors":["Yifei Liu","Zuo Gan","Zhenghao Gan","Weiye Wang","Chen Chen","Yizhou Shan","Xusheng Chen","Zhenhua Han","Yifei Zhu","Shixuan Sun","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2506.14851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14107v1","updated":"2025-06-17T01:59:10Z","published":"2025-06-17T01:59:10Z","title":"Déjà Vu: Efficient Video-Language Query Engine with Learning-based\n  Inter-Frame Computation Reuse","summary":"  Recently, Video-Language Models (VideoLMs) have demonstrated remarkable\ncapabilities, offering significant potential for flexible and powerful video\nquery systems. These models typically rely on Vision Transformers (ViTs), which\nprocess video frames individually to extract visual embeddings. However,\ngenerating embeddings for large-scale videos requires ViT inferencing across\nnumerous frames, posing a major hurdle to real-world deployment and\nnecessitating solutions for integration into scalable video data management\nsystems. This paper introduces D\\'ej\\`a Vu, a video-language query engine that\naccelerates ViT-based VideoLMs by reusing computations across consecutive\nframes. At its core is ReuseViT, a modified ViT model specifically designed for\nVideoLM tasks, which learns to detect inter-frame reuse opportunities, striking\nan effective balance between accuracy and reuse. Although ReuseViT\nsignificantly reduces computation, these savings do not directly translate into\nperformance gains on GPUs. To overcome this, D\\'ej\\`a Vu integrates\nmemory-compute joint compaction techniques that convert the FLOP savings into\ntangible performance gains. Evaluations on three VideoLM tasks show that\nD\\'ej\\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error\nbound, dramatically enhancing the practicality of VideoLMs for large-scale\nvideo analytics.\n","authors":["Jinwoo Hwang","Daeun Kim","Sangyeop Lee","Yoonsung Kim","Guseul Heo","Hojoon Kim","Yunseok Jeong","Tadiwos Meaza","Eunhyeok Park","Jeongseob Ahn","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2506.14107v1.pdf","comment":"Accepted to 2025 VLDB"}]},"2025-06-16T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2506.14034v1","updated":"2025-06-16T22:13:48Z","published":"2025-06-16T22:13:48Z","title":"Sketched Sum-Product Networks for Joins","summary":"  Sketches have shown high accuracy in multi-way join cardinality estimation, a\ncritical problem in cost-based query optimization. Accurately estimating the\ncardinality of a join operation -- analogous to its computational cost --\nallows the optimization of query execution costs in relational database\nsystems. However, although sketches have shown high efficacy in query\noptimization, they are typically constructed specifically for predefined\nselections in queries that are assumed to be given a priori, hindering their\napplicability to new queries. As a more general solution, we propose for\nSum-Product Networks to dynamically approximate sketches on-the-fly.\nSum-Product Networks can decompose and model multivariate distributions, such\nas relations, as linear combinations of multiple univariate distributions. By\nrepresenting these univariate distributions as sketches, Sum-Product Networks\ncan combine them element-wise to efficiently approximate the sketch of any\nquery selection. These approximate sketches can then be applied to join\ncardinality estimation. In particular, we implement the Fast-AGMS and Bound\nSketch methods, which have successfully been used in prior work, despite their\ncostly construction. By accurately approximating them instead, our work\nprovides a practical alternative to apply these sketches to query optimization.\n","authors":["Brian Tsan","Abylay Amanbayev","Asoke Datta","Florin Rusu"],"pdf_url":"https://arxiv.org/pdf/2506.14034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13670v1","updated":"2025-06-16T16:29:29Z","published":"2025-06-16T16:29:29Z","title":"Parachute: Single-Pass Bi-Directional Information Passing","summary":"  Sideways information passing is a well-known technique for mitigating the\nimpact of large build sides in a database query plan. As currently implemented\nin production systems, sideways information passing enables only a\nuni-directional information flow, as opposed to instance-optimal algorithms,\nsuch as Yannakakis'. On the other hand, the latter require an additional pass\nover the input, which hinders adoption in production systems.\n  In this paper, we make a step towards enabling single-pass bi-directional\ninformation passing during query execution. We achieve this by statically\nanalyzing between which tables the information flow is blocked and by\nleveraging precomputed join-induced fingerprint columns on FK-tables. On the\nJOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time\nwithout and with semi-join filtering by 1.54x and 1.24x, respectively, when\nallowed to use 15% extra space.\n","authors":["Mihail Stoian","Andreas Zimmerer","Skander Krid","Amadou Latyr Ngom","Jialin Ding","Tim Kraska","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2506.13670v1.pdf","comment":"To appear at VLDB 2025"},{"id":"http://arxiv.org/abs/2411.17603v2","updated":"2025-06-16T15:46:33Z","published":"2024-11-26T17:11:10Z","title":"Is Integer Linear Programming All You Need for Deletion Propagation? A\n  Unified and Practical Approach for Generalized Deletion Propagation","summary":"  Deletion Propagation (DP) refers to a family of database problems rooted in\nthe classical view-update problem: how to propagate intended deletions in a\nview (query output) back to the source database while satisfying constraints\nand minimizing side effects. Although studied for over 40 years, DP variants,\ntheir complexities, and practical algorithms have been typically explored in\nisolation.\n  This work presents a unified and generalized framework for DP with several\nkey benefits: (1) It unifies and generalizes all previously known DP variants,\neffectively subsuming them within a broader class of problems, including new,\nwell-motivated variants. (2) It comes with a practical and general-purpose\nalgorithm that is ``coarse-grained instance-optimal'': it runs in PTIME for all\nknown PTIME cases and can automatically exploit structural regularities in the\ndata, i.e. it does not rely on hints about such regularities as part of the\ninput. (3) It is complete: our framework handles all known DP variants in all\nsettings (including those involving self-joins, unions, and bag semantics), and\nallows us to provide new complexity results. (4) It is easy to implement and,\nin many cases, outperforms prior variant-specific solutions, sometimes by\norders of magnitude. We provide the first experimental results for several DP\nvariants previously studied only in theory.\n","authors":["Neha Makhija","Wolfgang Gatterbauer"],"pdf_url":"https://arxiv.org/pdf/2411.17603v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.11703v2","updated":"2025-06-16T07:42:11Z","published":"2023-10-18T04:31:06Z","title":"A Comprehensive Survey on Vector Database: Storage and Retrieval\n  Technique, Challenge","summary":"  Vector databases (VDBs) have emerged to manage high-dimensional data that\nexceed the capabilities of traditional database management systems, and are now\ntightly integrated with large language models as well as widely applied in\nmodern artificial intelligence systems. Although relatively few studies\ndescribe existing or introduce new vector database architectures, the core\ntechnologies underlying VDBs, such as approximate nearest neighbor search, have\nbeen extensively studied and are well documented in the literature. In this\nwork, we present a comprehensive review of the relevant algorithms to provide a\ngeneral understanding of this booming research area. Specifically, we first\nprovide a review of storage and retrieval techniques in VDBs, with detailed\ndesign principles and technological evolution. Then, we conduct an in-depth\ncomparison of several advanced VDB solutions with their strengths, limitations,\nand typical application scenarios. Finally, we also outline emerging\nopportunities for coupling VDBs with large language models, including open\nresearch problems and trends, such as novel indexing strategies. This survey\naims to serve as a practical resource, enabling readers to quickly gain an\noverall understanding of the current knowledge landscape in this rapidly\ndeveloping area.\n","authors":["Le Ma","Ran Zhang","Yikun Han","Shirui Yu","Zaitian Wang","Zhiyuan Ning","Jinghan Zhang","Ping Xu","Pengjiang Li","Wei Ju","Chong Chen","Dongjie Wang","Kunpeng Liu","Pengyang Wang","Pengfei Wang","Yanjie Fu","Chunjiang Liu","Yuanchun Zhou","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2310.11703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13144v1","updated":"2025-06-16T06:57:33Z","published":"2025-06-16T06:57:33Z","title":"EnhanceGraph: A Continuously Enhanced Graph-based Index for\n  High-dimensional Approximate Nearest Neighbor Search","summary":"  Recently, Approximate Nearest Neighbor Search in high-dimensional vector\nspaces has garnered considerable attention due to the rapid advancement of deep\nlearning techniques. We observed that a substantial amount of search and\nconstruction logs are generated throughout the lifespan of a graph-based index.\nHowever, these two types of valuable logs are not fully exploited due to the\nstatic nature of existing indexes. We present the EnhanceGraph framework, which\nintegrates two types of logs into a novel structure called a conjugate graph.\nThe conjugate graph is then used to improve search quality. Through theoretical\nanalyses and observations of the limitations of graph-based indexes, we propose\nseveral optimization methods. For the search logs, the conjugate graph stores\nthe edges from local optima to global optima to enhance routing to the nearest\nneighbor. For the construction logs, the conjugate graph stores the pruned\nedges from the proximity graph to enhance retrieving of k nearest neighbors.\nOur experimental results on several public and real-world industrial datasets\nshow that EnhanceGraph significantly improves search accuracy with the greatest\nimprovement on recall from 41.74% to 93.42%, but does not sacrifices search\nefficiency. In addition, our EnhanceGraph algorithm has been integrated into\nAnt Group's open-source vector library, VSAG.\n","authors":["Xiaoyao Zhong","Jiabao Jin","Peng Cheng","Mingyu Yang","Lei Chen","Haoyang Li","Zhitao Shen","Xuemin Lin","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2506.13144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10422v2","updated":"2025-06-16T02:33:57Z","published":"2025-06-12T07:21:54Z","title":"A Hybrid Heuristic Framework for Resource-Efficient Querying of\n  Scientific Experiments Data","summary":"  Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.\n","authors":["Mayank Patel","Minal Bhise"],"pdf_url":"https://arxiv.org/pdf/2506.10422v2.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2506.13935v1","updated":"2025-06-16T19:18:56Z","published":"2025-06-16T19:18:56Z","title":"ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in\n  Precision Agriculture","summary":"  To empower precision agriculture through distributed machine learning (DML),\nsplit learning (SL) has emerged as a promising paradigm, partitioning deep\nneural networks (DNNs) between edge devices and servers to reduce computational\nburdens and preserve data privacy. However, conventional SL frameworks'\none-split-fits-all strategy is a critical limitation in agricultural ecosystems\nwhere edge insect monitoring devices exhibit vast heterogeneity in\ncomputational power, energy constraints, and connectivity. This leads to\nstraggler bottlenecks, inefficient resource utilization, and compromised model\nperformance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement\nlearning (RL)-driven framework that dynamically tailors DNN split points for\neach device, optimizing efficiency without sacrificing accuracy. Specifically,\na Q-learning agent acts as an adaptive orchestrator, balancing workloads and\nlatency thresholds across devices to mitigate computational starvation or\noverload. By framing split layer selection as a finite-state Markov decision\nprocess, ReinDSplit convergence ensures that highly constrained devices\ncontribute meaningfully to model training over time. Evaluated on three insect\nclassification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit\nachieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit\npioneers a paradigm shift in SL by harmonizing RL for resource efficiency,\nprivacy, and scalability in heterogeneous environments.\n","authors":["Vishesh Kumar Tanwar","Soumik Sarkar","Asheesh K. Singh","Sajal K. Das"],"pdf_url":"https://arxiv.org/pdf/2506.13935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07838v4","updated":"2025-06-16T18:10:41Z","published":"2025-06-09T15:04:48Z","title":"A Terminology for Scientific Workflow Systems","summary":"  The term scientific workflow has evolved over the last two decades to\nencompass a broad range of compositions of interdependent compute tasks and\ndata movements. It has also become an umbrella term for processing in modern\nscientific applications. Today, many scientific applications can be considered\nas workflows made of multiple dependent steps, and hundreds of workflow\nmanagement systems (WMSs) have been developed to manage and run these\nworkflows. However, no turnkey solution has emerged to address the diversity of\nscientific processes and the infrastructure on which they are implemented.\nInstead, new research problems requiring the execution of scientific workflows\nwith some novel feature often lead to the development of an entirely new WMS. A\ndirect consequence is that many existing WMSs share some salient features,\noffer similar functionalities, and can manage the same categories of workflows\nbut also have some distinct capabilities. This situation makes researchers who\ndevelop workflows face the complex question of selecting a WMS. This selection\ncan be driven by technical considerations, to find the system that is the most\nappropriate for their application and for the resources available to them, or\nother factors such as reputation, adoption, strong community support, or\nlong-term sustainability. To address this problem, a group of WMS developers\nand practitioners joined their efforts to produce a community-based terminology\nof WMSs. This paper summarizes their findings and introduces this new\nterminology to characterize WMSs. This terminology is composed of fives axes:\nworkflow characteristics, composition, orchestration, data management, and\nmetadata capture. Each axis comprises several concepts that capture the\nprominent features of WMSs. Based on this terminology, this paper also presents\na classification of 23 existing WMSs according to the proposed axes and terms.\n","authors":["Frédéric Suter","Tainã Coleman","İlkay Altintaş","Rosa M. Badia","Bartosz Balis","Kyle Chard","Iacopo Colonnelli","Ewa Deelman","Paolo Di Tommaso","Thomas Fahringer","Carole Goble","Shantenu Jha","Daniel S. Katz","Johannes Köster","Ulf Leser","Kshitij Mehta","Hilary Oliver","J. -Luc Peterson","Giovanni Pizzi","Loïc Pottier","Raül Sirvent","Eric Suchyta","Douglas Thain","Sean R. Wilkinson","Justin M. Wozniak","Rafael Ferreira da Silva"],"pdf_url":"https://arxiv.org/pdf/2506.07838v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13730v1","updated":"2025-06-16T17:40:34Z","published":"2025-06-16T17:40:34Z","title":"BanditWare: A Contextual Bandit-based Framework for Hardware Prediction","summary":"  Distributed computing systems are essential for meeting the demands of modern\napplications, yet transitioning from single-system to distributed environments\npresents significant challenges. Misallocating resources in shared systems can\nlead to resource contention, system instability, degraded performance, priority\ninversion, inefficient utilization, increased latency, and environmental\nimpact.\n  We present BanditWare, an online recommendation system that dynamically\nselects the most suitable hardware for applications using a contextual\nmulti-armed bandit algorithm. BanditWare balances exploration and exploitation,\ngradually refining its hardware recommendations based on observed application\nperformance while continuing to explore potentially better options. Unlike\ntraditional statistical and machine learning approaches that rely heavily on\nlarge historical datasets, BanditWare operates online, learning and adapting in\nreal-time as new workloads arrive.\n  We evaluated BanditWare on three workflow applications: Cycles (an\nagricultural science scientific workflow) BurnPro3D (a web-based platform for\nfire science) and a matrix multiplication application. Designed for seamless\nintegration with the National Data Platform (NDP), BanditWare enables users of\nall experience levels to optimize resource allocation efficiently.\n","authors":["Tainã Coleman","Hena Ahmed","Ravi Shende","Ismael Perez","Ïlkay Altintaş"],"pdf_url":"https://arxiv.org/pdf/2506.13730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13720v1","updated":"2025-06-16T17:26:27Z","published":"2025-06-16T17:26:27Z","title":"POPQC: Parallel Optimization for Quantum Circuits (Extended Version)","summary":"  Optimization of quantum programs or circuits is a fundamental problem in\nquantum computing and remains a major challenge. State-of-the-art quantum\ncircuit optimizers rely on heuristics and typically require superlinear, and\neven exponential, time. Recent work proposed a new approach that pursues a\nweaker form of optimality called local optimality. Parameterized by a natural\nnumber $\\Omega$, local optimality insists that each and every $\\Omega$-segment\nof the circuit is optimal with respect to an external optimizer, called the\noracle. Local optimization can be performed using only a linear number of calls\nto the oracle but still incurs quadratic computational overheads in addition to\noracle calls. Perhaps most importantly, the algorithm is sequential.\n  In this paper, we present a parallel algorithm for local optimization of\nquantum circuits. To ensure efficiency, the algorithm operates by keeping a set\nof fingers into the circuit and maintains the invariant that a $\\Omega$-deep\ncircuit needs to be optimized only if it contains a finger. Operating in\nrounds, the algorithm selects a set of fingers, optimizes in parallel the\nsegments containing the fingers, and updates the finger set to ensure the\ninvariant. For constant $\\Omega$, we prove that the algorithm requires\n$O(n\\lg{n})$ work and $O(r\\lg{n})$ span, where $n$ is the circuit size and $r$\nis the number of rounds. We prove that the optimized circuit returned by the\nalgorithm is locally optimal in the sense that any $\\Omega$-segment of the\ncircuit is optimal with respect to the oracle.\n","authors":["Pengyu Liu","Jatin Arora","Mingkuan Xu","Umut A. Acar"],"pdf_url":"https://arxiv.org/pdf/2506.13720v1.pdf","comment":"SPAA25"},{"id":"http://arxiv.org/abs/2506.13612v1","updated":"2025-06-16T15:39:10Z","published":"2025-06-16T15:39:10Z","title":"EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated\n  Learning","summary":"  Despite federated learning (FL)'s potential in collaborative learning, its\nperformance has deteriorated due to the data heterogeneity of distributed\nusers. Recently, clustered federated learning (CFL) has emerged to address this\nchallenge by partitioning users into clusters according to their similarity.\nHowever, CFL faces difficulties in training when users are unwilling to share\ntheir cluster identities due to privacy concerns. To address these issues, we\npresent an innovative Efficient and Robust Secure Aggregation scheme for CFL,\ndubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while\nmaintaining users' cluster identity confidentially. Moreover, it detects\npotential poisonous attacks without compromising individual client gradients by\ndiscarding negatively correlated gradients and aggregating positively\ncorrelated ones using a weighted approach. The server also authenticates\ncorrect gradient encoding by clients. EBS-CFL has high efficiency with\nclient-side overhead O(ml + m^2) for communication and O(m^2l) for computation,\nwhere m is the number of cluster identities, and l is the gradient size. When m\n= 1, EBS-CFL's computational efficiency of client is at least O(log n) times\nbetter than comparison schemes, where n is the number of clients.In addition,\nwe validate the scheme through extensive experiments. Finally, we theoretically\nprove the scheme's security.\n","authors":["Zhiqiang Li","Haiyong Bao","Menghong Guan","Hao Pan","Cheng Huang","Hong-Ning Dai"],"pdf_url":"https://arxiv.org/pdf/2506.13612v1.pdf","comment":"Accepted by AAAI 25"},{"id":"http://arxiv.org/abs/2503.13075v3","updated":"2025-06-16T15:18:31Z","published":"2025-03-17T11:28:48Z","title":"ILVES: Accurate and efficient bond length and angle constraints in\n  molecular dynamics","summary":"  All-atom, force field-based molecular dynamics simulations are essential\ntools in computational chemistry, enabling the prediction and analysis of\nbiomolecular systems with atomic-level resolution. However, as system sizes and\nsimulation timescales increase, so does the associated computational cost. To\nextend simulated time using the same resources, a common strategy is to\nconstrain the fastest degrees of freedom, such as bond lengths, allowing for\nlarger integration time steps without compromising accuracy. The de facto\nstate-of-the-art algorithms for this purpose (SHAKE, LINCS, and P-LINCS) are\nintegrated into most molecular dynamics packages and widely adopted across the\nfield. Despite their impact, these methods exhibit limitations: all converge\nslowly when high numerical accuracy is required, and the LINCS and P-LINCS\nalgorithms cannot handle general angular constraints, limiting further\nincreases in time step.\n  In this article, we introduce ILVES, a family of parallel algorithms that\nconverge so rapidly that it is now practical to solve bond length and\nassociated angular constraint equations as accurately as the hardware will\nallow. We have integrated ILVES into Gromacs and our analysis demonstrates that\nit is superior to the state-of-the-art when constraining bond lengths. Due to\nits better convergence properties, we also show that if the time step is\nincreased up to 3.5 fs by enforcing angular constraints, ILVES enables a 1.65x\nincrease in simulated time using the same computational resources and\nwall-clock time, an outcome unattainable with current methods. This advance can\nsignificantly reduce the computational cost of most all-atom molecular dynamics\nsimulations while improving their accuracy and extending access to larger\nsystems and longer timescales.\n","authors":["Lorién López-Villellas","Carl Christian Kjelgaard Mikkelsen","Juan José Galano-Frutos","Santiago Marco-Sola","Jesús Alastruey-Benedé","Pablo Ibáñez","Pablo Echenique","Miquel Moretó","Maria Cristina De Rosa","Pablo García-Risueño"],"pdf_url":"https://arxiv.org/pdf/2503.13075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13561v1","updated":"2025-06-16T14:47:02Z","published":"2025-06-16T14:47:02Z","title":"Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated\n  Learning","summary":"  Federated learning (FL) shows great promise in large-scale machine learning\nbut introduces new privacy and security challenges. We propose ByITFL and\nLoByITFL, two novel FL schemes that enhance resilience against Byzantine users\nwhile keeping the users' data private from eavesdroppers. To ensure privacy and\nByzantine resilience, our schemes build on having a small representative\ndataset available to the federator and crafting a discriminator function\nallowing the mitigation of corrupt users' contributions. ByITFL employs\nLagrange coded computing and re-randomization, making it the first\nByzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,\nthough at the cost of a significant communication overhead. LoByITFL, on the\nother hand, achieves Byzantine resilience and IT privacy at a significantly\nreduced communication cost, but requires a Trusted Third Party, used only in a\none-time initialization phase before training. We provide theoretical\nguarantees on privacy and Byzantine resilience, along with convergence\nguarantees and experimental results validating our findings.\n","authors":["Yue Xia","Christoph Hofmeister","Maximilian Egger","Rawad Bitar"],"pdf_url":"https://arxiv.org/pdf/2506.13561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.12663v3","updated":"2025-06-16T14:38:53Z","published":"2021-05-26T16:21:33Z","title":"EvalNet: A Practical Toolchain for Generation and Analysis of\n  Extreme-Scale Interconnects","summary":"  The diversity of communication paths in a network - especially non-minimal\npaths - is a key enabler of performance at extreme scales. We present EvalNet,\na toolchain for scalable generation and analysis over 25 important network\ntopologies, such as Slim Fly, PolarFly, and Orthogonal Fat Trees, with a strong\nfocus on path diversity metrics. EvalNet provides an extensive and fine-grained\nanalysis of shortest and non-shortest paths, including their multiplicities,\nlengths, and interference. It supports exact measurement and visualization of\nbandwidth and throughput between every router pair, enabling unprecedented\ninsight into routing potential. EvalNet also includes detailed models for\nconstruction cost and power consumption, and interfaces seamlessly with\nestablished simulators, which we tune to support large-scale evaluations on\nlow-cost hardware. Using EvalNet, we deliver the widest and most comprehensive\npath diversity study to date, demonstrating how path diversity underpins\nthroughput and scalability, and facilitating progress towards new frontiers in\nextreme-scale network design.\n","authors":["Maciej Besta","Patrick Iff","Marcel Schneider","Nils Blach","Alessandro Maissen","Salvatore Di Girolamo","Jens Domke","Jascha Krattenmacher","Ankit Singla","Kartik Lakhotia","Laura Monroe","Fabrizio Petrini","Robert Gerstenberger","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2105.12663v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12788v2","updated":"2025-06-16T14:32:25Z","published":"2025-03-17T03:54:49Z","title":"Byzantine-Tolerant Consensus in GPU-Inspired Shared Memory","summary":"  In this work, we formalize a novel shared memory model inspired by the\npopular GPU architecture. Within this model, we develop algorithmic solutions\nto the Byzantine Consensus problem and analyze their fault-resilience.\n","authors":["Chryssis Georgiou","Manaswini Piduguralla","Sathya Peri"],"pdf_url":"https://arxiv.org/pdf/2503.12788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10883v3","updated":"2025-06-16T14:05:55Z","published":"2023-02-21T18:58:32Z","title":"Blockchain and Biometrics: Survey, GDPR Elements, and Future Directions","summary":"  Biometric recognition as an efficient and hard-to-forge way of identification\nand verification has become an indispensable part of the current digital world.\nThe fast evolution of this technology has been a strong incentive for\nintegration into many applications. Meanwhile, blockchain, the decentralized\nledger technology, has been widely received by both research and industry in\nthe past few years, and it is being increasingly deployed today in many\ndifferent applications, such as money transfer, IoT, healthcare, or logistics.\nRecently, researchers have started to speculate on the pros and cons and what\nthe best applications would be when these two technologies cross paths. This\npaper provides a survey of the research literature on the combination of\nblockchain and biometrics and includes a first legal analysis of this\nintegration based on GDPR to shed light on challenges and potentials. Although\nthe integration of blockchain technology into the biometric sector is still in\nits infancy, with a growing body of literature discussing specific applications\nand advanced technological setups, this paper aims to provide a holistic\nunderstanding of blockchain applicability in biometrics. Based on published\nstudies, this article discusses, among others, practical examples combining\nblockchain and biometrics for novel applications in PKI systems, distributed\ntrusted services, and identity management. Challenges and limitations when\ncombining blockchain and biometrics that motivate future work will also be\ndiscussed; e.g., blockchain networks at their current stage may not be\nefficient or economical for some real-time biometric applications. Finally, we\nalso discuss key legal aspects of the EU General Data Protection Regulation\n(GDPR) related to this combination of technologies (blockchain and biometrics);\nfor example, accountability, immutability, anonymity, and data protection\nelements.\n","authors":["Mahdi Ghafourian","Ruben Vera-Rodriguez","Julian Fierrez","Bilgesu Sumer","Ruben Tolosana","Aythami Moralez","Els Kindt"],"pdf_url":"https://arxiv.org/pdf/2302.10883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13497v1","updated":"2025-06-16T13:54:41Z","published":"2025-06-16T13:54:41Z","title":"DDiT: Dynamic Resource Allocation for Diffusion Transformer Model\n  Serving","summary":"  The Text-to-Video (T2V) model aims to generate dynamic and expressive videos\nfrom textual prompts. The generation pipeline typically involves multiple\nmodules, such as language encoder, Diffusion Transformer (DiT), and Variational\nAutoencoders (VAE). Existing serving systems often rely on monolithic model\ndeployment, while overlooking the distinct characteristics of each module,\nleading to inefficient GPU utilization. In addition, DiT exhibits varying\nperformance gains across different resolutions and degrees of parallelism, and\nsignificant optimization potential remains unexplored. To address these\nproblems, we present DDiT, a flexible system that integrates both inter-phase\nand intra-phase optimizations. DDiT focuses on two key metrics: optimal degree\nof parallelism, which prevents excessive parallelism for specific resolutions,\nand starvation time, which quantifies the sacrifice of each request. To this\nend, DDiT introduces a decoupled control mechanism to minimize the\ncomputational inefficiency caused by imbalances in the degree of parallelism\nbetween the DiT and VAE phases. It also designs a greedy resource allocation\nalgorithm with a novel scheduling mechanism that operates at the single-step\ngranularity, enabling dynamic and timely resource scaling. Our evaluation on\nthe T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets\nreveals that DDiT significantly outperforms state-of-the-art baselines by up to\n1.44x in p99 latency and 1.43x in average latency.\n","authors":["Heyang Huang","Cunchen Hu","Jiaqi Zhu","Ziyuan Gao","Liangliang Xu","Yizhou Shan","Yungang Bao","Sun Ninghui","Tianwei Zhang","Sa Wang"],"pdf_url":"https://arxiv.org/pdf/2506.13497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13246v1","updated":"2025-06-16T08:43:56Z","published":"2025-06-16T08:43:56Z","title":"On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains","summary":"  This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.\n","authors":["Craig Steven Wright"],"pdf_url":"https://arxiv.org/pdf/2506.13246v1.pdf","comment":"47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema"},{"id":"http://arxiv.org/abs/2505.04947v3","updated":"2025-06-16T06:11:54Z","published":"2025-05-08T04:58:45Z","title":"DFPL: Decentralized Federated Prototype Learning Across Heterogeneous\n  Data Distributions","summary":"  Federated learning is a distributed machine learning paradigm through\ncentralized model aggregation. However, standard federated learning relies on a\ncentralized server, making it vulnerable to server failures. While existing\nsolutions utilize blockchain technology to implement Decentralized Federated\nLearning (DFL), the statistical heterogeneity of data distributions among\nclients severely degrades the performance of DFL. Driven by this issue, this\npaper proposes a decentralized federated prototype learning framework, named\nDFPL, which significantly improves the performance of DFL across heterogeneous\ndata distributions. Specifically, DFPL introduces prototype learning into DFL\nto mitigate the impact of statistical heterogeneity and reduces the amount of\nparameters exchanged between clients. Additionally, blockchain is embedded into\nour framework, enabling the training and mining processes to be implemented\nlocally on each client. From a theoretical perspective, we analyze the\nconvergence of DFPL by modeling the required computational resources during\nboth training and mining processes. The experiment results highlight the\nsuperiority of our DFPL in model performance and communication efficiency\nacross four benchmark datasets with heterogeneous data distributions.\n","authors":["Hongliang Zhang","Fenghua Xu","Zhongyuan Yu","Shanchen Pang","Chunqiang Hu","Jiguo Yu"],"pdf_url":"https://arxiv.org/pdf/2505.04947v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10422v2","updated":"2025-06-16T02:33:57Z","published":"2025-06-12T07:21:54Z","title":"A Hybrid Heuristic Framework for Resource-Efficient Querying of\n  Scientific Experiments Data","summary":"  Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.\n","authors":["Mayank Patel","Minal Bhise"],"pdf_url":"https://arxiv.org/pdf/2506.10422v2.pdf","comment":null}]},"2025-06-15T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2506.12990v1","updated":"2025-06-15T23:13:20Z","published":"2025-06-15T23:13:20Z","title":"Humans, Machine Learning, and Language Models in Union: A Cognitive\n  Study on Table Unionability","summary":"  Data discovery and table unionability in particular became key tasks in\nmodern Data Science. However, the human perspective for these tasks is still\nunder-explored. Thus, this research investigates the human behavior in\ndetermining table unionability within data discovery. We have designed an\nexperimental survey and conducted a comprehensive analysis, in which we assess\nhuman decision-making for table unionability. We use the observations from the\nanalysis to develop a machine learning framework to boost the (raw) performance\nof humans. Furthermore, we perform a preliminary study on how LLM performance\nis compared to humans indicating that it is typically better to consider a\ncombination of both. We believe that this work lays the foundations for\ndeveloping future Human-in-the-Loop systems for efficient data discovery.\n","authors":["Sreeram Marimuthu","Nina Klimenkova","Roee Shraga"],"pdf_url":"https://arxiv.org/pdf/2506.12990v1.pdf","comment":"6 Pages, 4 figures, ACM SIGMOD HILDA '25 (Status-Accepted)"},{"id":"http://arxiv.org/abs/2502.17248v2","updated":"2025-06-15T16:16:30Z","published":"2025-02-24T15:26:22Z","title":"Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search","summary":"  Text-to-SQL, which enables natural language interaction with databases,\nserves as a pivotal method across diverse industries. With new, more powerful\nlarge language models (LLMs) emerging every few months, fine-tuning has become\nincredibly costly, labor-intensive, and error-prone. As an alternative,\nzero-shot Text-to-SQL, which leverages the growing knowledge and reasoning\ncapabilities encoded in LLMs without task-specific fine-tuning, presents a\npromising and more challenging direction. To address this challenge, we propose\nAlpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)\nframework to iteratively infer SQL construction actions based on partial\nreasoning states. To enhance the framework's reasoning capabilities, we\nintroduce LLM-as-Action-Model to dynamically generate SQL construction actions\nduring the MCTS process, steering the search toward more promising SQL queries.\nMoreover, Alpha-SQL employs a self-supervised reward function to evaluate the\nquality of candidate SQL queries, ensuring more accurate and efficient query\ngeneration. Experimental results show that Alpha-SQL achieves 69.7% execution\naccuracy on the BIRD development set, using a 32B open-source LLM without\nfine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based\non GPT-4o by 2.5% on the BIRD development set.\n","authors":["Boyan Li","Jiayi Zhang","Ju Fan","Yanwei Xu","Chong Chen","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2502.17248v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2408.05109v5","updated":"2025-06-15T15:53:09Z","published":"2024-08-09T14:59:36Z","title":"A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are\n  we going?","summary":"  Translating users' natural language queries (NL) into SQL queries (i.e.,\nText-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing\nrelational databases and support various commercial applications. The\nperformance of Text-to-SQL has been greatly enhanced with the emergence of\nLarge Language Models (LLMs). In this survey, we provide a comprehensive review\nof Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from\nthe following four aspects: (1) Model: Text-to-SQL translation techniques that\ntackle not only NL ambiguity and under-specification, but also properly map NL\nwith database schema and instances; (2) Data: From the collection of training\ndata, data synthesis due to training data scarcity, to Text-to-SQL benchmarks;\n(3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using\ndifferent metrics and granularities; and (4) Error Analysis: analyzing\nText-to-SQL errors to find the root cause and guiding Text-to-SQL models to\nevolve. Moreover, we offer a rule of thumb for developing Text-to-SQL\nsolutions. Finally, we discuss the research challenges and open problems of\nText-to-SQL in the LLMs era.\n","authors":["Xinyu Liu","Shuyu Shen","Boyan Li","Peixian Ma","Runzhi Jiang","Yuxin Zhang","Ju Fan","Guoliang Li","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2408.05109v5.pdf","comment":"20 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.11984v2","updated":"2025-06-15T15:17:10Z","published":"2025-03-15T03:54:10Z","title":"NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL\n  Translation","summary":"  Natural Language to SQL (i.e., NL2SQL) translation is crucial for\ndemocratizing database access, but even state-of-the-art models frequently\ngenerate semantically incorrect SQL queries, hindering the widespread adoption\nof these techniques by database vendors. While existing NL2SQL benchmarks\nprimarily focus on correct query translation, we argue that a benchmark\ndedicated to identifying common errors in NL2SQL translations is equally\nimportant, as accurately detecting these errors is a prerequisite for any\nsubsequent correction-whether performed by humans or models. To address this\ngap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and\ncategorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a\ntwo-level taxonomy to systematically classify semantic errors, covering 9 main\ncategories and 31 subcategories. The benchmark consists of 2,018\nexpert-annotated instances, each containing a natural language query, database\nschema, and SQL query, with detailed error annotations for semantically\nincorrect queries. Through comprehensive experiments, we demonstrate that\ncurrent large language models exhibit significant limitations in semantic error\ndetection, achieving an average detection accuracy of 75.16%. Specifically, our\nmethod successfully detected 106 errors (accounting for 6.91%) in BIRD, a\nwidely-used NL2SQL dataset, which were previously undetected annotation errors.\nThis highlights the importance of semantic error detection in NL2SQL systems.\nThe benchmark is publicly available at https://nl2sql-bugs.github.io/.\n","authors":["Xinyu Liu","Shuyu Shen","Boyan Li","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2503.11984v2.pdf","comment":"12 pages, 6 figures, 4 tables, KDD 2025"},{"id":"http://arxiv.org/abs/2506.12837v1","updated":"2025-06-15T13:15:28Z","published":"2025-06-15T13:15:28Z","title":"Towards Visualizing Electronic Medical Records via Natural Language\n  Queries","summary":"  Electronic medical records (EMRs) contain essential data for patient care and\nclinical research. With the diversity of structured and unstructured data in\nEHR, data visualization is an invaluable tool for managing and explaining these\ncomplexities. However, the scarcity of relevant medical visualization data and\nthe high cost of manual annotation required to develop such datasets pose\nsignificant challenges to advancing medical visualization techniques. To\naddress this issue, we propose an innovative approach using large language\nmodels (LLMs) for generating visualization data without labor-intensive manual\nannotation. We introduce a new pipeline for building text-to-visualization\nbenchmarks suitable for EMRs, enabling users to visualize EMR statistics\nthrough natural language queries (NLQs). The dataset presented in this paper\nprimarily consists of paired text medical records, NLQs, and corresponding\nvisualizations, forming the first large-scale text-to-visual dataset for\nelectronic medical record information called MedicalVis with 35,374 examples.\nAdditionally, we introduce an LLM-based approach called MedCodeT5, showcasing\nits viability in generating EMR visualizations from NLQs, outperforming various\nstrong text-to-visualization baselines. Our work facilitates standardized\nevaluation of EMR visualization methods while providing researchers with tools\nto advance this influential field of application. In a nutshell, this study and\ndataset have the potential to promote advancements in eliciting medical\ninsights through visualization.\n","authors":["Haodi Zhang","Siqi Ning","Qiyong Zheng","Jinyin Nie","Liangjie Zhang","Weicheng Wang","Yuanfeng Song"],"pdf_url":"https://arxiv.org/pdf/2506.12837v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2506.12959v1","updated":"2025-06-15T20:44:56Z","published":"2025-06-15T20:44:56Z","title":"Distributed Computing From First Principles","summary":"  This book on Distributed Computing aims to benefit a diverse audience,\nranging from aspiring engineers, and seasoned researchers, to a wide range of\nprofessionals. Driven by my passion for making the core concepts of distributed\ncomputing accessible, this work is a significant undertaking designed to\nempower individuals from all backgrounds to gain valuable insight. Have you\never wondered how a typical distributed system works under the hood? Are you\nlooking for a pedagogical guide with complete implementations? In this work, we\nhave implemented several foundational algorithms in Distributed Computing.\nWhether your expertise lies in the theoretical foundations or the practical\napplications of the principles of Distributed Systems, this book is for you.\n","authors":["Kenneth Odoh"],"pdf_url":"https://arxiv.org/pdf/2506.12959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12900v1","updated":"2025-06-15T16:15:09Z","published":"2025-06-15T16:15:09Z","title":"Self-Stabilizing Replicated State Machine Coping with Byzantine and\n  Recurring Transient Faults","summary":"  The ability to perform repeated Byzantine agreement lies at the heart of\nimportant applications such as blockchain price oracles or replicated state\nmachines. Any such protocol requires the following properties: (1)\n\\textit{Byzantine fault-tolerance}, because not all participants can be assumed\nto be honest, (2) r\\textit{ecurrent transient fault-tolerance}, because even\nhonest participants may be subject to transient ``glitches'', (3)\n\\textit{accuracy}, because the results of quantitative queries (such as price\nquotes) must lie within the interval of honest participants' inputs, and (4)\n\\textit{self-stabilization}, because it is infeasible to reboot a distributed\nsystem following a fault.\n  This paper presents the first protocol for repeated Byzantine agreement that\nsatisfies the properties listed above. Specifically, starting in an arbitrary\nsystem configuration, our protocol establishes consistency. It preserves\nconsistency in the face of up to $\\lceil n/3 \\rceil -1$ Byzantine participants\n{\\em and} constant recurring (``noise'') transient faults, of up to $\\lceil n/6\n\\rceil-1$ additional malicious transient faults, or even more than $\\lceil n/6\n\\rceil-1$ (uniformly distributed) random transient faults, in each repeated\nByzantine agreement.\n","authors":["Shlomi Dolev","Amit Hendin","Maurice Herlihy","Maria Potop Butucaru","Elad Michael Schiller"],"pdf_url":"https://arxiv.org/pdf/2506.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17246v2","updated":"2025-06-15T13:04:14Z","published":"2024-12-23T03:38:46Z","title":"BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching","summary":"  Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.\n","authors":["Dingyan Zhang","Haotian Wang","Yang Liu","Xingda Wei","Yizhou Shan","Rong Chen","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.17246v2.pdf","comment":"In proceedings of OSDI'25"},{"id":"http://arxiv.org/abs/2503.14729v2","updated":"2025-06-15T12:40:00Z","published":"2025-03-18T20:59:06Z","title":"zkMixer: A Configurable Zero-Knowledge Mixer with Anti-Money Laundering\n  Consensus Protocols","summary":"  We introduce a zero-knowledge cryptocurrency mixer framework that allows\ngroups of users to set up a mixing pool with configurable governance\nconditions, configurable deposit delays, and the ability to refund or\nconfiscate deposits if it is suspected that funds originate from crime. Using a\nconsensus process, group participants can monitor inputs to the mixer and\ndetermine whether the inputs satisfy the mixer conditions. If a deposit is\naccepted by the group, it will enter the mixer and become untraceable. If it is\nnot accepted, the verifiers can freeze the deposit and collectively vote to\neither refund the deposit back to the user, or confiscate the deposit and send\nit to a different user. This behaviour can be used to examine deposits,\ndetermine if they originate from a legitimate source, and if not, return\ndeposits to victims of crime.\n","authors":["Theodoros Constantinides","John Cartlidge"],"pdf_url":"https://arxiv.org/pdf/2503.14729v2.pdf","comment":"10 pages, 8 figures, accepted author manuscript"},{"id":"http://arxiv.org/abs/2506.12737v1","updated":"2025-06-15T06:14:02Z","published":"2025-06-15T06:14:02Z","title":"Cross-architecture universal feature coding via distribution alignment","summary":"  Feature coding has become increasingly important in scenarios where semantic\nrepresentations rather than raw pixels are transmitted and stored. However,\nmost existing methods are architecture-specific, targeting either CNNs or\nTransformers. This design limits their applicability in real-world scenarios\nwhere features from both architectures coexist. To address this gap, we\nintroduce a new research problem: cross-architecture universal feature coding\n(CAUFC), which seeks to build a unified codec that can effectively compress\nfeatures from heterogeneous architectures. To tackle this challenge, we propose\na two-step distribution alignment method. First, we design the format alignment\nmethod that unifies CNN and Transformer features into a consistent 2D token\nformat. Second, we propose the feature value alignment method that harmonizes\nstatistical distributions via truncation and normalization. As a first attempt\nto study CAUFC, we evaluate our method on the image classification task.\nExperimental results demonstrate that our method achieves superior\nrate-accuracy trade-offs compared to the architecture-specific baseline. This\nwork marks an initial step toward universal feature compression across\nheterogeneous model architectures.\n","authors":["Changsheng Gao","Shan Liu","Feng Wu","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2506.12737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12686v1","updated":"2025-06-15T02:13:08Z","published":"2025-06-15T02:13:08Z","title":"Energy-Efficient Real-Time Job Mapping and Resource Management in\n  Mobile-Edge Computing","summary":"  Mobile-edge computing (MEC) has emerged as a promising paradigm for enabling\nInternet of Things (IoT) devices to handle computation-intensive jobs. Due to\nthe imperfect parallelization of algorithms for job processing on servers and\nthe impact of IoT device mobility on data communication quality in wireless\nnetworks, it is crucial to jointly consider server resource allocation and IoT\ndevice mobility during job scheduling to fully benefit from MEC, which is often\noverlooked in existing studies. By jointly considering job scheduling, server\nresource allocation, and IoT device mobility, we investigate the\ndeadline-constrained job offloading and resource management problem in MEC with\nboth communication and computation contentions, aiming to maximize the total\nenergy saved for IoT devices. For the offline version of the problem, where job\ninformation is known in advance, we formulate it as an Integer Linear\nProgramming problem and propose an approximation algorithm, $\\mathtt{LHJS}$,\nwith a constant performance guarantee. For the online version, where job\ninformation is only known upon release, we propose a heuristic algorithm,\n$\\mathtt{LBS}$, that is invoked whenever a job is released. Finally, we conduct\nexperiments with parameters from real-world applications to evaluate their\nperformance.\n","authors":["Chuanchao Gao","Niraj Kumar","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2506.12686v1.pdf","comment":null}]},"2025-06-14T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2506.12488v1","updated":"2025-06-14T12:58:02Z","published":"2025-06-14T12:58:02Z","title":"Redbench: A Benchmark Reflecting Real Workloads","summary":"  Instance-optimized components have made their way into production systems. To\nsome extent, this adoption is due to the characteristics of customer workloads,\nwhich can be individually leveraged during the model training phase. However,\nthere is a gap between research and industry that impedes the development of\nrealistic learned components: the lack of suitable workloads. Existing ones,\nsuch as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail\nto exhibit real workload patterns, particularly distribution shifts.\n  In this paper, we introduce Redbench, a collection of 30 workloads that\nreflect query patterns observed in the real world. The workloads were obtained\nby sampling queries from support benchmarks and aligning them with workload\ncharacteristics observed in Redset.\n","authors":["Skander Krid","Mihail Stoian","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2506.12488v1.pdf","comment":"Eighth International Workshop on Exploiting Artificial Intelligence\n  Techniques for Data Management (aiDM 2025)"},{"id":"http://arxiv.org/abs/2506.12365v1","updated":"2025-06-14T05:55:19Z","published":"2025-06-14T05:55:19Z","title":"Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and\n  Ethics","summary":"  This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), such as enhancing their reasoning skills, adaptability\nto various tasks, increased computational efficiency, and ability to make\nethical decisions. The techniques that have been most effective in bridging the\ngap between human and machine communications include the Chain-of-Thought\nprompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.\nThe improvements in multimodal learning and few-shot or zero-shot techniques\nhave further empowered LLMs to handle complex jobs with minor input. They also\nmanage to do more with less by applying scaling and optimization tricks for\ncomputing power conservation. This survey also offers a broader perspective on\nrecent advancements in LLMs going beyond isolated aspects such as model\narchitecture or ethical concerns. It categorizes emerging methods that enhance\nLLM reasoning, efficiency, and ethical alignment. It also identifies\nunderexplored areas such as interpretability, cross-modal integration and\nsustainability. With recent progress, challenges like huge computational costs,\nbiases, and ethical risks remain constant. Addressing these requires bias\nmitigation, transparent decision-making, and clear ethical guidelines. Future\nresearch will focus on enhancing models ability to handle multiple input,\nthereby making them more intelligent, safe, and reliable.\n","authors":["Asifullah khan","Muhammad Zaeem Khan","Saleha Jamshed","Sadia Ahmad","Aleesha Zainab","Kaynat Khatib","Faria Bibi","Abdul Rehman"],"pdf_url":"https://arxiv.org/pdf/2506.12365v1.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2506.12611v1","updated":"2025-06-14T19:24:35Z","published":"2025-06-14T19:24:35Z","title":"Accelerating Cloud-Based Transcriptomics: Performance Analysis and\n  Optimization of the STAR Aligner Workflow","summary":"  In this work, we explore the Transcriptomics Atlas pipeline adapted for\ncost-efficient and high-throughput computing in the cloud. We propose a\nscalable, cloud-native architecture designed for running a resource-intensive\naligner -- STAR -- and processing tens or hundreds of terabytes of\nRNA-sequencing data. We implement multiple optimization techniques that give\nsignificant execution time and cost reduction. The impact of particular\noptimizations is measured in medium-scale experiments followed by a large-scale\nexperiment that leverages all of them and validates the current design. Early\nstopping optimization allows a reduction in total alignment time by 23%. We\nanalyze the scalability and efficiency of one of the most widely used sequence\naligners. For the cloud environment, we identify one of the most suitable EC2\ninstance types and verify the applicability of spot instances usage.\n","authors":["Piotr Kica","Sabina Lichołai","Michał Orzechowski","Maciej Malawski"],"pdf_url":"https://arxiv.org/pdf/2506.12611v1.pdf","comment":"Accepted at ICCS2025"},{"id":"http://arxiv.org/abs/2502.13839v2","updated":"2025-06-14T15:38:52Z","published":"2025-02-19T15:59:15Z","title":"Performance optimization of BLAS algorithms with band matrices for\n  RISC-V processors","summary":"  The rapid development of RISC-V instruction set architecture presents new\nopportunities and challenges for software developers. Is it sufficient to\nsimply recompile high-performance software optimized for x86-64 onto RISC-V\nCPUs? Are current compilers capable of effectively optimizing C and C++ codes\nor is it necessary to use intrinsics or assembler? Can we analyze and improve\nperformance without well-developed profiling tools? Do standard optimization\ntechniques work? Are there specific RISC-V features that need to be considered?\nThese and other questions require careful consideration. In this paper, we\npresent our experience optimizing four BLAS algorithms for band matrix\noperations on RISC-V processors. We demonstrate how RISC-V-optimized\nimplementations of OpenBLAS algorithms can be significantly accelerated through\nimproved vectorization of computationally intensive loops. Experiments on\nLichee Pi 4A and Banana Pi BPI-F3 devices using RVV 0.7.1 and RVV 1.0 vector\ninstruction sets respectively, show speedups of 1.5x to 10x depending on the\noperation compared to the OpenBLAS baseline. In particular, the successful use\nof vector register grouping with RVV can lead to significant performance\nimprovements.\n","authors":["Anna Pirova","Anastasia Vodeneeva","Konstantin Kovalev","Alexander Ustinov","Evgeny Kozinov","Alexey Liniov","Valentin Volokitin","Iosif Meyerov"],"pdf_url":"https://arxiv.org/pdf/2502.13839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07725v2","updated":"2025-06-14T13:40:09Z","published":"2024-05-13T13:21:06Z","title":"Decentralized Distributed Graph Coloring: Cluster Graphs","summary":"  Graph coloring is fundamental to distributed computing. We give the first\nsub-logarithmic distributed algorithm for coloring cluster graphs. These graphs\nare obtained from the underlying communication network by contracting nodes and\nedges, and they appear frequently as components in the study of distributed\nalgorithms. In particular, we give a $O(\\log^* n)$-round algorithm to\n$(\\Delta+1)$-color cluster graphs of at least polylogarithmic degree. The\nprevious best bound known was $\\operatorname{poly}(\\log n)$ [Flin et al.,\nSODA'24]. This properly generalizes results in the CONGEST model and shows that\ndistributed graph problems can be solved quickly even when the node itself is\ndecentralized.\n","authors":["Maxime Flin","Magnus M. Halldorsson","Alexandre Nolin"],"pdf_url":"https://arxiv.org/pdf/2405.07725v2.pdf","comment":"81 pages, accepted to PODC 2025"},{"id":"http://arxiv.org/abs/2506.12479v1","updated":"2025-06-14T12:43:07Z","published":"2025-06-14T12:43:07Z","title":"AI Flow: Perspectives, Scenarios, and Approaches","summary":"  Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems.\n","authors":["Hongjun An","Sida Huang","Siqi Huang","Ruanjun Li","Yuanzhi Liang","Jiawei Shao","Zihan Wang","Cheng Yuan","Chi Zhang","Hongyuan Zhang","Wenhao Zhuang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2506.12479v1.pdf","comment":"Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom"},{"id":"http://arxiv.org/abs/2506.12425v1","updated":"2025-06-14T09:52:24Z","published":"2025-06-14T09:52:24Z","title":"Optimizing Federated Learning using Remote Embeddings for Graph Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) have experienced rapid advancements in recent\nyears due to their ability to learn meaningful representations from graph data\nstructures. Federated Learning (FL) has emerged as a viable machine learning\napproach for training a shared model on decentralized data, addressing privacy\nconcerns while leveraging parallelism. Existing methods that address the unique\nrequirements of federated GNN training using remote embeddings to enhance\nconvergence accuracy are limited by their diminished performance due to large\ncommunication costs with a shared embedding server. In this paper, we present\nOpES, an optimized federated GNN training framework that uses remote\nneighbourhood pruning, and overlaps pushing of embeddings to the server with\nlocal training to reduce the network costs and training time. The modest drop\nin per-round accuracy due to pre-emptive push of embeddings is out-stripped by\nthe reduction in per-round training time for large and dense graphs like Reddit\nand Products, converging up to $\\approx2\\times$ faster than the\nstate-of-the-art technique using an embedding server and giving up to $20\\%$\nbetter accuracy than vanilla federated GNN learning.\n","authors":["Pranjal Naman","Yogesh Simmhan"],"pdf_url":"https://arxiv.org/pdf/2506.12425v1.pdf","comment":"Preprint of paper in the proceedings of the 30th International\n  European Conference on Parallel and Distributed Computing (Euro-Par)"},{"id":"http://arxiv.org/abs/2504.00791v3","updated":"2025-06-14T09:39:31Z","published":"2025-04-01T13:50:53Z","title":"Optimizing Resource Allocation and Energy Efficiency in Federated Fog\n  Computing for IoT","summary":"  Fog computing significantly enhances the efficiency of IoT applications by\nproviding computation, storage, and networking resources at the edge of the\nnetwork. In this paper, we propose a federated fog computing framework designed\nto optimize resource management, minimize latency, and reduce energy\nconsumption across distributed IoT environments. Our framework incorporates\npredictive scheduling, energy-aware resource allocation, and adaptive mobility\nmanagement strategies. Experimental results obtained from extensive simulations\nusing the OMNeT++ environment demonstrate that our federated approach\noutperforms traditional non-federated architectures in terms of resource\nutilization, latency, energy efficiency, task execution time, and scalability.\nThese findings underline the suitability and effectiveness of the proposed\nframework for supporting sustainable and high-performance IoT services.\n","authors":["Syed Sarmad Shah","Anas Ali"],"pdf_url":"https://arxiv.org/pdf/2504.00791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.11486v3","updated":"2025-06-14T09:27:57Z","published":"2021-10-21T21:23:04Z","title":"Boosting Resource-Constrained Federated Learning Systems with Guessed\n  Updates","summary":"  Federated learning (FL) enables a set of client devices to collaboratively\ntrain a model without sharing raw data. This process, though, operates under\nthe constrained computation and communication resources of edge devices. These\nconstraints combined with systems heterogeneity force some participating\nclients to perform fewer local updates than expected by the server, thus\nslowing down convergence. Exhaustive tuning of hyperparameters in FL,\nfurthermore, can be resource-intensive, without which the convergence is\nadversely affected. In this work, we propose GEL, the guess and learn\nalgorithm. GEL enables constrained edge devices to perform additional learning\nthrough guessed updates on top of gradient-based steps. These guesses are\ngradientless, i.e., participating clients leverage them for free. Our generic\nguessing algorithm (i) can be flexibly combined with several state-of-the-art\nalgorithms including FEDPROX, FEDNOVA, FEDYOGI or SCALEFL; and (ii) achieves\nsignificantly improved performance when the learning rates are not best tuned.\nWe conduct extensive experiments and show that GEL can boost empirical\nconvergence by up to 40% in resource constrained networks while relieving the\nneed for exhaustive learning rate tuning.\n","authors":["Mohamed Yassine Boukhari","Akash Dhasade","Anne-Marie Kermarrec","Rafael Pires","Othmane Safsafi","Rishi Sharma"],"pdf_url":"https://arxiv.org/pdf/2110.11486v3.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.12415v1","updated":"2025-06-14T09:16:27Z","published":"2025-06-14T09:16:27Z","title":"QoS-aware Scheduling of Periodic Real-time Task Graphs on Heterogeneous\n  Pre-occupied MECs","summary":"  In latency-sensitive applications, efficient task scheduling is crucial for\nmaintaining Quality of Service (QoS) while meeting strict timing constraints.\nThis paper addresses the challenge of scheduling periodic tasks structured as\ndirected acyclic graphs (DAGs) within heterogeneous, pre-occupied Mobile Edge\nComputing (MEC) networks. We propose a modified version of the Heterogeneous\nEarliest Finish Time (HEFT) algorithm designed to exploit residual processing\ncapacity in preoccupied MEC environments. Our approach dynamically identifies\nidle intervals on processors to create a feasible hyperperiodic schedule that\nspecifies an allocated virtual machine (VM), task version, and start time for\neach task. This scheduling strategy maximizes the aggregate QoS by optimizing\ntask execution without disrupting the existing periodic workload, while also\nadhering to periodicity, precedence, and resource constraints.Experimental\nresults demonstrate that our method achieves enhanced load balancing and\nresource utilization, highlighting its potential to improve performance in\nheterogeneous MEC infrastructures supporting real-time, periodic applications.\n","authors":["Ashutosh Shankar","Astha Kumari"],"pdf_url":"https://arxiv.org/pdf/2506.12415v1.pdf","comment":"9 pages, 8 figures, 1 table,2 algorithm"},{"id":"http://arxiv.org/abs/2506.12370v1","updated":"2025-06-14T06:36:54Z","published":"2025-06-14T06:36:54Z","title":"Efficient Unified Caching for Accelerating Heterogeneous AI Workloads","summary":"  Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.\n","authors":["Tianze Wang","Yifei Liu","Chen Chen","Pengfei Zuo","Jiawei Zhang","Qizhen Weng","Yin Chen","Zhenhua Han","Jieru Zhao","Quan Chen","Minyi Guo"],"pdf_url":"https://arxiv.org/pdf/2506.12370v1.pdf","comment":"15 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.11447v3","updated":"2025-06-14T06:10:04Z","published":"2024-12-16T04:58:42Z","title":"Decouple and Decompose: Scaling Resource Allocation with DeDe","summary":"  Efficient resource allocation is essential in cloud systems to facilitate\nresource sharing among tenants. However, the growing scale of these\noptimization problems have outpaced commercial solvers commonly employed in\nproduction. To accelerate resource allocation, prior approaches either\ncustomize solutions for narrow domains or impose workload-specific assumptions.\nIn this work, we revisit real-world resource allocation problems and uncover a\ncommon underlying structure: the vast majority of these problems are inherently\nseparable, i.e., they optimize the aggregate utility of individual resource and\ndemand allocations, under separate constraints for each resource and each\ndemand. Building on this observation, we develop DeDe, a scalable and\ntheoretically rooted optimization framework for large-scale resource\nallocation. At the core of DeDe is a decouple-and-decompose approach: it\ndecouples entangled resource and demand constraints and thereby decomposes the\noverall optimization into alternating per-resource and per-demand subproblems\nthat can be solved efficiently and in parallel. We have implemented and\nreleased DeDe as a Python package with a familiar modeling interface. Our\nexperiments on three representative resource allocation tasks -- cluster\nscheduling, traffic engineering, and load balancing -- demonstrate that DeDe\ndelivers significant speedups while generating higher-quality allocations.\n","authors":["Zhiying Xu","Minlan Yu","Francis Y. Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09061v2","updated":"2025-06-14T04:43:24Z","published":"2025-06-06T01:56:07Z","title":"EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge\n  Using Analytical Model","summary":"  This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.\n","authors":["Alyssa Pinnock","Shakya Jayakody","Kawsher A Roxy","Md Rubel Ahmed"],"pdf_url":"https://arxiv.org/pdf/2506.09061v2.pdf","comment":"4 figures, 7 pages, IEEE conference template"},{"id":"http://arxiv.org/abs/2506.02634v2","updated":"2025-06-14T04:39:21Z","published":"2025-06-03T08:51:38Z","title":"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider","summary":"  Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.\n","authors":["Jiahao Wang","Jinbo Han","Xingda Wei","Sijie Shen","Dingyan Zhang","Chenguang Fang","Rong Chen","Wenyuan Yu","Haibo Chen"],"pdf_url":"https://arxiv.org/pdf/2506.02634v2.pdf","comment":"Accepted by USENIX ATC'25"},{"id":"http://arxiv.org/abs/2506.12335v1","updated":"2025-06-14T04:02:35Z","published":"2025-06-14T04:02:35Z","title":"GroupNL: Low-Resource and Robust CNN Design over Cloud and Device","summary":"  It has become mainstream to deploy Convolutional Neural Network (CNN) models\non ubiquitous Internet of Things (IoT) devices with the help of the cloud to\nprovide users with a variety of high-quality services. Most existing methods\nhave two limitations: (i) low robustness in handling corrupted image data\ncollected by IoT devices; and (ii) high consumption of computational and\ntransmission resources. To this end, we propose the Grouped NonLinear\ntransformation generation method (GroupNL), which generates diversified feature\nmaps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to\nimprove the robustness of the CNN model. Specifically, partial convolution\nfilters are designated as seed filters in a convolutional layer, and a small\nset of feature maps, i.e., seed feature maps, are first generated based on\nvanilla convolution operation. Then, we split seed feature maps into several\ngroups, each with a set of different NLFs, to generate corresponding diverse\nfeature maps with in-place nonlinear processing. Moreover, GroupNL effectively\nreduces the parameter transmission between multiple nodes during model training\nby setting the hyperparameters of NLFs to random initialization and not\nupdating them during model training, and reduces the computing resources by\nusing NLFs to generate feature maps instead of most feature maps generated\nbased on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,\nIcons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the\nproposed GroupNL outperforms other state-of-the-art methods in model robust and\ntraining acceleration. Specifically, on the Icons-50 dataset, the accuracy of\nGroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla\nResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN\nwhen trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.\n","authors":["Chuntao Ding","Jianhang Xie","Junna Zhang","Salman Raza","Shangguang Wang","Jiannong Cao"],"pdf_url":"https://arxiv.org/pdf/2506.12335v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2506.12282v1","updated":"2025-06-14T00:07:14Z","published":"2025-06-14T00:07:14Z","title":"Towards Energy-Efficient Distributed Agreement","summary":"  We study fault-tolerant consensus in a variant of the synchronous message\npassing model, where, in each round, every node can choose to be awake or\nasleep. This is known as the sleeping model (Chatterjee, Gmyr, Pandurangan PODC\n2020) and defines the awake complexity (also called \\emph{energy complexity}),\nwhich measures the maximum number of rounds that any node is awake throughout\nthe execution. Only awake nodes can send and receive messages in a given round\nand all messages sent to sleeping nodes are lost. We present new deterministic\nconsensus algorithms that tolerate up to $f<n$ crash failures, where $n$ is the\nnumber of nodes. Our algorithms match the optimal time complexity lower bound\nof $f+1$ rounds. For multi-value consensus, where the input values are chosen\nfrom some possibly large set, we achieve an energy complexity of ${O}(\\lceil\nf^2 / n \\rceil)$ rounds, whereas for binary consensus, we show that ${O}(\\lceil\nf / \\sqrt{n} \\rceil)$ rounds are possible.\n","authors":["Hugo Mirault","Peter Robinson"],"pdf_url":"https://arxiv.org/pdf/2506.12282v1.pdf","comment":"To appear at PODC 2025 as brief announcement"}]},"2025-06-13T00:00:00Z":{"Databases":[{"id":"http://arxiv.org/abs/2506.11986v1","updated":"2025-06-13T17:46:02Z","published":"2025-06-13T17:46:02Z","title":"Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task","summary":"  Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.\n","authors":["Wuzhenghong Wen","Su Pan","yuwei Sun"],"pdf_url":"https://arxiv.org/pdf/2506.11986v1.pdf","comment":"11 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2506.11870v1","updated":"2025-06-13T15:23:07Z","published":"2025-06-13T15:23:07Z","title":"LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection","summary":"  Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations.\n","authors":["Ce Lyu","Minghao Zhao","Yanhao Wang","Liang Jie"],"pdf_url":"https://arxiv.org/pdf/2506.11870v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2206.09032v8","updated":"2025-06-13T10:02:41Z","published":"2022-06-17T22:18:02Z","title":"Conjunctive Queries with Free Access Patterns under Updates","summary":"  We study the problem of answering conjunctive queries with free access\npatterns (CQAPs) under updates. A free access pattern is a partition of the\nfree variables of the query into input and output. The query returns tuples\nover the output variables given a tuple of values over the input variables.\n  We introduce a fully dynamic evaluation approach that works for all CQAPs and\nis optimal for two classes of CQAPs. This approach recovers prior work on the\ndynamic evaluation of conjunctive queries without access patterns.\n  We first give a syntactic characterisation of all CQAPs that admit constant\ntime per single-tuple update and whose output tuples can be enumerated with\nconstant delay given a tuple of values over the input variables.\n  We further chart the complexity trade-off between the preprocessing time,\nupdate time and enumeration delay for a class of CQAPs. For some of these\nCQAPs, our approach achieves optimal, albeit non-constant, update time and\ndelay. This optimality is predicated on the Online Matrix-Vector Multiplication\nconjecture.\n  We finally adapt our approach to the dynamic evaluation of tractable CQAPs\nover probabilistic databases under updates.\n","authors":["Ahmet Kara","Milos Nikolic","Dan Olteanu","Haozhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2206.09032v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11541v1","updated":"2025-06-13T07:52:09Z","published":"2025-06-13T07:52:09Z","title":"OCPQ: Object-Centric Process Querying & Constraints","summary":"  Process querying is used to extract information and insights from process\nexecution data. Similarly, process constraints can be checked against input\ndata, yielding information on which process instances violate them.\nTraditionally, such process mining techniques use case-centric event data as\ninput. However, with the uptake of Object-Centric Process Mining (OCPM),\nexisting querying and constraint checking techniques are no longer applicable.\nObject-Centric Event Data (OCED) removes the requirement to pick a single case\nnotion (i.e., requiring that events belong to exactly one case) and can thus\nrepresent many real-life processes much more accurately. In this paper, we\npresent a novel highly-expressive approach for object-centric process querying,\ncalled OCPQ. It supports a wide variety of applications, including OCED-based\nconstraint checking and filtering. The visual representation of nested queries\nin OCPQ allows users to intuitively read and create queries and constraints. We\nimplemented our approach using (1) a high-performance execution engine backend\nand (2) an easy-to-use editor frontend. Additionally, we evaluated our approach\non a real-life dataset, showing the lack in expressiveness of prior work and\nruntime performance significantly better than the general querying solutions\nSQLite and Neo4j, as well as comparable to the performance-focused DuckDB.\n","authors":["Aaron Küsters","Wil M. P. van der Aalst"],"pdf_url":"https://arxiv.org/pdf/2506.11541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04713v3","updated":"2025-06-13T06:20:03Z","published":"2024-04-06T19:25:00Z","title":"Faster Algorithms for Fair Max-Min Diversification in $\\mathbb{R}^d$","summary":"  The task of extracting a diverse subset from a dataset, often referred to as\nmaximum diversification, plays a pivotal role in various real-world\napplications that have far-reaching consequences. In this work, we delve into\nthe realm of fairness-aware data subset selection, specifically focusing on the\nproblem of selecting a diverse set of size $k$ from a large collection of $n$\ndata points (FairDiv).\n  The FairDiv problem is well-studied in the data management and theory\ncommunity. In this work, we develop the first constant approximation algorithm\nfor FairDiv that runs in near-linear time using only linear space. In contrast,\nall previously known constant approximation algorithms run in super-linear time\n(with respect to $n$ or $k$) and use super-linear space. Our approach achieves\nthis efficiency by employing a novel combination of the Multiplicative Weight\nUpdate method and advanced geometric data structures to implicitly and\napproximately solve a linear program. Furthermore, we improve the efficiency of\nour techniques by constructing a coreset. Using our coreset, we also propose\nthe first efficient streaming algorithm for the FairDiv problem whose\nefficiency does not depend on the distribution of data points. Empirical\nevaluation on million-sized datasets demonstrates that our algorithm achieves\nthe best diversity within a minute. All prior techniques are either highly\ninefficient or do not generate a good solution.\n","authors":["Yash Kurkure","Miles Shamo","Joseph Wiseman","Sainyam Galhotra","Stavros Sintos"],"pdf_url":"https://arxiv.org/pdf/2404.04713v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10886v2","updated":"2025-06-13T01:44:10Z","published":"2025-06-12T16:50:04Z","title":"S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable\n  with DBOS","summary":"  To meet the needs of a large pharmaceutical organization, we set out to\ncreate S3Mirror - an application for transferring large genomic sequencing\ndatasets between S3 buckets quickly, reliably, and observably. We used the DBOS\nTransact durable execution framework to achieve these goals and benchmarked the\nperformance and cost of the application. S3Mirror is an open source DBOS Python\napplication that can run in a variety of environments, including DBOS Cloud\nPro, where it runs as much as 40x faster than AWS DataSync at a fraction of the\ncost. Moreover, S3Mirror is resilient to failures and allows for real-time\nfilewise observability of ongoing and past transfers.\n","authors":["Steven Vasquez-Grinnell","Alex Poliakov"],"pdf_url":"https://arxiv.org/pdf/2506.10886v2.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2506.12213v1","updated":"2025-06-13T20:31:17Z","published":"2025-06-13T20:31:17Z","title":"Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with\n  Heterogeneous LoRA Allocation","summary":"  Federated Learning has recently been utilized to collaboratively fine-tune\nfoundation models across multiple clients. Notably, federated low-rank\nadaptation LoRA-based fine-tuning methods have recently gained attention, which\nallows clients to fine-tune FMs with a small portion of trainable parameters\nlocally. However, most existing methods do not account for the heterogeneous\nresources of clients or lack an effective local training strategy to maximize\nglobal fine-tuning performance under limited resources. In this work, we\npropose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that\nenables clients to collaboratively fine-tune an FM with different local\ntrainable LoRA layers. To ensure its effectiveness, we develop several\nheterogeneous LoRA allocation (HLA) strategies that adaptively allocate local\ntrainable LoRA layers based on clients' resource capabilities and the layer\nimportance. Specifically, based on the dynamic layer importance, we design a\nFisher Information Matrix score-based HLA that leverages dynamic gradient norm\ninformation. To better stabilize the training process, we consider the\nintrinsic importance of LoRA layers and design a Geometrically-Defined HLA\nstrategy. It shapes the collective distribution of trainable LoRA layers into\nspecific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck,\nand Uniform. Moreover, we extend GD-HLA into a randomized version, named\nRandomized Geometrically-Defined HLA, for enhanced model accuracy with\nrandomness. By co-designing the proposed HLA strategies, we incorporate both\nthe dynamic and intrinsic layer importance into the design of our HLA strategy.\nWe evaluate our approach on five datasets under diverse federated LoRA\nfine-tuning settings, covering three levels of data distribution from IID to\nextreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both\neffective and efficient.\n","authors":["Zikai Zhang","Ping Liu","Jiahao Xu","Rui Hu"],"pdf_url":"https://arxiv.org/pdf/2506.12213v1.pdf","comment":"Accepted to TNNLS 2025"},{"id":"http://arxiv.org/abs/2410.04285v2","updated":"2025-06-13T17:24:41Z","published":"2024-10-05T21:11:32Z","title":"MindFlayer SGD: Efficient Parallel SGD in the Presence of Heterogeneous\n  and Random Worker Compute Times","summary":"  We investigate the problem of minimizing the expectation of smooth nonconvex\nfunctions in a distributed setting with multiple parallel workers that are able\nto compute stochastic gradients. A significant challenge in this context is the\npresence of arbitrarily heterogeneous and stochastic compute times among\nworkers, which can severely degrade the performance of existing parallel\nstochastic gradient descent (SGD) methods. While some parallel SGD algorithms\nachieve optimal performance under deterministic but heterogeneous delays, their\neffectiveness diminishes when compute times are random - a scenario not\nexplicitly addressed in their design. To bridge this gap, we introduce\nMindFlayer SGD, a novel parallel SGD method specifically designed to handle\nstochastic and heterogeneous compute times. Through theoretical analysis and\nempirical evaluation, we demonstrate that MindFlayer SGD consistently\noutperforms existing baselines, particularly in environments with heavy-tailed\nnoise. Our results highlight its robustness and scalability, making it a\ncompelling choice for large-scale distributed learning tasks.\n","authors":["Artavazd Maranjyan","Omar Shaikh Omar","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2410.04285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11950v1","updated":"2025-06-13T16:58:55Z","published":"2025-06-13T16:58:55Z","title":"Secure API-Driven Research Automation to Accelerate Scientific Discovery","summary":"  The Secure Scientific Service Mesh (S3M) provides API-driven infrastructure\nto accelerate scientific discovery through automated research workflows. By\nintegrating near real-time streaming capabilities, intelligent workflow\norchestration, and fine-grained authorization within a service mesh\narchitecture, S3M revolutionizes programmatic access to high performance\ncomputing (HPC) while maintaining uncompromising security. This framework\nallows intelligent agents and experimental facilities to dynamically provision\nresources and execute complex workflows, accelerating experimental lifecycles,\nand unlocking the full potential of AI-augmented autonomous science. S3M\nsignals a new era in scientific computing infrastructure that eliminates\ntraditional barriers between researchers, computational resources, and\nexperimental facilities.\n","authors":["Tyler J. Skluzacek","Paul Bryant","A. J. Ruckman","Daniel Rosendo","Suzanne Prentice","Michael J. Brim","Ryan Adamson","Sarp Oral","Mallikarjun Shankar","Rafael Ferreira da Silva"],"pdf_url":"https://arxiv.org/pdf/2506.11950v1.pdf","comment":"PEARC 2025, 5 pages"},{"id":"http://arxiv.org/abs/2506.11800v1","updated":"2025-06-13T14:04:21Z","published":"2025-06-13T14:04:21Z","title":"A retrospective on DISPEED -- Leveraging heterogeneity in a drone swarm\n  for IDS execution","summary":"  Swarms of drones are gaining more and more autonomy and efficiency during\ntheir missions. However, security threats can disrupt their missions'\nprogression. To overcome this problem, Network Intrusion Detection Systems\n((N)IDS) are promising solutions to detect malicious behavior on network\ntraffic. However, modern NIDS rely on resource-hungry machine learning\ntechniques, that can be difficult to deploy on a swarm of drones. The goal of\nthe DISPEED project is to leverage the heterogeneity (execution platforms,\nmemory) of the drones composing a swarm to deploy NIDS. It is decomposed in two\nphases: (1) a characterization phase that consists in characterizing various\nIDS implementations on diverse embedded platforms, and (2) an IDS\nimplementation mapping phase that seeks to develop selection strategies to\nchoose the most relevant NIDS depending on the context. On the one hand, the\ncharacterization phase allowed us to identify 36 relevant IDS implementations\non three different embedded platforms: a Raspberry Pi 4B, a Jetson Xavier, and\na Pynq-Z2. On the other hand, the IDS implementation mapping phase allowed us\nto design both standalone and distributed strategies to choose the best NIDSs\nto deploy depending on the context. The results of the project have led to\nthree publications in international conferences, and one publication in a\njournal.\n","authors":["Vincent Lannurien","Camélia Slimani","Louis Morge-Rollet","Laurent Lemarchand","David Espes","Frédéric Le Roy","Jalil Boukhobza"],"pdf_url":"https://arxiv.org/pdf/2506.11800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03664v2","updated":"2025-06-13T13:54:42Z","published":"2025-03-15T08:48:38Z","title":"PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices","summary":"  The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory.\n","authors":["Yangyijian Liu","Jun Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2504.03664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16783v2","updated":"2025-06-13T13:46:31Z","published":"2025-03-21T01:39:29Z","title":"CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based\n  Proof-of-Stake Blockchains","summary":"  We present a formal analysis of quorum-based State Machine Replication (SMR)\nprotocols in Proof-of-Stake (PoS) systems under a hybrid threat model\ncomprising honest, Byzantine, and rational validators. Our analysis of\ntraditional quorum-based protocols establishes two fundamental impossibility\nresults: (1) in partially synchronous networks, no quorum-based protocol can\nachieve SMR when rational and Byzantine validators comprise more than $1/3$ of\nparticipants, and (2) in synchronous networks, SMR remains impossible when\nrational and Byzantine validators comprise $2/3$ or more of participants.\n  To overcome these limitations, we propose two complementary solutions in our\nhybrid model. First, we introduce a protocol that enforces a bound on the\nvolume of the total transacted amount that is finalized within any time window\n$\\Delta$ and prove that this bound is necessary for secure SMR protocols in our\nmodel. Second, we present the \\emph{strongest chain rule}, which enables\nefficient finalization of transactions when the majority of honest participants\nprovably support the SMR execution. Through empirical analysis of Ethereum and\nCosmos networks, we demonstrate that validator participation consistently\nexceeds the required ${5}/{6}$ threshold, establishing the practical\nfeasibility of our solution in production PoS systems.\n","authors":["Zeta Avarikioti","Eleftherios Kokoris Kogias","Ray Neiheiser","Christos Stefo"],"pdf_url":"https://arxiv.org/pdf/2503.16783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13794v4","updated":"2025-06-13T11:48:05Z","published":"2023-01-31T17:37:48Z","title":"Auctions with Tokens: Monetary Policy as a Mechanism Design Choice","summary":"  I study a repeated auction in which payments are made with a blockchain token\ncreated and initially owned by the auction designer. Unlike the ``virtual\nmoney'' previously examined in mechanism design, such tokens can be saved and\ntraded outside the mechanism. I show that the present-discounted value of\nexpected revenues equals that of a conventional dollar auction, but revenues\naccrue earlier and are less volatile. The optimal monetary policy burns the\ntokens used for payment, a practice common in blockchain-based protocols. I\nalso show that the same outcome can be reproduced in a dollar auction if the\nauctioneer issues a suitable dollar-denominated security. This equivalence\nbreaks down with moral hazard and contracting frictions: with severe\ncontracting frictions the token auction dominates, whereas with mild\ncontracting frictions the dollar auction combined with a dollar-denominated\nfinancial instrument is preferred.\n","authors":["Andrea Canidio"],"pdf_url":"https://arxiv.org/pdf/2301.13794v4.pdf","comment":"Mechanism design, Auctions, Blockchain, Cryptocurrencies, Tokens,\n  Private Money"},{"id":"http://arxiv.org/abs/2506.11644v1","updated":"2025-06-13T10:18:36Z","published":"2025-06-13T10:18:36Z","title":"Bounded Memory in Distributed Networks","summary":"  The recent advent of programmable switches makes distributed algorithms\nreadily deployable in real-world datacenter networks. However, there are still\ngaps between theory and practice that prevent the smooth adaptation of CONGEST\nalgorithms to these environments. In this paper, we focus on the memory\nrestrictions that arise in real-world deployments. We introduce the\n$\\mu$-CONGEST model where on top of the bandwidth restriction, the memory of\nnodes is also limited to $\\mu$ words, in line with real-world systems. We\nprovide fast algorithms of two main flavors.\n  First, we observe that many algorithms in the CONGEST model are\nmemory-intensive and do not work in $\\mu$-CONGEST. A prime example of a family\nof algorithms that use large memory is clique-listing algorithms. We show that\nthe memory issue that arises here cannot be resolved without incurring a cost\nin the round complexity, by establishing a lower bound on the round complexity\nof listing cliques in $\\mu$-CONGEST. We introduce novel techniques to overcome\nthese issues and generalize the algorithms to work within a given memory bound.\nCombined with our lower bound, these provide tight tradeoffs between the\nrunning time and memory of nodes.\n  Second, we show that it is possible to efficiently simulate various families\nof streaming algorithms in $\\mu$-CONGEST. These include fast simulations of\n$p$-pass algorithms, random order streams, and various types of mergeable\nstreaming algorithms.\n  Combining our contributions, we show that we can use streaming algorithms to\nefficiently generate statistics regarding combinatorial structures in the\nnetwork. An example of an end result of this type is that we can efficiently\nidentify and provide the per-color frequencies of the frequent monochromatic\ntriangles in $\\mu$-CONGEST.\n","authors":["Ran Ben Basat","Keren Censor-Hillel","Yi-Jun Chang","Wenchen Han","Dean Leitersdorf","Gregory Schwartzman"],"pdf_url":"https://arxiv.org/pdf/2506.11644v1.pdf","comment":"Accepted at The 37th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA '25). 22 pages"},{"id":"http://arxiv.org/abs/2506.11483v1","updated":"2025-06-13T06:12:31Z","published":"2025-06-13T06:12:31Z","title":"Capsule: Efficient Player Isolation for Datacenters","summary":"  Cloud gaming is increasingly popular. A challenge for cloud provider is to\nkeep datacenter utilization high: a non-trivial task due to application\nvariety. These applications come in different shapes and sizes. So do cloud\ndatacenter resources, e.g., CPUs, GPUs, NPUs.\n  Part of the challenge stems from game engines being predominantly designed to\nrun only one player. One player in a lightweight game might utilize only a\nfraction of the cloud server GPU. The remaining GPU capacity will be left\nunderutilized, an undesired outcome for the cloud provider. We introduce\nCapsule, a mechanism that allows multiple players to seamlessly share one GPU.\n  We implemented Capsule in O3DE, a popular open source game engine. Our\nevaluations show that Capsule can increase datacenter resource utilization by\naccommodating up to 2.25x more players, without degrading player gaming\nexperience. Capsule is also application agnostic. We ran four applications on\nCapsule-based O3DE with no application changes. Our experiences show that\nCapsule design can be adopted by other game engines to increase datacenter\nutilization across cloud providers.\n","authors":["Zhouheng Du","Nima Davari","Li Li","Nodir Kodirov"],"pdf_url":"https://arxiv.org/pdf/2506.11483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03148v3","updated":"2025-06-13T04:29:46Z","published":"2024-10-04T05:15:23Z","title":"Level set-based inverse homogenisation of three-dimensional\n  piezoelectric materials","summary":"  In this paper we use memory-distributed level set-based topology optimisation\nto design three-dimensional periodic piezoelectric materials with enhanced\nproperties. We compare and assess several existing iterative solvers with\nrespect to their weak scalability and find that an approximate Schur complement\npreconditioned generalized minimal residual method method demonstrates the best\nperformance and scalability for solving the piezoelectric homogenisation\nequations. We use the developed techniques to computationally design\nhigh-resolution piezoelectric metamaterials with enhanced stiffness and\npiezoelectric properties that yield new insights into material design for\nsensor, hydrophone, and actuator applications. We suggest two robust structures\nwith no fine-scale features that exhibit enhanced piezoelectric properties\nseveral times larger than those of the base material. We find that level\nset-based topology optimisation is well suited to problems involving\npiezoelectricity and has the advantage of avoiding large regions of\nintermediate density material. Our memory-distributed level-set implementation\nis open source and provided for practitioners in the community.\n","authors":["Zachary J. Wegert","Anthony P. Roberts","Vivien J. Challis"],"pdf_url":"https://arxiv.org/pdf/2410.03148v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11446v1","updated":"2025-06-13T03:55:48Z","published":"2025-06-13T03:55:48Z","title":"Topology-Aware Virtualization over Inter-Core Connected Neural\n  Processing Units","summary":"  With the rapid development of artificial intelligence (AI) applications, an\nemerging class of AI accelerators, termed Inter-core Connected Neural\nProcessing Units (NPU), has been adopted in both cloud and edge computing\nenvironments, like Graphcore IPU, Tenstorrent, etc. Despite their innovative\ndesign, these NPUs often demand substantial hardware resources, leading to\nsuboptimal resource utilization due to the imbalance of hardware requirements\nacross various tasks. To address this issue, prior research has explored\nvirtualization techniques for monolithic NPUs, but has neglected inter-core\nconnected NPUs with the hardware topology.\n  This paper introduces vNPU, the first comprehensive virtualization design for\ninter-core connected NPUs, integrating three novel techniques: (1) NPU route\nvirtualization, which redirects instruction and data flow from virtual NPU\ncores to physical ones, creating a virtual topology; (2) NPU memory\nvirtualization, designed to minimize translation stalls for SRAM-centric and\nNoC-equipped NPU cores, thereby maximizing the memory bandwidth; and (3)\nBest-effort topology mapping, which determines the optimal mapping from all\ncandidate virtual topologies, balancing resource utilization with end-to-end\nperformance. We have developed a prototype of vNPU on both an FPGA platform\n(Chipyard+FireSim) and a simulator (DCRA). Evaluation results indicate that,\ncompared to other virtualization approaches such as unified virtual memory and\nMIG, vNPU achieves up to a 2x performance improvement across various ML models,\nwith only 2% hardware cost.\n","authors":["Dahu Feng","Erhu Feng","Dong Du","Pinjie Xu","Yubin Xia","Haibo Chen","Rong Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.11446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06474v3","updated":"2025-06-13T02:40:25Z","published":"2024-09-10T13:04:13Z","title":"Advancing Hybrid Defense for Byzantine Attacks in Federated Learning","summary":"  Federated learning (FL) enables multiple clients to collaboratively train a\nglobal model without sharing their local data. Recent studies have highlighted\nthe vulnerability of FL to Byzantine attacks, where malicious clients send\npoisoned updates to degrade model performance. In particular, many attacks have\nbeen developed targeting specific aggregation rules, whereas various defense\nmechanisms have been designed for dedicated threat models. This paper studies\nthe resilience of attack-agnostic FL scenarios, where the server lacks prior\nknowledge of both the attackers' strategies and the number of malicious clients\ninvolved. We first introduce hybrid defenses against state-of-the-art attacks.\nOur goal is to identify a general-purpose aggregation rule that performs well\non average while also avoiding worst-case vulnerabilities. By adaptively\nselecting from available defenses, we demonstrate that the server remains\nrobust even when confronted with a substantial proportion of poisoned updates.\nWe also emphasize that existing FL defenses should not automatically be\nregarded as secure, as demonstrated by the newly proposed Trapsetter attack.\nThe proposed attack outperforms other state-of-the-art attacks by further\nincreasing the impact of the attack by 5-15%. Our findings highlight the\nongoing need for the development of Byzantine-resilient aggregation algorithms\nin FL.\n","authors":["Kai Yue","Richeng Jin","Chau-Wai Wong","Huaiyu Dai"],"pdf_url":"https://arxiv.org/pdf/2409.06474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09805v2","updated":"2025-06-13T02:12:50Z","published":"2025-04-14T02:09:13Z","title":"You can lie but not deny: SWMR registers with signature properties in\n  systems with Byzantine processes","summary":"  We define and show how to implement SWMR registers that provide properties of\nunforgeable digital signatures - without actually using such signatures - in\nsystems with Byzantine processes. Intuitively, processes can use these\nregisters to write values as if they are ``signed'', such that these ``signed\nvalues'' can be ``verified'' by any process and ``relayed'' to any process. All\nour register implementations are from SWMR registers, and they work in systems\nwith $n > 3f$ processes, $f$ of which can be Byzantine. We show that these\nimplementations are optimal in the number of Byzantine processes they can\ntolerate: more precisely, we prove that if $3 \\le n \\le 3f$, the registers that\nwe propose cannot be implemented from SWMR registers without using signatures.\nThe registers that we introduce in this paper can also be implemented without\nsignatures in message-passing systems with $n > 3f$ processes, $f$ of which can\nbe Byzantine: this is because SWMR registers can be implemented in such systems\n(Most\\'efaoui, Petrolia, Raynal, and Jard 2017).\n","authors":["Xing Hu","Sam Toueg"],"pdf_url":"https://arxiv.org/pdf/2504.09805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14941v4","updated":"2025-06-13T01:19:34Z","published":"2025-04-21T08:02:25Z","title":"WindVE: Collaborative CPU-NPU Vector Embedding","summary":"  Retrieval-Augmented Generation is a technology that enhances large language\nmodels by integrating information retrieval. In the industry, inference\nservices based on LLMs are highly sensitive to cost-performance ratio,\nprompting the need for improving hardware resource utilization in the inference\nservice. Specifically, vector embedding and retrieval processes take up to 20%\nof the total latency. Therefore, optimizing the utilization of computational\nresources in vector embeddings is crucial for enhancing the cost-performance\nratio of inference processes, which in turn boosts their product\ncompetitiveness.In this paper, we analyze the deployment costs of vector\nembedding technology in inference services, propose a theoretical formula, and\ndetermine through the mathematical expression that increasing the capacity to\nprocess concurrent queries is the key to reducing the deployment costs of\nvector embeddings. Therefore, in this paper, we focus on improving the\nproduct's capability to process concurrent queries. To optimize concurrency\nwithout sacrificing performance, we have designed a queue manager that adeptly\noffloads CPU peak queries. This manager utilizes a linear regression model to\nascertain the optimal queue depths, a critical parameter that significantly\ninfluences the efficacy of the system. We further develop a system named WindVE\nthat uses a CPU-NPU heterogeneous architecture to offload peak concurrent\nqueries, which leverages the performance differences between the two processors\nto effectively manage traffic surges. Through experiments, we compare WindVE to\nthe state-of-the-art vector embedding framework FlagEmbedding, and achieve a\nconcurrency level up to 22.3% higher than the scheme without offloading.\n","authors":["Jinqi Huang","Xuebing Yu","Yi Xiong","Wenjie Huang","Entong Li","Li Zeng","Xin chen"],"pdf_url":"https://arxiv.org/pdf/2504.14941v4.pdf","comment":null}]}}