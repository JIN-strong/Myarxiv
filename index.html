<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-18T00:00:00Z">2025-06-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the
  Filter-and-Verification Tree and MapReduce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Feng, Fangcao Jian, Yixuan Cao, Xiaobin Jian, Jia Wang, Haiyue Feng, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two different collections of sets, the exact set similarity R-S Join
finds all set pairs with similarity no less than a given threshold, which has
widespread applications. While existing algorithms accelerate large-scale R-S
Joins using a two-stage filter-and-verification framework along with the
parallel and distributed MapReduce framework, they suffer from excessive
candidate set pairs, leading to significant I/O, data transfer, and
verification overhead, and ultimately degrading the performance. This paper
proposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate
filtering and verification into a single stage through filter-and-verification
trees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT
(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that
compresses elements and associated sets in memory, enabling single-stage
processing that eliminates the candidate set generation, fast lookups, and
reduced database scans. Correctness proofs are provided. Second, CF-RS-Join
with LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,
which compresses non-branching paths into single nodes and stores them in
linear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and
MR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce
for parallel processing. Empirical studies on 7 real-world datasets have been
conducted to evaluate the performance of the proposed algorithms against
selected existing algorithms in terms of execution time, scalability, memory
usage, and disk usage. Experimental results demonstrate that our algorithm
using MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-based Anchor Embedding for Efficient Exact Subgraph Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00031v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00031v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yang, Zhaonian Zou, Jianxiong Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subgraph matching query is a fundamental problem in graph data management and
has a variety of real-world applications. Several recent works utilize deep
learning (DL) techniques to process subgraph matching queries. Most of them
find approximate subgraph matching results without accuracy guarantees. Unlike
these DL-based inexact subgraph matching methods, we propose a learning-based
exact subgraph matching framework, called \textit{graph neural network
(GNN)-based anchor embedding framework} (GNN-AE). In contrast to traditional
exact subgraph matching methods that rely on creating auxiliary summary
structures online for each specific query, our method indexes small feature
subgraphs in the data graph offline and uses GNNs to perform graph isomorphism
tests for these indexed feature subgraphs to efficiently obtain high-quality
candidates. To make a tradeoff between query efficiency and index storage cost,
we use two types of feature subgraphs, namely anchored subgraphs and anchored
paths. Based on the proposed techniques, we transform the exact subgraph
matching problem into a search problem in the embedding space. Furthermore, to
efficiently retrieve all matches, we develop a parallel matching growth
algorithm and design a cost-based DFS query planning method to further improve
the matching growth algorithm. Extensive experiments on 6 real-world and 3
synthetic datasets indicate that GNN-AE is more efficient than the baselines,
especially outperforming the exploration-based baseline methods by up to 1--2
orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for MRI-based BrainAGE: a multicenter study on
  post-stroke functional outcome prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Roca, Marc Tommasi, Paul Andrey, Aurélien Bellet, Markus D. Schirmer, Hilde Henon, Laurent Puy, Julien Ramon, Grégory Kuchcinski, Martin Bretzner, Renaud Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale
  Heterogeneous Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunming Zhang, Hanlong Liao, Guoming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parallel computing with multiple GPUs has become the dominant paradigm for
machine learning tasks, especially those of large language models (LLMs). To
reduce the latency incurred by inter-GPU communication, a common practice for
parallel tasks has been to allocate GPUs based on their physical proximity.
However, this long-standing assumption has notable limitations, particularly in
large-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs
is irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU
dispatching system based on global perspectives. To tackle the difficulty of
storing massive GPU topology information, LiteGD adopts a computation-aware
design that leverages a lightweight Transformer network trained on sampled
data. Our customized design for network structure ensures both transferability
and scalability. LiteGD also employs a bidirectional tree search approach to
find the optimal GPU dispatching in the data generated in the previous step,
which can identify near-optimal solutions while reducing search overhead. We
implement and evaluate LiteGD in both real and simulated GPU clusters with
homogeneous and heterogeneous interconnects, respectively. Experimental results
demonstrate that LiteGD consistently achieves high GPU bandwidth efficacy
(approximately 90\%) across various cluster configurations and 80\% in
real-world H100 cluster, significantly outperforming conventional default and
interconnect topology-aware dispatching methods, particularly in large-scale
heterogeneous environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 19 figures,7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Metadata Capture and Processing for High-Performance Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Polina Shpilker, Line Pouchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern workflows run on increasingly heterogeneous computing architectures
and with this heterogeneity comes additional complexity. We aim to apply the
FAIR principles for research reproducibility by developing software to collect
metadata annotations for workflows run on HPC systems. We experiment with two
possible formats to uniformly store these metadata, and reorganize the
collected metadata to be as easy to use as possible for researchers studying
their workflow performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimizing Communication for Parallel Symmetric Tensor Times Same Vector
  Computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussam Al Daas, Grey Ballard, Laura Grigori, Suraj Kumar, Kathryn Rouse, Mathieu Vérité
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we focus on the parallel communication cost of multiplying
the same vector along two modes of a $3$-dimensional symmetric tensor. This is
a key computation in the higher-order power method for determining eigenpairs
of a $3$-dimensional symmetric tensor and in gradient-based methods for
computing a symmetric CP decomposition. We establish communication lower bounds
that determine how much data movement is required to perform the specified
computation in parallel. The core idea of the proof relies on extending a key
geometric inequality for $3$-dimensional symmetric computations. We demonstrate
that the communication lower bounds are tight by presenting an optimal
algorithm where the data distribution is a natural extension of the triangle
block partition scheme for symmetric matrices to 3-dimensional symmetric
tensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All is Not Lost: LLM Recovery without Checkpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training LLMs on decentralized and wimpy computation nodes, e.g., multiple
on-spot instances, lowers the training cost and enables model democratization.
The inevitable challenge here is the churn of nodes due to failures and the
operator's scheduling policies, leading to losing a stage - a part of the
model. The conventional approaches to recover from failures are to either use
checkpointing, where periodically a copy of the entire model is sent to an
additional storage, or redundant computation. These approaches yield
significant communication and/or computation overhead even in non-failure cases
and scale poorly in settings with large models. In this paper, we propose,
CheckFree, an efficient recovery method where a failing stage is substituted by
a weighted average of the closest neighboring stages. In contrast to the state
of the art, CheckFree requires no additional computation or storage. However,
because of the nature of averaging neighbouring stages, it can only recover
failures of intermediate stages. We further extend our method to CheckFree+
with out-of-order pipeline execution to tolerate crashes of the first and last
stages. Thanks to out-of-order pipelining, behaviour of those stages is
mimicked by their neighboring ones, which allows CheckFree+ to recover them by
simply copying the weights from the immediate neighbour. To be able to recover
the (de)embedding layers, CheckFree+ copies those layers to the neighboring
stages, which requires relatively small storage overhead. We extensively
evaluate our method on LLaMa models of model sizes from 124M to 1.5B with
varying failure frequencies. In the case of low and medium failure rates
(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant
computation in terms of convergence in wall-clock time by over 12%. Both of our
proposals can be run via our code available at:
https://github.com/gensyn-ai/CheckFree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP,
  and CUDA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nizar ALHafez, Ahmad Kurdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive comparison of three dominant parallel
programming models in High Performance Computing (HPC): Message Passing
Interface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device
Architecture (CUDA). Selecting optimal programming approaches for modern
heterogeneous HPC architectures has become increasingly critical. We
systematically analyze these models across multiple dimensions: architectural
foundations, performance characteristics, domain-specific suitability,
programming complexity, and recent advancements. We examine each model's
strengths, weaknesses, and optimization techniques. Our investigation
demonstrates that MPI excels in distributed memory environments with
near-linear scalability for communication-intensive applications, but faces
communication overhead challenges. OpenMP provides strong performance and
usability in shared-memory systems and loop-centric tasks, though it is limited
by shared memory contention. CUDA offers substantial performance gains for
data-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires
specialized expertise. Performance evaluations across scientific simulations,
machine learning, and data analytics reveal that hybrid approaches combining
two or more models often yield optimal results in heterogeneous environments.
The paper also discusses implementation challenges, optimization best
practices, and emerging trends such as performance portability frameworks,
task-based programming, and the convergence of HPC and Big Data. This research
helps developers and researchers make informed decisions when selecting
programming models for modern HPC applications, emphasizing that the best
choice depends on application requirements, hardware, and development
constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fast Fourier Transforms on the Tenstorrent Wormhole 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Brown, Jake Davies, Felix LeClair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whilst numerous areas of computing have adopted the RISC-V Instruction Set
Architecture (ISA) wholesale in recent years, it is yet to become widespread in
HPC. RISC-V accelerators offer a compelling option where the HPC community can
benefit from the specialisation offered by the open nature of the standard but
without the extensive ecosystem changes required when adopting RISC-V CPUs. In
this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)
algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon
Tenstorrent's Tensix architecture, this technology decouples the movement of
data from compute, potentially offering increased control to the programmer.
Exploring different optimisation techniques to address the bottlenecks inherent
in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is
slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around
8 times less power and consumes around 2.8 times less energy than the CPU when
computing the Fourier transform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author accepted version of paper submitted to RISC-V for HPC ISC
  workshop 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RISC-V for HPC: An update of where we are and main action points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This extended abstract is submitted on behalf of the RISC-V HPC SIG who have
been undertaking an analysis to explore the current state and limitations of
the RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has
been great progress made in recent years, we also highlight limitations and
where effort should be focussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract accepted to the RISC-V Summit Europe 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroid Approximation for Byzantine-Tolerant Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Cambus, Darya Melnyk, Tijana Milentijević, Stefan Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eLLM: Elastic Memory Management Framework for Efficient LLM Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Xu, Rui Zhang, Yi Xiong, Cong Guo, Zihan Liu, Yangjie Zhou, Weiming Hu, Hao Wu, Changxu Shao, Ziqing Wang, Yongjie Yuan, Junping Zhao, Minyi Guo, Jingwen Leng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models are increasingly being deployed in datacenters. Serving
these models requires careful memory management, as their memory usage includes
static weights, dynamic activations, and key-value caches. While static weights
are constant and predictable, dynamic components such as activations and KV
caches change frequently during runtime, presenting significant challenges for
efficient memory management. Modern LLM serving systems typically handle
runtime memory and KV caches at distinct abstraction levels: runtime memory
management relies on static tensor abstractions, whereas KV caches utilize a
page table-based virtualization layer built on top of the tensor abstraction.
This virtualization dynamically manages KV caches to mitigate memory
fragmentation. However, this dual-level approach fundamentally isolates runtime
memory and KV cache management, resulting in suboptimal memory utilization
under dynamic workloads, which can lead to a nearly 20% drop in throughput.
  To address these limitations, we propose eLLM, an elastic memory management
framework inspired by the classical memory ballooning mechanism in operating
systems. The core components of eLLM include: (1) Virtual Tensor Abstraction,
which decouples the virtual address space of tensors from the physical GPU
memory, creating a unified and flexible memory pool; (2) an Elastic Memory
Mechanism that dynamically adjusts memory allocation through runtime memory
inflation and deflation, leveraging CPU memory as an extensible buffer; and (3)
a Lightweight Scheduling Strategy employing SLO-aware policies to optimize
memory utilization and effectively balance performance trade-offs under
stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM
significantly outperforms state-of-the-art systems, 2.32x higher decoding
throughput, and supporting 3x larger batch sizes for 128K-token inputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel Data Object Creation: Towards Scalable Metadata Management in
  High-Performance I/O Library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youjia Li, Robert Latham, Robert Ross, Ankit Agrawal, Alok Choudhary, Wei-Keng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by
large-scale scientific applications to perform I/O tasks in parallel. These I/O
libraries store the metadata such as data types and dimensionality along with
the raw data in the same files. While these libraries are well-optimized for
concurrent access to the raw data, they are designed neither to handle a large
number of data objects efficiently nor to create different data objects
independently by multiple processes, as they require applications to call data
object creation APIs collectively with consistent metadata among all processes.
Applications that process data gathered from remote sensors, such as particle
collision experiments in high-energy physics, may generate data of different
sizes from different sensors and desire to store them as separate data objects.
For such applications, the I/O library's requirement on collective data object
creation can become very expensive, as the cost of metadata consistency check
increases with the metadata volume as well as the number of processes. To
address this limitation, using PnetCDF as an experimental platform, we
investigate solutions in this paper that abide the netCDF file format, as well
as propose a new file header format that enables independent data object
creation. The proposed file header consists of two sections, an index table and
a list of metadata blocks. The index table contains the reference to the
metadata blocks and each block stores metadata of objects that can be created
collectively or independently. The new design achieves a scalable performance,
cutting data object creation times by up to 582x when running on 4096 MPI
processes to create 5,684,800 data objects in parallel. Additionally, the new
method reduces the memory footprints, with each process requiring an amount of
memory space inversely proportional to the number of processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAGs for the Masses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Anoprenko, Andrei Tonkikh, Alexander Spiegelman, Petr Kuznetsov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent approach to building consensus protocols on top of Directed Acyclic
Graphs (DAGs) shows much promise due to its simplicity and stable throughput.
However, as each node in the DAG typically includes a linear number of
references to the nodes in the previous round, prior DAG protocols only scale
up to a certain point when the overhead of maintaining the graph becomes the
bottleneck.
  To enable large-scale deployments of DAG-based protocols, we propose a sparse
DAG architecture, where each node includes only a constant number of references
to random nodes in the previous round. We present a sparse version of Bullshark
-- one of the most prominent DAG-based consensus protocols -- and demonstrate
its improved scalability.
  Remarkably, unlike other protocols that use random sampling to reduce
communication complexity, we manage to avoid sacrificing resilience: the
protocol can tolerate up to $f<n/3$ Byzantine faults (where $n$ is the number
of participants), same as its less scalable deterministic counterpart. The
proposed ``sparse'' methodology can be applied to any protocol that maintains
disseminated system updates and causal relations between them in a graph-like
structure. Our simulations show that the considerable reduction of transmitted
metadata in sparse DAGs results in more efficient network utilization and
better scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computing the Schulze Method for Large-Scale Preference Data Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresa Csar, Martin Lackner, Reinhard Pichler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Schulze method is a voting rule widely used in practice and enjoys many
positive axiomatic properties. While it is computable in polynomial time, its
straight-forward implementation does not scale well for large elections. In
this paper, we develop a highly optimised algorithm for computing the Schulze
method with Pregel, a framework for massively parallel computation of graph
problems, and demonstrate its applicability for large preference data sets. In
addition, our theoretic analysis shows that the Schulze method is indeed
particularly well-suited for parallel computation, in stark contrast to the
related ranked pairs method. More precisely we show that winner determination
subject to the Schulze method is NL-complete, whereas this problem is
P-complete for the ranked pairs method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version of the original 2018 IJCAI conference
  publication. It corrects the P-completeness proof for the ranked pairs method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Candidate-Free R-S Set Similarity Join Algorithm with the
  Filter-and-Verification Tree and MapReduce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Feng, Fangcao Jian, Yixuan Cao, Xiaobin Jian, Jia Wang, Haiyue Feng, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given two different collections of sets, the exact set similarity R-S Join
finds all set pairs with similarity no less than a given threshold, which has
widespread applications. While existing algorithms accelerate large-scale R-S
Joins using a two-stage filter-and-verification framework along with the
parallel and distributed MapReduce framework, they suffer from excessive
candidate set pairs, leading to significant I/O, data transfer, and
verification overhead, and ultimately degrading the performance. This paper
proposes novel candidate-free R-S Join (CF-RS-Join) algorithms that integrate
filtering and verification into a single stage through filter-and-verification
trees (FVTs) and their linear variants (LFVTs). First, CF-RS-Join with FVT
(CF-RS-Join/FVT) is proposed to leverage an innovative FVT structure that
compresses elements and associated sets in memory, enabling single-stage
processing that eliminates the candidate set generation, fast lookups, and
reduced database scans. Correctness proofs are provided. Second, CF-RS-Join
with LFVT (CF-RS-Join/LFVT) is proposed to exploit a more compact Linear FVT,
which compresses non-branching paths into single nodes and stores them in
linear arrays for optimized traversal. Third, MR-CF-RS-Join/FVT and
MR-CF-RS-Join/LFVT have been proposed to extend our approaches using MapReduce
for parallel processing. Empirical studies on 7 real-world datasets have been
conducted to evaluate the performance of the proposed algorithms against
selected existing algorithms in terms of execution time, scalability, memory
usage, and disk usage. Experimental results demonstrate that our algorithm
using MapReduce, i.e., MR-CF-RS-Join/LFVT, achieves the best performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint
  Phase-shift Optimization and Multi-User Power Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangwei Qi, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicular edge computing (VEC) is an emerging technology with significant
potential in the field of internet of vehicles (IoV), enabling vehicles to
perform intensive computational tasks locally or offload them to nearby edge
devices. However, the quality of communication links may be severely
deteriorated due to obstacles such as buildings, impeding the offloading
process. To address this challenge, we introduce the use of Reconfigurable
Intelligent Surfaces (RIS), which provide alternative communication pathways to
assist vehicular communication. By dynamically adjusting the phase-shift of the
RIS, the performance of VEC systems can be substantially improved. In this
work, we consider a RIS-assisted VEC system, and design an optimal scheme for
local execution power, offloading power, and RIS phase-shift, where random task
arrivals and channel variations are taken into account. To address the scheme,
we propose an innovative deep reinforcement learning (DRL) framework that
combines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing
RIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy
Gradient (MADDPG) algorithm for optimizing the power allocation of vehicle user
(VU). Simulation results show that our proposed scheme outperforms the
traditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient
(TD3) and some typical stochastic schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Internet of Things Journal. The
  source code has been released at
  https://github.com/qiongwu86/DDPG-RIS-MADDPG-POWER. arXiv admin note: text
  overlap with arXiv:2406.11318</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Serving Large Language Models on Huawei CloudMatrix384 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs), driven by growing
parameter scales, adoption of mixture-of-experts (MoE) architectures, and
expanding context lengths, imposes unprecedented demands on AI infrastructure.
Traditional AI clusters face limitations in compute intensity, memory
bandwidth, inter-chip communication, and latency, compounded by variable
workloads and strict service-level objectives. Addressing these issues requires
fundamentally redesigned hardware-software integration. This paper introduces
Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in
the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C
NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified
Bus (UB) network, enabling direct all-to-all communication and dynamic pooling
of resources. These features optimize performance for communication-intensive
operations, such as large-scale MoE expert parallelism and distributed
key-value cache access. To fully leverage CloudMatrix384, we propose
CloudMatrix-Infer, an advanced LLM serving solution incorporating three core
innovations: a peer-to-peer serving architecture that independently scales
prefill, decode, and caching; a large-scale expert parallelism strategy
supporting EP320 via efficient UB-based token dispatch; and hardware-aware
optimizations including specialized operators, microbatch-based pipelining, and
INT8 quantization. Evaluation with the DeepSeek-R1 model shows
CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of
6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms
TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s
per NPU even under stringent 15 ms latency constraints, while INT8 quantization
maintains model accuracy across benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Liu, Zhen Liu, Dinghuai Zhang, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-quality and photorealistic 3D assets remains a longstanding
challenge in 3D vision and computer graphics. Although state-of-the-art
generative models, such as diffusion models, have made significant progress in
3D generation, they often fall short of human-designed content due to limited
ability to follow instructions, align with human preferences, or produce
realistic textures, geometries, and physical attributes. In this paper, we
introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement
learning alignment framework for 3D-native diffusion models using 2D rewards.
Built upon the recently proposed Nabla-GFlowNet method, which matches the score
function to reward gradients in a principled manner for reward finetuning, our
Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D
reward signals. Extensive experiments show that, unlike vanilla finetuning
baselines which either struggle to converge or suffer from reward hacking,
Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting
within a few finetuning steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report (21 pages, 21 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Particle-Grid Neural Dynamics for Learning Deformable Object Models from
  RGB-D Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Zhang, Baoyu Li, Kris Hauser, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling the dynamics of deformable objects is challenging due to their
diverse physical properties and the difficulty of estimating states from
limited visual information. We address these challenges with a neural dynamics
framework that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and motion
information while predicting dense particle movements, enabling the modeling of
objects with varied shapes and materials. Particles represent object shapes,
while the spatial grid discretizes the 3D space to ensure spatial continuity
and enhance learning efficiency. Coupled with Gaussian Splattings for visual
rendering, our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos. Through
experiments, we demonstrate that our model learns the dynamics of diverse
objects -- such as ropes, cloths, stuffed animals, and paper bags -- from
sparse-view RGB-D recordings of robot-object interactions, while also
generalizing at the category level to unseen instances. Our approach
outperforms state-of-the-art learning-based and physics-based simulators,
particularly in scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The project page
is available at https://kywind.github.io/pgnd .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kywind.github.io/pgnd</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense SAE Latents Are Features, Not Bugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) are designed to extract interpretable features
from language models by enforcing a sparsity constraint. Ideally, training an
SAE would yield latents that are both sparse and semantically meaningful.
However, many SAE latents activate frequently (i.e., are \emph{dense}), raising
concerns that they may be undesirable artifacts of the training procedure. In
this work, we systematically investigate the geometry, function, and origin of
dense latents and show that they are not only persistent but often reflect
meaningful model representations. We first demonstrate that dense latents tend
to form antipodal pairs that reconstruct specific directions in the residual
stream, and that ablating their subspace suppresses the emergence of new dense
features in retrained SAEs -- suggesting that high density features are an
intrinsic property of the residual space. We then introduce a taxonomy of dense
latents, identifying classes tied to position tracking, context binding,
entropy regulation, letter-specific output signals, part-of-speech, and
principal component reconstruction. Finally, we analyze how these features
evolve across layers, revealing a shift from structural features in early
layers, to semantic features in mid layers, and finally to output-oriented
signals in the last layers of the model. Our findings indicate that dense
latents serve functional roles in language model computation and should not be
dismissed as training noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-Integrated Framework for Learning Fractional-Order Nonlinear
  Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahram Yaghooti, Chengyu Li, Bruno Sinopoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a data-integrated framework for learning the dynamics of
fractional-order nonlinear systems in both discrete-time and continuous-time
settings. The proposed framework consists of two main steps. In the first step,
input-output experiments are designed to generate the necessary datasets for
learning the system dynamics, including the fractional order, the drift vector
field, and the control vector field. In the second step, these datasets, along
with the memory-dependent property of fractional-order systems, are used to
estimate the system's fractional order. The drift and control vector fields are
then reconstructed using orthonormal basis functions. To validate the proposed
approach, the algorithm is applied to four benchmark fractional-order systems.
The results confirm the effectiveness of the proposed framework in learning the
system dynamics accurately. Finally, the same datasets are used to learn
equivalent integer-order models. The numerical comparisons demonstrate that
fractional-order models better capture long-range dependencies, highlighting
the limitations of integer-order representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Upper Bounds for the Matrix Spectral Norm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Naumov, Maxim Rakhuba, Denis Ryapolov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of estimating the spectral norm of a matrix using
only matrix-vector products. We propose a new Counterbalance estimator that
provides upper bounds on the norm and derive probabilistic guarantees on its
underestimation. Compared to standard approaches such as the power method, the
proposed estimator produces significantly tighter upper bounds in both
synthetic and real-world settings. Our method is especially effective for
matrices with fast-decaying spectra, such as those arising in deep learning and
inverse problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranting Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (offline RL) algorithms often require
additional constraints or penalty terms to address distribution shift issues,
such as adding implicit or explicit policy constraints during policy
optimization to reduce the estimation bias of functions. This paper focuses on
a limitation of the Advantage-Weighted Regression family (AWRs), i.e., the
potential for learning over-conservative policies due to data corruption,
specifically the poor explorations in suboptimal offline data. We study it from
two perspectives: (1) how poor explorations impact the theoretically optimal
policy based on KL divergence, and (2) how such poor explorations affect the
approximation of the theoretically optimal policy. We prove that such
over-conservatism is mainly caused by the sensitivity of the loss function for
policy optimization to poor explorations, and the proportion of poor
explorations in offline datasets. To address this concern, we propose
Corruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a
set of robust loss functions during policy optimization and an advantage-based
prioritized experience replay method to filter out poor explorations. Numerical
experiments on the D4RL benchmark show that our method can learn superior
policies from suboptimal offline data, significantly enhancing the performance
of policy optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards
  Improve Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tevin Wang, Chenyan Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rule-based rewards offer a promising strategy for improving reinforcement
learning from human feedback (RLHF), but current approaches often rely on
manual rule engineering. We present AutoRule, a fully automated method for
extracting rules from preference feedback and formulating them into rule-based
rewards. AutoRule extraction operates in three stages: it leverages a reasoning
model to interpret user preferences, identifies candidate rules from the
reasoning chain of these interpretations, and synthesizes them into a unified
rule set. Leveraging the finalized rule set, we employ language-model verifiers
to compute the fraction of rules satisfied by each output, using this metric as
an auxiliary reward alongside the learned reward model during policy
optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\%
relative improvement in length-controlled win rate on AlpacaEval2.0, and a
6.1\% relative gain in second-turn performance on a held-out MT-Bench subset,
compared to a GRPO baseline trained with the same learned reward model but
without the rule-based auxiliary reward. Our analysis confirms that the
extracted rules exhibit good agreement with dataset preference. We find that
AutoRule demonstrates reduced reward hacking compared to a learned reward model
when run over two episodes. Finally, our case study suggests that the extracted
rules capture unique qualities valued in different datasets. The extracted
rules are provided in the appendix, and the code is open-sourced at
https://github.com/cxcscmu/AutoRule.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment
  for Fast and Faithful VLM Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Deria, Adinath Madhavrao Dukre, Feilong Tang, Sara Atito, Sudipta Roy, Muhammad Awais, Muhammad Haris Khan, Imran Razzak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in inference-time search for vision-language
models (VLMs), existing approaches remain both computationally expensive and
prone to unpenalized, low-confidence generations which often lead to persistent
hallucinations. We introduce \textbf{Value-guided Inference with Margin-based
Reward (ViMaR)}, a two-stage inference framework that improves both efficiency
and output fidelity by combining a temporal-difference value model with a
margin-aware reward adjustment. In the first stage, we perform a single pass to
identify the highest-value caption among diverse candidates. In the second
stage, we selectively refine only those segments that were overlooked or
exhibit weak visual grounding, thereby eliminating frequently rewarded
evaluations. A calibrated margin-based penalty discourages low-confidence
continuations while preserving descriptive richness. Extensive experiments
across multiple VLM architectures demonstrate that ViMaR generates captions
that are significantly more reliable, factually accurate, detailed, and
explanatory, while achieving over 4$\times$ speedup compared to existing
value-guided methods. Specifically, we show that ViMaR trained solely on LLaVA
Mistral-7B, \textit{generalizes effectively to guide decoding in a stronger
unseen model}. To further validate this, we adapt the ViMaR to steer generation
in LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption
quality and demonstrating robust cross-model guidance. This cross-model
generalization highlights ViMaR's flexibility and modularity, positioning it as
a scalable and transferable inference-time decoding strategy. Furthermore, when
ViMaR-generated captions are used for self-training, the underlying models
achieve substantial gains across a broad suite of visual comprehension
benchmarks, underscoring the potential of fast, accurate, and self-improving
VLM pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through
  Fuzzing LLM-Augmented Harnesses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Androutsopoulos, Antonio Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Rust ensures memory safety by default, it also permits the use of
unsafe code, which can introduce memory safety vulnerabilities if misused.
Unfortunately, existing tools for detecting memory bugs in Rust typically
exhibit limited detection capabilities, inadequately handle Rust-specific
types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates
static analysis with Large Language Model (LLM)-guided fuzzing harness
generation to effectively identify memory safety vulnerabilities in Rust
libraries, specifically targeting unsafe code. deepSURF introduces a novel
approach for handling generics by substituting them with custom types and
generating tailored implementations for the required traits, enabling the
fuzzer to simulate user-defined behaviors within the fuzzed library.
Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,
facilitating exploration of complex API interactions and significantly
increasing the likelihood of exposing memory safety vulnerabilities. We
evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20
known memory safety bugs and uncovering 6 previously unknown vulnerabilities,
demonstrating clear improvements over state-of-the-art tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Randomization in Greedy Model Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chen, Jason M. Klusowski, Yan Shuo Tan, Chang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining randomized estimators in an ensemble, such as via random forests,
has become a fundamental technique in modern data science, but can be
computationally expensive. Furthermore, the mechanism by which this improves
predictive performance is poorly understood. We address these issues in the
context of sparse linear regression by proposing and analyzing an ensemble of
greedy forward selection estimators that are randomized by feature subsampling
-- at each iteration, the best feature is selected from within a random subset.
We design a novel implementation based on dynamic programming that greatly
improves its computational efficiency. Furthermore, we show via careful
numerical experiments that our method can outperform popular methods such as
lasso and elastic net across a wide range of settings. Next, contrary to
prevailing belief that randomized ensembling is analogous to shrinkage, we show
via numerical experiments that it can simultaneously reduce training error and
degrees of freedom, thereby shifting the entire bias-variance trade-off curve
of the base estimator. We prove this fact rigorously in the setting of
orthogonal features, in which case, the ensemble estimator rescales the
ordinary least squares coefficients with a two-parameter family of logistic
weights, thereby enlarging the model search space. These results enhance our
understanding of random forests and suggest that implicit regularization in
general may have more complicated effects than explicit regularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for MRI-based BrainAGE: a multicenter study on
  post-stroke functional outcome prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Roca, Marc Tommasi, Paul Andrey, Aurélien Bellet, Markus D. Schirmer, Hilde Henon, Laurent Puy, Julien Ramon, Grégory Kuchcinski, Martin Bretzner, Renaud Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GFLC: Graph-based Fairness-aware Label Correction for Fair
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Modar Sulaiman, Kallol Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in machine learning (ML) has a critical importance for building
trustworthy machine learning system as artificial intelligence (AI) systems
increasingly impact various aspects of society, including healthcare decisions
and legal judgments. Moreover, numerous studies demonstrate evidence of unfair
outcomes in ML and the need for more robust fairness-aware methods. However,
the data we use to train and develop debiasing techniques often contains biased
and noisy labels. As a result, the label bias in the training data affects
model performance and misrepresents the fairness of classifiers during testing.
To tackle this problem, our paper presents Graph-based Fairness-aware Label
Correction (GFLC), an efficient method for correcting label noise while
preserving demographic parity in datasets. In particular, our approach combines
three key components: prediction confidence measure, graph-based regularization
through Ricci-flow-optimized graph Laplacians, and explicit demographic parity
incentives. Our experimental findings show the effectiveness of our proposed
approach and show significant improvements in the trade-off between performance
and fairness metrics compared to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Compositional Architecture of Regret in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangxiang Cui, Shu Yang, Tianjin Huang, Wanyu Lin, Lijie Hu, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regret in Large Language Models refers to their explicit regret expression
when presented with evidence contradicting their previously generated
misinformation. Studying the regret mechanism is crucial for enhancing model
reliability and helps in revealing how cognition is coded in neural networks.
To understand this mechanism, we need to first identify regret expressions in
model outputs, then analyze their internal representation. This analysis
requires examining the model's hidden states, where information processing
occurs at the neuron level. However, this faces three key challenges: (1) the
absence of specialized datasets capturing regret expressions, (2) the lack of
metrics to find the optimal regret representation layer, and (3) the lack of
metrics for identifying and analyzing regret neurons. Addressing these
limitations, we propose: (1) a workflow for constructing a comprehensive regret
dataset through strategically designed prompting scenarios, (2) the Supervised
Compression-Decoupling Index (S-CDI) metric to identify optimal regret
representation layers, and (3) the Regret Dominance Score (RDS) metric to
identify regret neurons and the Group Impact Coefficient (GIC) to analyze
activation patterns. Our experimental results successfully identified the
optimal regret representation layer using the S-CDI metric, which significantly
enhanced performance in probe classification experiments. Additionally, we
discovered an M-shaped decoupling pattern across model layers, revealing how
information processing alternates between coupling and decoupling phases.
Through the RDS metric, we categorized neurons into three distinct functional
groups: regret neurons, non-regret neurons, and dual neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrel J. Perin, Runjin Chen, Xuxi Chen, Nina S. T. Hirata, Zhangyang Wang, Junyuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become indispensable in real-world
applications. However, their widespread adoption raises significant safety
concerns, particularly in responding to socially harmful questions. Despite
substantial efforts to improve model safety through alignment, aligned models
can still have their safety protections undermined by subsequent fine-tuning -
even when the additional training data appears benign. In this paper, we
empirically demonstrate that this vulnerability stems from the sensitivity of
safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building
on this insight, we propose a novel training-free method, termed Low-Rank
Extrapolation (LoX), to enhance safety robustness by extrapolating the safety
subspace of an aligned LLM. Our experimental results confirm the effectiveness
of LoX, demonstrating significant improvements in robustness against both
benign and malicious fine-tuning attacks while preserving the model's
adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute
reductions in attack success rates (ASR) facing benign or malicious fine-tuning
attacks. By investigating the ASR landscape of parameters, we attribute the
success of LoX to that the extrapolation moves LLM parameters to a flatter
zone, thereby less sensitive to perturbations. The code is available at
github.com/VITA-Group/LoX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and
  Charts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Foroutan, Angelika Romanou, Matin Ansaripour, Julian Martin Eisenschlos, Karl Aberer, Rémi Lebret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are fundamental to preserving and disseminating information, often
incorporating complex layouts, tables, and charts that pose significant
challenges for automatic document understanding (DU). While vision-language
large models (VLLMs) have demonstrated improvements across various tasks, their
effectiveness in processing long-context vision inputs remains unclear. This
paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice
questions (MCQs) designed to evaluate cross-modal reasoning over tables and
charts extracted from 4,000 Wikipedia pages spanning seven distinct topics.
Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring
models to synthesize information from multiple modalities. We evaluate 12
state-of-the-art vision-language models, revealing that while proprietary
models achieve ~70% accuracy when provided with direct context, their
performance deteriorates significantly when retrieval from long documents is
required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this
setting, whereas open-source models perform considerably worse, with a maximum
accuracy of 27%. These findings underscore the challenges of long-context,
multi-modal reasoning and establish WikiMixQA as a crucial benchmark for
advancing document understanding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2025 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Efficient Differentially Private Training with Gradient Random
  Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Mulrooney, Devansh Gupta, James Flemings, Huanyu Zhang, Murali Annavaram, Meisam Razaviyayn, Xinwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) protects sensitive data during neural network
training, but standard methods like DP-Adam suffer from high memory overhead
due to per-sample gradient clipping, limiting scalability. We introduce
DP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly
reduces memory usage while maintaining utility on par with first-order DP
approaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces
three key modifications: (1) gradients are privatized after projection, (2)
random Gaussian matrices replace SVD-based subspaces, and (3) projection is
applied during backpropagation. These contributions eliminate the need for
costly SVD computations, enable substantial memory savings, and lead to
improved utility. Despite operating in lower-dimensional subspaces, our
theoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off
comparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE
can reduce the memory footprint of DP training without sacrificing accuracy or
training time. In particular, DP-GRAPE reduces memory usage by over 63% when
pre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as
compared to DP-Adam, while achieving similar performance. We further
demonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with
up to 6.7 billion parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Tu Nguyen Thi Ngoc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time mesh smoothing at scale remains a formidable challenge: classical
Ricci-flow solvers demand costly global updates, while greedy heuristics suffer
from slow convergence or brittle tuning. We present MicroRicci, the first truly
self-tuning, local Ricci-flow solver that borrows ideas from coding theory and
packs them into just 1K + 200 parameters. Its primary core is a greedy
syndrome-decoding step that pinpoints and corrects the largest curvature error
in O(E) time, augmented by two tiny neural modules that adaptively choose
vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,
MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),
tightens curvature spread from 0.19 to 0.185, and achieves a remarkable
UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per
iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration
over state-of-the-art methods. MicroRicci's combination of linear-time updates,
automatic hyperparameter adaptation, and high-quality geometric and perceptual
results makes it well suited for real-time, resource-limited applications in
graphics, simulation, and related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Managing Complex Failure Analysis Workflows with LLM-based Reasoning and
  Acting Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aline Dobrovsky, Konstantin Schekotihin, Christian Burmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Failure Analysis (FA) is a highly intricate and knowledge-intensive process.
The integration of AI components within the computational infrastructure of FA
labs has the potential to automate a variety of tasks, including the detection
of non-conformities in images, the retrieval of analogous cases from diverse
data sources, and the generation of reports from annotated images. However, as
the number of deployed AI models increases, the challenge lies in orchestrating
these components into cohesive and efficient workflows that seamlessly
integrate with the FA process.
  This paper investigates the design and implementation of a Large Language
Model (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their
analysis cases. The LPA integrates LLMs with advanced planning capabilities and
external tool utilization, enabling autonomous processing of complex queries,
retrieval of relevant data from external systems, and generation of
human-readable responses. Evaluation results demonstrate the agent's
operational effectiveness and reliability in supporting FA tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Agnostic Experts Composition for Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Quarantiello, Andrea Cossu, Vincenzo Lomonaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositionality is one of the fundamental abilities of the human reasoning
process, that allows to decompose a complex problem into simpler elements. Such
property is crucial also for neural networks, especially when aiming for a more
efficient and sustainable AI framework. We propose a compositional approach by
ensembling zero-shot a set of expert models, assessing our methodology using a
challenging benchmark, designed to test compositionality capabilities. We show
that our Expert Composition method is able to achieve a much higher accuracy
than baseline algorithms while requiring less computational resources, hence
being more efficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explainable Indoor Localization: Interpreting Neural Network
  Learning on Wi-Fi Fingerprints Using Logic Gates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danish Gufran, Sudeep Pasricha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor localization using deep learning (DL) has demonstrated strong accuracy
in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing
DL frameworks function as black-box models, offering limited insight into how
predictions are made or how models respond to real-world noise over time. This
lack of interpretability hampers our ability to understand the impact of
temporal variations - caused by environmental dynamics - and to adapt models
for long-term reliability. To address this, we introduce LogNet, a novel logic
gate-based framework designed to interpret and enhance DL-based indoor
localization. LogNet enables transparent reasoning by identifying which access
points (APs) are most influential for each reference point (RP) and reveals how
environmental noise disrupts DL-driven localization decisions. This
interpretability allows us to trace and diagnose model failures and adapt DL
systems for more stable long-term deployments. Evaluations across multiple
real-world building floorplans and over two years of temporal variation show
that LogNet not only interprets the internal behavior of DL models but also
improves performance-achieving up to 1.1x to 2.8x lower localization error,
3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to
prior DL-based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAILOC: Domain-Incremental Learning for Indoor Localization using
  Smartphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Singampalli, Danish Gufran, Sudeep Pasricha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wi-Fi fingerprinting-based indoor localization faces significant challenges
in real-world deployments due to domain shifts arising from device
heterogeneity and temporal variations within indoor environments. Existing
approaches often address these issues independently, resulting in poor
generalization and susceptibility to catastrophic forgetting over time. In this
work, we propose DAILOC, a novel domain-incremental learning framework that
jointly addresses both temporal and device-induced domain shifts. DAILOC
introduces a novel disentanglement strategy that separates domain shifts from
location-relevant features using a multi-level variational autoencoder.
Additionally, we introduce a novel memory-guided class latent alignment
mechanism to address the effects of catastrophic forgetting over time.
Experiments across multiple smartphones, buildings, and time instances
demonstrate that DAILOC significantly outperforms state-of-the-art methods,
achieving up to 2.74x lower average error and 4.6x lower worst-case error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Gradients for Stable Learning at Scale in Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roger Creus Castanyer, Johan Obando-Ceron, Lu Li, Pierre-Luc Bacon, Glen Berseth, Aaron Courville, Pablo Samuel Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling deep reinforcement learning networks is challenging and often results
in degraded performance, yet the root causes of this failure mode remain poorly
understood. Several recent works have proposed mechanisms to address this, but
they are often complex and fail to highlight the causes underlying this
difficulty. In this work, we conduct a series of empirical analyses which
suggest that the combination of non-stationarity with gradient pathologies, due
to suboptimal architectural choices, underlie the challenges of scale. We
propose a series of direct interventions that stabilize gradient flow, enabling
robust performance across a range of network depths and widths. Our
interventions are simple to implement and compatible with well-established
algorithms, and result in an effective mechanism that enables strong
performance even at large scales. We validate our findings on a variety of
agents and suites of environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Algorithms in the Limit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hristo Papazov, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of learning computable functions in the limit
by extending Gold's inductive inference framework to incorporate
\textit{computational observations} and \textit{restricted input sources}.
Complimentary to the traditional Input-Output Observations, we introduce
Time-Bound Observations, and Policy-Trajectory Observations to study the
learnability of general recursive functions under more realistic constraints.
While input-output observations do not suffice for learning the class of
general recursive functions in the limit, we overcome this learning barrier by
imposing computational complexity constraints or supplementing with approximate
time-bound observations. Further, we build a formal framework around
observations of \textit{computational agents} and show that learning computable
functions from policy trajectories reduces to learning rational functions from
input and output, thereby revealing interesting connections to finite-state
transducer inference. On the negative side, we show that computable or
polynomial-mass characteristic sets cannot exist for the class of linear-time
computable functions even for policy-trajectory observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLT 2025. This version matches the proceedings version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing Polysemanticity with PRISM: A Multi-Concept Feature
  Description Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Kopf, Nils Feldhus, Kirill Bykov, Philine Lou Bommer, Anna Hedström, Marina M. -C. Höhne, Oliver Eberle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated interpretability research aims to identify concepts encoded in
neural network features to enhance human understanding of model behavior.
Current feature description methods face two critical challenges: limited
robustness and the flawed assumption that each neuron encodes only a single
concept (monosemanticity), despite growing evidence that neurons are often
polysemantic. This assumption restricts the expressiveness of feature
descriptions and limits their ability to capture the full range of behaviors
encoded in model internals. To address this, we introduce Polysemantic FeatuRe
Identification and Scoring Method (PRISM), a novel framework that captures the
inherent complexity of neural network features. Unlike prior approaches that
assign a single description per feature, PRISM provides more nuanced
descriptions for both polysemantic and monosemantic features. We apply PRISM to
language models and, through extensive benchmarking against existing methods,
demonstrate that our approach produces more accurate and faithful feature
descriptions, improving both overall description quality (via a description
score) and the ability to capture distinct concepts when polysemanticity is
present (via a polysemanticity score).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simplified Analysis of SGD for Linear Regression with Weight Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru Meterez, Depen Morwani, Costin-Andrei Oncescu, Jingfeng Wu, Cengiz Pehlevan, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theoretically understanding stochastic gradient descent (SGD) in
overparameterized models has led to the development of several optimization
algorithms that are widely used in practice today. Recent work
by~\citet{zou2021benign} provides sharp rates for SGD optimization in linear
regression using constant learning rate, both with and without tail iterate
averaging, based on a bias-variance decomposition of the risk. In our work, we
provide a simplified analysis recovering the same bias and variance bounds
provided in~\citep{zou2021benign} based on simple linear algebra tools,
bypassing the requirement to manipulate operators on positive semi-definite
(PSD) matrices. We believe our work makes the analysis of SGD on linear
regression very accessible and will be helpful in further analyzing
mini-batching and learning rate scheduling, leading to improvements in the
training of realistic models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music
  <span class="highlight-title">Diffusion Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teysir Baoueb, Xiaoyu Bie, Xi Wang, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breakthroughs in text-to-music generation models are transforming the
creative landscape, equipping musicians with innovative tools for composition
and experimentation like never before. However, controlling the generation
process to achieve a specific desired outcome remains a significant challenge.
Even a minor change in the text prompt, combined with the same random seed, can
drastically alter the generated piece. In this paper, we explore the
application of existing text-to-music diffusion models for instrument editing.
Specifically, for an existing audio track, we aim to leverage a pretrained
text-to-music diffusion model to edit the instrument while preserving the
underlying content. Based on the insight that the model first focuses on the
overall structure or content of the audio, then adds instrument information,
and finally refines the quality, we show that selecting a well-chosen
intermediate timestep, identified through an instrument classifier, yields a
balance between preserving the original piece's content and achieving the
desired timbre. Our method does not require additional training of the
text-to-music diffusion model, nor does it compromise the generation process's
speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Vu Anh, Nguyen Viet Anh, Mehmet Dik, Luong Van Nghia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has become a common strategy for
updating large language model (LLM) responses with current, external
information. However, models may still rely on memorized training data, bypass
the retrieved evidence, and produce contaminated outputs. We introduce
Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects
such behavior without requiring model access or retraining. RePCS compares two
inference paths: (i) a parametric path using only the query, and (ii) a
retrieval-augmented path using both the query and retrieved context by
computing the Kullback-Leibler (KL) divergence between their output
distributions. A low divergence suggests that the retrieved context had minimal
impact, indicating potential memorization. This procedure is model-agnostic,
requires no gradient or internal state access, and adds only a single
additional forward pass. We further derive PAC-style guarantees that link the
KL threshold to user-defined false positive and false negative rates. On the
Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result
outperforms the strongest prior method by 6.5 percentage points while keeping
latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,
black-box safeguard to verify whether a RAG system meaningfully leverages
retrieval, making it especially valuable in safety-critical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Over-squashing in Spatiotemporal Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Marisca, Jacob Bamberger, Cesare Alippi, Michael M. Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have achieved remarkable success across various
domains. However, recent theoretical advances have identified fundamental
limitations in their information propagation capabilities, such as
over-squashing, where distant nodes fail to effectively exchange information.
While extensively studied in static contexts, this issue remains unexplored in
Spatiotemporal GNNs (STGNNs), which process sequences associated with graph
nodes. Nonetheless, the temporal dimension amplifies this challenge by
increasing the information that must be propagated. In this work, we formalize
the spatiotemporal over-squashing problem and demonstrate its distinct
characteristics compared to the static case. Our analysis reveals that
counterintuitively, convolutional STGNNs favor information propagation from
points temporally distant rather than close in time. Moreover, we prove that
architectures that follow either time-and-space or time-then-space processing
paradigms are equally affected by this phenomenon, providing theoretical
justification for computationally efficient implementations. We validate our
findings on synthetic and real-world datasets, providing deeper insights into
their operational dynamics and principled guidance for more effective designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights on Adversarial Attacks for Tabular Machine Learning via a
  Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salijona Dyrmishi, Mohamed Djilani, Thibault Simonetto, Salah Ghamizi, Maxime Cordy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks in machine learning have been extensively reviewed in
areas like computer vision and NLP, but research on tabular data remains
scattered. This paper provides the first systematic literature review focused
on adversarial attacks targeting tabular machine learning models. We highlight
key trends, categorize attack strategies and analyze how they address practical
considerations for real-world applicability. Additionally, we outline current
challenges and open research questions. By offering a clear and structured
overview, this review aims to guide future efforts in understanding and
addressing adversarial vulnerabilities in tabular machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review at ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-dependent density estimation using binary classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnimitra Dasgupta, Javier Murgoitio-Esandi, Ali Fardisi, Assad A Oberai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data-driven method to learn the time-dependent probability
density of a multivariate stochastic process from sample paths, assuming that
the initial probability density is known and can be evaluated. Our method uses
a novel time-dependent binary classifier trained using a contrastive
estimation-based objective that trains the classifier to discriminate between
realizations of the stochastic process at two nearby time instants.
Significantly, the proposed method explicitly models the time-dependent
probability distribution, which means that it is possible to obtain the value
of the probability density within the time horizon of interest. Additionally,
the input before the final activation in the time-dependent classifier is a
second-order approximation to the partial derivative, with respect to time, of
the logarithm of the density. We apply the proposed approach to approximate the
time-dependent probability density functions for systems driven by stochastic
excitations. We also use the proposed approach to synthesize new samples of a
random vector from a given set of its realizations. In such applications, we
generate sample paths necessary for training using stochastic interpolants.
Subsequently, new samples are generated using gradient-based Markov chain Monte
Carlo methods because automatic differentiation can efficiently provide the
necessary gradient. Further, we demonstrate the utility of an explicit
approximation to the time-dependent probability density function through
applications in unsupervised outlier detection. Through several numerical
experiments, we show that the proposed method accurately reconstructs complex
time-dependent, multi-modal, and near-degenerate densities, scales effectively
to moderately high-dimensional problems, and reliably detects rare events among
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Hyperbole and Metaphor Detection with Their Bidirectional
  Dynamic Interaction and Emotion Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zheng, Sihang Wang, Hao Fei, Zuquan Peng, Fei Li, Jianming Fu, Chong Teng, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based hyperbole and metaphor detection are of great significance for
natural language processing (NLP) tasks. However, due to their semantic
obscurity and expressive diversity, it is rather challenging to identify them.
Existing methods mostly focus on superficial text features, ignoring the
associations of hyperbole and metaphor as well as the effect of implicit
emotion on perceiving these rhetorical devices. To implement these hypotheses,
we propose an emotion-guided hyperbole and metaphor detection framework based
on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis
module deeply mines the emotion connotations behind hyperbole and metaphor.
Next, the emotion-based domain mapping module identifies the target and source
domains to gain a deeper understanding of the implicit meanings of hyperbole
and metaphor. Finally, the bidirectional dynamic interaction module enables the
mutual promotion between hyperbole and metaphor. Meanwhile, a verification
mechanism is designed to ensure detection accuracy and reliability. Experiments
show that EmoBi outperforms all baseline methods on four datasets.
Specifically, compared to the current SoTA, the F1 score increased by 28.1% for
hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on
the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore
the effectiveness and potential of our approach for advancing hyperbole and
metaphor detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-level Certified Explanations via Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaa Anani, Tobias Lorenz, Mario Fritz, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-hoc attribution methods aim to explain deep learning predictions by
highlighting influential input pixels. However, these explanations are highly
non-robust: small, imperceptible input perturbations can drastically alter the
attribution map while maintaining the same prediction. This vulnerability
undermines their trustworthiness and calls for rigorous robustness guarantees
of pixel-level attribution scores. We introduce the first certification
framework that guarantees pixel-level robustness for any black-box attribution
method using randomized smoothing. By sparsifying and smoothing attribution
maps, we reformulate the task as a segmentation problem and certify each
pixel's importance against $\ell_2$-bounded perturbations. We further propose
three evaluation metrics to assess certified robustness, localization, and
faithfulness. An extensive evaluation of 12 attribution methods across 5
ImageNet models shows that our certified attributions are robust,
interpretable, and faithful, enabling reliable use in downstream tasks. Our
code is at https://github.com/AlaaAnani/certified-attributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPARE: Single-Pass Annotation with Reference-Guided Evaluation for
  Automatic Process Supervision and Reward Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Imbesat Hassan Rizvi, Xiaodan Zhu, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process or step-wise supervision has played a crucial role in advancing
complex multi-step reasoning capabilities of Large Language Models (LLMs).
However, efficient, high-quality automated process annotation remains a
significant challenge. To address this, we introduce Single-Pass Annotation
with Reference-Guided Evaluation (SPARE), a novel structured framework that
enables single-pass, per-step annotation by aligning each solution step to one
or multiple steps in a reference solution, accompanied by explicit reasoning
for evaluation. We show that reference-guided step-level evaluation effectively
facilitates process supervision on four datasets spanning three domains:
mathematical reasoning, multi-hop compositional question answering, and spatial
reasoning. We demonstrate that SPARE, when compared to baselines, improves
reasoning performance when used for: (1) fine-tuning models in an offline RL
setup for inference-time greedy-decoding, and (2) training reward models for
ranking/aggregating multiple LLM-generated outputs. Additionally, SPARE
achieves competitive performance on challenging mathematical datasets while
offering 2.6 times greater efficiency, requiring only 38% of the runtime,
compared to tree search-based automatic annotation. The codebase, along with a
trained SPARE-PRM model, is publicly released to facilitate further research
and reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages main content, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIT-LVM: Structured Regularization for Interaction Terms in Linear
  Predictors using Latent Variable Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Nemati, Zhipeng Huang, Kevin S. Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some of the simplest, yet most frequently used predictors in statistics and
machine learning use weighted linear combinations of features. Such linear
predictors can model non-linear relationships between features by adding
interaction terms corresponding to the products of all pairs of features. We
consider the problem of accurately estimating coefficients for interaction
terms in linear predictors. We hypothesize that the coefficients for different
interaction terms have an approximate low-dimensional structure and represent
each feature by a latent vector in a low-dimensional space. This
low-dimensional representation can be viewed as a structured regularization
approach that further mitigates overfitting in high-dimensional settings beyond
standard regularizers such as the lasso and elastic net. We demonstrate that
our approach, called LIT-LVM, achieves superior prediction accuracy compared to
elastic net and factorization machines on a wide variety of simulated and real
data, particularly when the number of interaction terms is high compared to the
number of samples. LIT-LVM also provides low-dimensional latent representations
for features that are useful for visualizing and analyzing their relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating User-steerable Projections with Interactive Semantic Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur André Oliveira, Mateus Espadoto, Roberto Hirata Jr., Roberto M. Cesar Jr., Alex C. Telea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction (DR) techniques map high-dimensional data into
lower-dimensional spaces. Yet, current DR techniques are not designed to
explore semantic structure that is not directly available in the form of
variables or class labels. We introduce a novel user-guided projection
framework for image and text data that enables customizable, interpretable,
data visualizations via zero-shot classification with Multimodal Large Language
Models (MLLMs). We enable users to steer projections dynamically via
natural-language guiding prompts, to specify high-level semantic relationships
of interest to the users which are not explicitly present in the data
dimensions. We evaluate our method across several datasets and show that it not
only enhances cluster separation, but also transforms DR into an interactive,
user-driven process. Our approach bridges the gap between fully automated DR
techniques and human-centered data exploration, offering a flexible and
adaptive way to tailor projections to specific analytical needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Creative Learning via Metropolis-Hastings Interaction between Humans
  and AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Okumura, Tadahiro Taniguchi, Akira Taniguchi, Yoshinobu Hagiwara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose co-creative learning as a novel paradigm where humans and AI,
i.e., biological and artificial agents, mutually integrate their partial
perceptual information and knowledge to construct shared external
representations, a process we interpret as symbol emergence. Unlike traditional
AI teaching based on unilateral knowledge transfer, this addresses the
challenge of integrating information from inherently different modalities. We
empirically test this framework using a human-AI interaction model based on the
Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference
mechanism. In an online experiment, 69 participants played a joint attention
naming game (JA-NG) with one of three computer agent types (MH-based,
always-accept, or always-reject) under partial observability. Results show that
human-AI pairs with an MH-based agent significantly improved categorization
accuracy through interaction and achieved stronger convergence toward a shared
sign system. Furthermore, human acceptance behavior aligned closely with the
MH-derived acceptance probability. These findings provide the first empirical
evidence for co-creative learning emerging in human-AI dyads via MHNG-based
interaction. This suggests a promising path toward symbiotic AI systems that
learn with humans, rather than from them, by dynamically aligning perceptual
experiences, opening a new venue for symbiotic AI alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Vu Anh, Mehmet Dik, Nguyen Viet Anh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical graphs often exhibit tree-like branching patterns, a structural
property that challenges the design of traditional graph filters. We introduce
a boundary-weighted operator that rescales each edge according to how far its
endpoints drift toward the graph's Gromov boundary. Using Busemann functions on
delta-hyperbolic networks, we prove a closed-form upper bound on the operator's
spectral norm: every signal loses a curvature-controlled fraction of its energy
at each pass. The result delivers a parameter-free, lightweight filter whose
stability follows directly from geometric first principles, offering a new
analytic tool for graph signal processing on data with dense or hidden
hierarchical structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All is Not Lost: LLM Recovery without Checkpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolay Blagoev, Oğuzhan Ersoy, Lydia Yiyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training LLMs on decentralized and wimpy computation nodes, e.g., multiple
on-spot instances, lowers the training cost and enables model democratization.
The inevitable challenge here is the churn of nodes due to failures and the
operator's scheduling policies, leading to losing a stage - a part of the
model. The conventional approaches to recover from failures are to either use
checkpointing, where periodically a copy of the entire model is sent to an
additional storage, or redundant computation. These approaches yield
significant communication and/or computation overhead even in non-failure cases
and scale poorly in settings with large models. In this paper, we propose,
CheckFree, an efficient recovery method where a failing stage is substituted by
a weighted average of the closest neighboring stages. In contrast to the state
of the art, CheckFree requires no additional computation or storage. However,
because of the nature of averaging neighbouring stages, it can only recover
failures of intermediate stages. We further extend our method to CheckFree+
with out-of-order pipeline execution to tolerate crashes of the first and last
stages. Thanks to out-of-order pipelining, behaviour of those stages is
mimicked by their neighboring ones, which allows CheckFree+ to recover them by
simply copying the weights from the immediate neighbour. To be able to recover
the (de)embedding layers, CheckFree+ copies those layers to the neighboring
stages, which requires relatively small storage overhead. We extensively
evaluate our method on LLaMa models of model sizes from 124M to 1.5B with
varying failure frequencies. In the case of low and medium failure rates
(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant
computation in terms of convergence in wall-clock time by over 12%. Both of our
proposals can be run via our code available at:
https://github.com/gensyn-ai/CheckFree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Warping and Matching Subsequences Between Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simiao Lin, Wannes Meert, Pieter Robberechts, Hendrik Blockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparing time series is essential in various tasks such as clustering and
classification. While elastic distance measures that allow warping provide a
robust quantitative comparison, a qualitative comparison on top of them is
missing. Traditional visualizations focus on point-to-point alignment and do
not convey the broader structural relationships at the level of subsequences.
This limitation makes it difficult to understand how and where one time series
shifts, speeds up or slows down with respect to another. To address this, we
propose a novel technique that simplifies the warping path to highlight,
quantify and visualize key transformations (shift, compression, difference in
amplitude). By offering a clearer representation of how subsequences match
between time series, our method enhances interpretability in time series
comparison.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-supervised Graph Anomaly Detection via Robust Homophily Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoguo Ai, Hezhe Qiao, Hui Yan, Guansong Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled
normal nodes to identify abnormal nodes from a large set of unlabeled nodes in
a graph. Current methods in this line posit that 1) normal nodes share a
similar level of homophily and 2) the labeled normal nodes can well represent
the homophily patterns in the normal class. However, this assumption often does
not hold well since normal nodes in a graph can exhibit diverse homophily in
real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily
Learning, to adaptively learn such homophily patterns. RHO consists of two
novel modules, adaptive frequency response filters (AdaFreq) and graph
normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters
that capture different frequency components of the labeled normal nodes with
varying homophily in the channel-wise and cross-channel views of node
attributes. GNA is introduced to enforce consistency between the channel-wise
and cross-channel homophily representations to robustify the normality learned
by the filters in the two views. Experiments on eight real-world GAD datasets
show that RHO can effectively learn varying, often under-represented, homophily
in the small normal node set and substantially outperforms state-of-the-art
competing methods. Code is available at https://github.com/mala-lab/RHO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Reinforcement Learning Under Partial Observability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Jeen, Tom Bewley, Jonathan M. Cullen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that, under certain assumptions, zero-shot
reinforcement learning (RL) methods can generalise to any unseen task in an
environment after reward-free pre-training. Access to Markov states is one such
assumption, yet, in many real-world applications, the Markov state is only
partially observable. Here, we explore how the performance of standard
zero-shot RL methods degrades when subjected to partially observability, and
show that, as in single-task RL, memory-based architectures are an effective
remedy. We evaluate our memory-based zero-shot RL methods in domains where the
states, rewards and a change in dynamics are partially observed, and show
improved performance over memory-free baselines. Our code is open-sourced via:
https://enjeeneer.io/projects/bfms-with-memory/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reinforcement Learning Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward Models in Deep Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yu, Shenghua Wan, Yucen Wang, Chen-Xiao Gao, Le Gan, Zongzhang Zhang, De-Chuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), agents continually interact with the
environment and use the feedback to refine their behavior. To guide policy
optimization, reward models are introduced as proxies of the desired
objectives, such that when the agent maximizes the accumulated reward, it also
fulfills the task designer's intentions. Recently, significant attention from
both academic and industrial researchers has focused on developing reward
models that not only align closely with the true objectives but also facilitate
policy optimization. In this survey, we provide a comprehensive review of
reward modeling techniques within the deep RL literature. We begin by outlining
the background and preliminaries in reward modeling. Next, we present an
overview of recent reward modeling approaches, categorizing them based on the
source, the mechanism, and the learning paradigm. Building on this
understanding, we discuss various applications of these reward modeling
techniques and review methods for evaluating reward models. Finally, we
conclude by highlighting promising research directions in reward modeling.
Altogether, this survey includes both established and emerging methods, filling
the vacancy of a systematic review of reward models in current literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2025 Survey Track (To Appear)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying VXAI: A Systematic <span class="highlight-title">Review</span> and Framework for the Evaluation of
  Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dembinsky, Adriano Lucieri, Stanislav Frolov, Hiba Najjar, Ko Watanabe, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern AI systems frequently rely on opaque black-box models, most notably
Deep Neural Networks, whose performance stems from complex architectures with
millions of learned parameters. While powerful, their complexity poses a major
challenge to trustworthiness, particularly due to a lack of transparency.
Explainable AI (XAI) addresses this issue by providing human-understandable
explanations of model behavior. However, to ensure their usefulness and
trustworthiness, such explanations must be rigorously evaluated. Despite the
growing number of XAI methods, the field lacks standardized evaluation
protocols and consensus on appropriate metrics. To address this gap, we conduct
a systematic literature review following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a
unified framework for the eValuation of XAI (VXAI). We identify 362 relevant
publications and aggregate their contributions into 41 functionally similar
metric groups. In addition, we propose a three-dimensional categorization
scheme spanning explanation type, evaluation contextuality, and explanation
quality desiderata. Our framework provides the most comprehensive and
structured overview of VXAI to date. It supports systematic metric selection,
promotes comparability across methods, and offers a flexible foundation for
future extensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TMLR, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NERO: Explainable Out-of-Distribution Detection with Neuron-level
  Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anju Chhetri, Jari Korhonen, Prashnna Gyawali, Binod Bhattarai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring reliability is paramount in deep learning, particularly within the
domain of medical imaging, where diagnostic decisions often hinge on model
outputs. The capacity to separate out-of-distribution (OOD) samples has proven
to be a valuable indicator of a model's reliability in research. In medical
imaging, this is especially critical, as identifying OOD inputs can help flag
potential anomalies that might otherwise go undetected. While many OOD
detection methods rely on feature or logit space representations, recent works
suggest these approaches may not fully capture OOD diversity. To address this,
we propose a novel OOD scoring mechanism, called NERO, that leverages
neuron-level relevance at the feature layer. Specifically, we cluster
neuron-level relevance for each in-distribution (ID) class to form
representative centroids and introduce a relevance distance metric to quantify
a new sample's deviation from these centroids, enhancing OOD separability.
Additionally, we refine performance by incorporating scaled relevance in the
bias term and combining feature norms. Our framework also enables explainable
OOD detection. We validate its effectiveness across multiple deep learning
architectures on the gastrointestinal imaging benchmarks Kvasir and
GastroVision, achieving improvements over state-of-the-art OOD detection
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn to Vaccinate: Combining Structure Learning and Effective
  Vaccination for Epidemic and Outbreak Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepehr Elahi, Paula Mürmann, Patrick Thiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Susceptible-Infected-Susceptible (SIS) model is a widely used model for
the spread of information and infectious diseases, particularly non-immunizing
ones, on a graph. Given a highly contagious disease, a natural question is how
to best vaccinate individuals to minimize the disease's extinction time. While
previous works showed that the problem of optimal vaccination is closely linked
to the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that
the graph is known, which is often not the case in practice. In this work, we
consider the problem of minimizing the extinction time of an outbreak modeled
by an SIS model where the graph on which the disease spreads is unknown and
only the infection states of the vertices are observed. To this end, we split
the problem into two: learning the graph and determining effective vaccination
strategies. We propose a novel inclusion-exclusion-based learning algorithm
and, unlike previous approaches, establish its sample complexity for graph
recovery. We then detail an optimal algorithm for the SRM problem and prove
that its running time is polynomial in the number of vertices for graphs with
bounded treewidth. This is complemented by an efficient and effective
polynomial-time greedy heuristic for any graph. Finally, we present experiments
on synthetic and real-world data that numerically validate our learning and
vaccination algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Timescale Gradient Sliding for Distributed Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhui Zhang, Patrick Jaillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two first-order methods for convex, non-smooth, distributed
optimization problems, hereafter called Multi-Timescale Gradient Sliding
(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take
advantage of similarities between (local) objectives to reduce the
communication rounds, are flexible so that different subsets (of agents) can
communicate at different, user-picked rates, and are fully deterministic. These
three desirable features are achieved through a block-decomposable primal-dual
formulation, and a multi-timescale variant of the sliding method introduced in
Lan et al. (2020), Lan (2016), where different dual blocks are updated at
potentially different rates.
  To find an $\epsilon$-suboptimal solution, the complexities of our algorithms
achieve optimal dependency on $\epsilon$: MT-GS needs
$O(\overline{r}A/\epsilon)$ communication rounds and
$O(\overline{r}/\epsilon^2)$ subgradient steps for Lipchitz objectives, and
AMT-GS needs $O(\overline{r}A/\sqrt{\epsilon\mu})$ communication rounds and
$O(\overline{r}/(\epsilon\mu))$ subgradient steps if the objectives are also
$\mu$-strongly convex. Here, $\overline{r}$ measures the ``average rate of
updates'' for dual blocks, and $A$ measures similarities between (subgradients
of) local functions. In addition, the linear dependency of communication rounds
on $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive
answer to the open question whether such dependency is achievable for
non-smooth objectives (Arjevani and Shamir 2015).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Maximum Entropy Manifold Exploration via <span class="highlight-title">Diffusion Models</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is critical for solving real-world decision-making problems such
as scientific discovery, where the objective is to generate truly novel designs
rather than mimic existing data distributions. In this work, we address the
challenge of leveraging the representational power of generative models for
exploration without relying on explicit uncertainty quantification. We
introduce a novel framework that casts exploration as entropy maximization over
the approximate data manifold implicitly defined by a pre-trained diffusion
model. Then, we present a novel principle for exploration based on density
estimation, a problem well-known to be challenging in practice. To overcome
this issue and render this method truly scalable, we leverage a fundamental
connection between the entropy of the density induced by a diffusion model and
its score function. Building on this, we develop an algorithm based on mirror
descent that solves the exploration problem as sequential fine-tuning of a
pre-trained diffusion model. We prove its convergence to the optimal
exploratory diffusion model under realistic assumptions by leveraging recent
understanding of mirror flows. Finally, we empirically evaluate our approach on
both synthetic and high-dimensional text-to-image diffusion, demonstrating
promising results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Ground Metric Learning with Applications to scRNA data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damin Kühn, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport provides a robust framework for comparing probability
distributions. Its effectiveness is significantly influenced by the choice of
the underlying ground metric. Traditionally, the ground metric has either been
(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a
supervised way, by utilizing labeled data to learn a suitable ground metric for
enhanced task-specific performance. Yet, predefined metrics typically cannot
account for the inherent structure and varying importance of different features
in the data, and existing supervised approaches to ground metric learning often
do not generalize across multiple classes or are restricted to distributions
with shared supports. To address these limitations, we propose a novel approach
for learning metrics for arbitrary distributions over a shared metric space.
Our method provides a distance between individual points like a global metric,
but requires only class labels on a distribution-level for training. The
learned global ground metric enables more accurate optimal transport distances,
leading to improved performance in embedding, clustering and classification
tasks. We demonstrate the effectiveness and interpretability of our approach
using patient-level scRNA-seq data spanning multiple diseases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This method is provided as a Python package on PyPI, see
  https://github.com/DaminK/ggml-ot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling 3D Molecular Conformers with Diffusion Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Thorben Frank, Winfried Ripken, Gregor Lied, Klaus-Robert Müller, Oliver T. Unke, Stefan Chmiela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformers (DiTs) have demonstrated strong performance in
generative modeling, particularly in image synthesis, making them a compelling
choice for molecular conformer generation. However, applying DiTs to molecules
introduces novel challenges, such as integrating discrete molecular graph
information with continuous 3D geometry, handling Euclidean symmetries, and
designing conditioning mechanisms that generalize across molecules of varying
sizes and structures. We propose DiTMC, a framework that adapts DiTs to address
these challenges through a modular architecture that separates the processing
of 3D coordinates from conditioning on atomic connectivity. To this end, we
introduce two complementary graph-based conditioning strategies that integrate
seamlessly with the DiT architecture. These are combined with different
attention mechanisms, including both standard non-equivariant and
SO(3)-equivariant formulations, enabling flexible control over the trade-off
between between accuracy and computational efficiency. Experiments on standard
conformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC
achieves state-of-the-art precision and physical validity. Our results
highlight how architectural choices and symmetry priors affect sample quality
and efficiency, suggesting promising directions for large-scale generative
modeling of molecular structures. Code available at
https://github.com/ML4MolSim/dit_mc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performative Validity of Recourse Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gunnar König, Hidde Fokkema, Timo Freiesleben, Celestine Mendler-Dünner, Ulrike Von Luxburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applicants get rejected by an algorithmic decision system, recourse
explanations provide actionable suggestions for how to change their input
features to get a positive evaluation. A crucial yet overlooked phenomenon is
that recourse explanations are performative: When many applicants act according
to their recommendations, their collective behavior may change statistical
regularities in the data and, once the model is refitted, also the decision
boundary. Consequently, the recourse algorithm may render its own
recommendations invalid, such that applicants who make the effort of
implementing their recommendations may be rejected again when they reapply. In
this work, we formally characterize the conditions under which recourse
explanations remain valid under performativity. A key finding is that recourse
actions may become invalid if they are influenced by or if they intervene on
non-causal variables. Based on our analysis, we caution against the use of
standard counterfactual explanations and causal recourse methods, and instead
advocate for recourse methods that recommend actions exclusively on causal
variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 3 figures, 1 table, Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing One-run Privacy Auditing with Quantile Regression-Based
  Membership Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terrance Liu, Matteo Boglioni, Yiwei Fu, Shengyuan Hu, Pratiksha Thaker, Zhiwei Steven Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) auditing aims to provide empirical lower bounds on
the privacy guarantees of DP mechanisms like DP-SGD. While some existing
techniques require many training runs that are prohibitively costly, recent
work introduces one-run auditing approaches that effectively audit DP-SGD in
white-box settings while still being computationally efficient. However, in the
more practical black-box setting where gradients cannot be manipulated during
training and only the last model iterate is observed, prior work shows that
there is still a large gap between the empirical lower bounds and theoretical
upper bounds. Consequently, in this work, we study how incorporating approaches
for stronger membership inference attacks (MIA) can improve one-run auditing in
the black-box setting. Evaluating on image classification models trained on
CIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes
quantile regression for MIA, achieves tighter bounds while crucially
maintaining the computational efficiency of one-run methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. S. Stankevich, I. B. Petrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in application of deep learning models to acoustic Full
Waveform Inversion (FWI) are marked by the use of diffusion models as prior
distributions for Bayesian-like inference procedures. The advantage of these
methods is the ability to generate high-resolution samples, which are otherwise
unattainable with classical inversion methods or other deep learning-based
solutions. However, the iterative and stochastic nature of sampling from
diffusion models along with heuristic nature of output control remain limiting
factors for their applicability. For instance, an optimal way to include the
approximate velocity model into diffusion-based inversion scheme remains
unclear, even though it is considered an essential part of FWI pipeline. We
address the issue by employing a Schr\"odinger Bridge that interpolates between
the distributions of ground truth and smoothed velocity models. To facilitate
the learning of nonlinear drifts that transfer samples between distributions we
extend the concept of Image-to-Image Schr\"odinger Bridge
($\text{I}^2\text{SB}$) to conditional sampling, resulting in a conditional
Image-to-Image Schr\"odinger Bridge (c$\text{I}^2\text{SB}$) framework. To
validate our method, we assess its effectiveness in reconstructing the
reference velocity model from its smoothed approximation, coupled with the
observed seismic signal of fixed shape. Our experiments demonstrate that the
proposed solution outperforms our reimplementation of conditional diffusion
model suggested in earlier works, while requiring only a few neural function
evaluations (NFEs) to achieve sample fidelity superior to that attained with
supervised learning-based approach. The supplementary code implementing the
algorithms described in this paper can be found in the repository
https://github.com/stankevich-mipt/seismic_inversion_via_I2SB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to "Computational Mathematics And Mathematical Physics",
  ISSN 1555-6662, issue 8, August 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Knowledge Distillation</span> Framework for Accelerating High-Accuracy Neural
  Network-Based Molecular Dynamics Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Matsumura, Yuta Yoshimoto, Yuto Iwasaki, Meguru Yamazaki, Yasufumi Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network potentials (NNPs) offer a powerful alternative to traditional
force fields for molecular dynamics (MD) simulations. Accurate and stable MD
simulations, crucial for evaluating material properties, require training data
encompassing both low-energy stable structures and high-energy structures.
Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as
a teacher model to generate training data for a student model. However, in
material-specific models, this fine-tuning process increases energy barriers,
making it difficult to create training data containing high-energy structures.
To address this, we propose a novel KD framework that leverages a
non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy
landscape facilitates the exploration of a wider range of structures, including
the high-energy structures crucial for stable MD simulations. Our framework
employs a two-stage training process: first, the student NNP is trained with a
dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a
smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate
the effectiveness of our framework by applying it to both organic (polyethylene
glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving
comparable or superior accuracy in reproducing physical properties compared to
existing methods. Importantly, our method reduces the number of expensive DFT
calculations by 10x compared to existing NNP generation methods, without
sacrificing accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Laboratory Model: prognosis of abnormal clinical outcomes
  based on routine tests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Karpov, Ilya Petrenkov, Ruslan Raiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical laboratory results are ubiquitous in any diagnosis making.
Predicting abnormal values of not prescribed tests based on the results of
performed tests looks intriguing, as it would be possible to make early
diagnosis available to everyone. The special place is taken by the Common Blood
Count (CBC) test, as it is the most widely used clinical procedure. Combining
routine biochemical panels with CBC presents a set of test-value pairs that
varies from patient to patient, or, in common settings, a table with missing
values. Here we formulate a tabular modeling problem as a set translation
problem where the source set comprises pairs of GPT-like label column embedding
and its corresponding value while the target set consists of the same type
embeddings only. The proposed approach can effectively deal with missing values
without implicitly estimating them and bridges the world of LLM with the
tabular domain. Applying this method to clinical laboratory data, we achieve an
improvement up to 8% AUC for joint predictions of high uric acid, glucose,
cholesterol, and low ferritin levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and How Unlabeled Data Provably Improve In-Context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that in-context learning (ICL) can be effective even
when demonstrations have missing or incorrect labels. To shed light on this
capability, we examine a canonical setting where the demonstrations are drawn
according to a binary Gaussian mixture model (GMM) and a certain fraction of
the demonstrations have missing labels. We provide a comprehensive theoretical
study to show that: (1) The loss landscape of one-layer linear attention models
recover the optimal fully-supervised estimator but completely fail to exploit
unlabeled data; (2) In contrast, multilayer or looped transformers can
effectively leverage unlabeled data by implicitly constructing estimators of
the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting
features and partially-observed labels (with missing entries set to zero). We
characterize the class of polynomials that can be expressed as a function of
depth and draw connections to Expectation Maximization, an iterative
pseudo-labeling algorithm commonly used in semi-supervised learning.
Importantly, the leading polynomial power is exponential in depth, so mild
amount of depth/looping suffices. As an application of theory, we propose
looping off-the-shelf tabular foundation models to enhance their
semi-supervision capabilities. Extensive evaluations on real-world datasets
show that our method significantly improves the semisupervised tabular learning
performance over the standard single pass inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximal Operators of Sorted Nonconvex Penalties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anne Gagneux, Mathurin Massias, Emmanuel Soubies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the problem of sparse signal recovery with automatic
grouping of variables. To this end, we investigate sorted nonsmooth penalties
as a regularization approach for generalized linear models. We focus on a
family of sorted nonconvex penalties which generalizes the Sorted L1 Norm
(SLOPE). These penalties are designed to promote clustering of variables due to
their sorted nature, while the nonconvexity reduces the shrinkage of
coefficients. Our goal is to provide efficient ways to compute their proximal
operator, enabling the use of popular proximal algorithms to solve composite
optimization problems with this choice of sorted penalties. We distinguish
between two classes of problems: the weakly convex case where computing the
proximal operator remains a convex problem, and the nonconvex case where
computing the proximal operator becomes a challenging nonconvex combinatorial
problem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how
the Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal
operator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that
a slight modification of this algorithm turns out to be remarkably efficient to
tackle the computation of the proximal operator. We also present new
theoretical insights on the minimizers of the nonconvex proximal problem. We
demonstrate the practical interest of using such penalties on several
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target
  Inhibitor Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Júlia Vilalta-Mor, Alexis Molina, Laura Ortega Varga, Isaac Filella-Merce, Victor Guallar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneously optimizing molecules against multiple therapeutic targets
remains a profound challenge in drug discovery, particularly due to sparse
rewards and conflicting design constraints. We propose a structured active
learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational
autoencoder (VAE) into iterative loops designed to balance chemical diversity,
molecular quality, and multi-target affinity. Our method alternates between
expanding chemically feasible regions of latent space and progressively
constraining molecules based on increasingly stringent multi-target docking
thresholds. In a proof-of-concept study targeting three related coronavirus
main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently
generated a structurally diverse set of pan-inhibitor candidates. We
demonstrate that careful timing and strategic placement of chemical filters
within this active learning pipeline markedly enhance exploration of beneficial
chemical space, transforming the sparse-reward, multi-objective drug design
problem into an accessible computational task. Our framework thus provides a
generalizable roadmap for efficiently navigating complex polypharmacological
landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language
  Models Using Forward-Only Passes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglong Luo, Zhuo Zhang, Yehong Zhang, Shiyu Liu, Ye Dong, Xun Zhou, Hui Wang, Yue Yu, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have transformed numerous fields, yet their
adaptation to specialized tasks in privacy-sensitive domains, such as
healthcare and finance, is constrained by the scarcity of accessible training
data due to stringent privacy requirements. Secure multi-party computation
(MPC)-based privacy-preserving machine learning offers a powerful approach to
protect both model parameters and user data, but its application to LLMs has
been largely limited to inference, as fine-tuning introduces significant
computational challenges, particularly in privacy-preserving backward
propagation and optimizer operations. This paper identifies two primary
obstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the
substantial computational overhead of backward and optimizer processes, and (2)
the inefficiency of softmax-based attention mechanisms in MPC settings. To
address these challenges, we propose SecFwT, the first MPC-based framework
designed for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a
forward-only tuning paradigm to eliminate backward and optimizer computations
and employs MPC-friendly Random Feature Attention to approximate softmax
attention, significantly reducing costly non-linear operations and
computational complexity. Experimental results demonstrate that SecFwT delivers
substantial improvements in efficiency and privacy preservation, enabling
scalable and secure fine-tuning of LLMs for privacy-critical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Generative Modeling for Enhanced Credit Risk Management in
  Supply Chain Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingkai Zhang, L. Jeff Hong, Houmin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of cross-border e-commerce (CBEC) has created significant
opportunities for small and medium-sized enterprises (SMEs), yet financing
remains a critical challenge due to SMEs' limited credit histories. Third-party
logistics (3PL)-led supply chain finance (SCF) has emerged as a promising
solution, leveraging in-transit inventory as collateral. We propose an advanced
credit risk management framework tailored for 3PL-led SCF, addressing the dual
challenges of credit risk assessment and loan size determination. Specifically,
we leverage conditional generative modeling of sales distributions through
Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for
risk estimation. We propose a unified framework that enables flexible
estimation of multiple risk measures while introducing a functional risk
measure formulation that systematically captures the relationship between these
risk measures and varying loan levels, supported by theoretical guarantees. To
capture complex covariate interactions in e-commerce sales data, we integrate
QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on
synthetic and real-world data validate the efficacy of our model for credit
risk assessment and loan size determination. This study represents a pioneering
application of generative AI in CBEC SCF risk management, offering a solid
foundation for enhanced credit practices and improved SME access to capital.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConLID: Supervised Contrastive Learning for Low-Resource Language
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Foroutan, Jakhongir Saydaliev, Ye Eun Kim, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language identification (LID) is a critical step in curating multilingual LLM
pretraining corpora from web crawls. While many studies on LID model training
focus on collecting diverse training data to improve performance, low-resource
languages -- often limited to single-domain data, such as the Bible -- continue
to perform poorly. To resolve these class imbalance and bias issues, we propose
a novel supervised contrastive learning (SCL) approach to learn
domain-invariant representations for low-resource languages. Through an
extensive analysis, we show that our approach improves LID performance on
out-of-domain data for low-resource languages by 3.2%, demonstrating its
effectiveness in enhancing LID models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EMNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for
  Optimizing Large-Scale EV Charging Infrastructure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Li, Shunyu Zhao, Vincent Gauthier, Hassine Moungla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerating uptake of battery-electric vehicles demands infrastructure
planning tools that are both data-rich and geographically scalable. Whereas
most prior studies optimise charging locations for single cities, state-wide
and national networks must reconcile the conflicting requirements of dense
metropolitan cores, car-dependent exurbs, and power-constrained rural
corridors.
  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,
Adaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework
that unifies these contexts in a single pipeline. The method rasterises
heterogeneous data (roads, population, night lights, POIs, and feeder lines)
onto a hierarchical H3 grid, infers intersection importance with a
zone-normalised graph neural network centrality model, and overlays a Voronoi
tessellation that guarantees at least one five-port DC fast charger within
every 30 km radius. Hourly arrival profiles, learned from loop-detector and
floating-car traces, feed a finite M/M/c queue to size ports under
feeder-capacity and outage-risk constraints. A greedy maximal-coverage
heuristic with income-weighted penalties then selects the minimum number of
sites that satisfy coverage and equity targets.
  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile
coverage by 12 percentage points, (ii) halves the mean distance that low-income
residents travel to the nearest charger, and (iii) meets sub-transmission
headroom everywhere -- all while remaining computationally tractable for
national-scale roll-outs. These results demonstrate that a tightly integrated,
GNN-driven, multi-resolution approach can bridge the gap between academic
optimisation and deployable infrastructure policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Post-hoc <span class="highlight-title">Dataset</span> Inference with Synthetic Data <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihe Zhao, Pratyush Maini, Franziska Boenisch, Adam Dziedzic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of Large Language Models (LLMs) can be mainly
attributed to their massive training datasets, which are often scraped from the
internet without respecting data owners' intellectual property rights. Dataset
Inference (DI) offers a potential remedy by identifying whether a suspect
dataset was used in training, thereby enabling data owners to verify
unauthorized use. However, existing DI methods require a private set-known to
be absent from training-that closely matches the compromised dataset's
distribution. Such in-distribution, held-out data is rarely available in
practice, severely limiting the applicability of DI. In this work, we address
this challenge by synthetically generating the required held-out set. Our
approach tackles two key obstacles: (1) creating high-quality, diverse
synthetic data that accurately reflects the original distribution, which we
achieve via a data generator trained on a carefully designed suffix-based
completion task, and (2) bridging likelihood gaps between real and synthetic
data, which is realized through post-hoc calibration. Extensive experiments on
diverse text datasets show that using our generated data as a held-out set
enables DI to detect the original training sets with high confidence, while
maintaining a low false positive rate. This result empowers copyright owners to
make legitimate claims on data usage and demonstrates our method's reliability
for real-world litigations. Our code is available at
https://github.com/sprintml/PostHocDatasetInference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centroid Approximation for Byzantine-Tolerant Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Cambus, Darya Melnyk, Tijana Milentijević, Stefan Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimizing Structural Vibrations via Guided Flow Matching Design
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan van Delden, Julius Schultz, Sebastian Rothe, Christian Libner, Sabine C. Langer, Timo Lüddecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural vibrations are a source of unwanted noise in engineering systems
like cars, trains or airplanes. Minimizing these vibrations is crucial for
improving passenger comfort. This work presents a novel design optimization
approach based on guided flow matching for reducing vibrations by placing
beadings (indentations) in plate-like structures. Our method integrates a
generative flow matching model and a surrogate model trained to predict
structural vibrations. During the generation process, the flow matching model
pushes towards manufacturability while the surrogate model pushes to
low-vibration solutions. The flow matching model and its training data
implicitly define the design space, enabling a broader exploration of potential
solutions as no optimization of manually-defined design parameters is required.
We apply our method to a range of differentiable optimization objectives,
including direct optimization of specific eigenfrequencies through careful
construction of the objective function. Results demonstrate that our method
generates diverse and manufacturable plate designs with reduced structural
vibrations compared to designs from random search, a criterion-based design
heuristic and genetic optimization. The code and data are available from
https://github.com/ecker-lab/Optimizing_Vibrating_Plates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Singular Value Decomposition on Kronecker Adaptation for Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yee Hin Chong, Peng Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained Transformer models achieve state-of-the-art results across
diverse language and reasoning tasks, but full fine-tuning incurs substantial
storage, memory, and computational overhead. Parameter-efficient fine-tuning
(PEFT) methods mitigate these costs by learning only a small subset of
task-specific parameters, yet existing approaches either introduce
inference-time latency (adapter modules), suffer from suboptimal convergence
(randomly initialized low-rank updates), or rely on fixed rank choices that may
not match task complexity (Kronecker-based decompositions).
  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that
combines Kronecker-product tensor factorization with SVD-driven initialization
and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)
procedure extracts principal components of the full weight update into compact
Kronecker factors, while an adaptive rank selection algorithm uses
energy-threshold and elbow-point criteria to prune negligible components.
  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal
mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires
only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or
exceeding baseline performance. Moreover, SoKA exhibits faster convergence and
more stable gradients, highlighting its robustness and efficiency for
large-scale model adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Deep Lagrangian Networks for Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Schulze, Jan Peters, Oleg Arenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling a robot based on physics-informed dynamic models, such as deep
Lagrangian networks (DeLaN), can improve the generalizability and
interpretability of the resulting behavior. However, in complex environments,
the number of objects to potentially interact with is vast, and their physical
properties are often uncertain. This complexity makes it infeasible to employ a
single global model. Therefore, we need to resort to online system
identification of context-aware models that capture only the currently relevant
aspects of the environment. While physical principles such as the conservation
of energy may not hold across varying contexts, ensuring physical plausibility
for any individual context-aware model can still be highly desirable,
particularly when using it for receding horizon control methods such as Model
Predictive Control (MPC). Hence, in this work, we extend DeLaN to make it
context-aware, combine it with a recurrent network for online system
identification, and integrate it with a MPC for adaptive, physics-informed
control. We also combine DeLaN with a residual dynamics model to leverage the
fact that a nominal model of the robot is typically available. We evaluate our
method on a 7-DOF robot arm for trajectory tracking under varying loads. Our
method reduces the end-effector tracking error by 39%, compared to a 21%
improvement achieved by a baseline that uses an extended Kalman filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretability and Generalization Bounds for Learning Spatial Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Francisco Queiruga, Theo Gutman-Solo, Shuai Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While there are many applications of ML to scientific problems that look
promising, visuals can be deceiving. For scientific applications, actual
quantitative accuracy is crucial. This work applies the rigor of numerical
analysis for differential equations to machine learning by specifically
quantifying the accuracy of applying different ML techniques to the elementary
1D Poisson differential equation. Beyond the quantity and discretization of
data, we identify that the function space of the data is critical to the
generalization of the model. We prove generalization bounds and convergence
rates under finite data discretizations and restricted training data subspaces
by analyzing the training dynamics and deriving optimal parameters for both a
white-box differential equation discovery method and a black-box linear model.
The analytically derived generalization bounds are replicated empirically.
Similar lack of generalization is empirically demonstrated for deep linear
models, shallow neural networks, and physics-specific DeepONets and Neural
Operators. We theoretically and empirically demonstrate that generalization to
the true physical equation is not guaranteed in each explored case.
Surprisingly, we find that different classes of models can exhibit opposing
generalization behaviors. Based on our theoretical analysis, we also
demonstrate a new mechanistic interpretability lens on scientific models
whereby Green's function representations can be extracted from the weights of
black-box models. Our results inform a new cross-validation technique for
measuring generalization in physical systems. We propose applying it to the
Poisson equation as an evaluation benchmark of future methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal
  Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyi Wang, Jingyang Ke, Bo Dai, Anqi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animals flexibly recombine a finite set of core motor primitives to meet
diverse task demands, but existing behavior-segmentation methods oversimplify
this process by imposing discrete syllables under restrictive generative
assumptions. To reflect the animal behavior generation procedure, we introduce
skill-based imitation learning (SKIL) for behavior understanding, a
reinforcement learning-based imitation framework that (1) infers interpretable
skill sets, i.e., latent basis functions of behavior, by leveraging
representation learning on transition probabilities, and (2) parameterizes
policies as dynamic mixtures of these skills. We validate our approach on a
simple grid world, a discrete labyrinth, and unconstrained videos of freely
moving animals. Across tasks, it identifies reusable skill components, learns
continuously evolving compositional policies, and generates realistic
trajectories beyond the capabilities of traditional discrete models. By
exploiting generative behavior modeling with compositional representations, our
method offers a concise, principled account of how complex animal behaviors
emerge from dynamic combinations of fundamental motor primitives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages and 4 figures for the main text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Multi-Parametric Body MRI Series Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boah Kim, Tejas Sudharshan Mathai, Kimberly Helm, Peter A. Pinto, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-parametric magnetic resonance imaging (mpMRI) exams have various series
types acquired with different imaging protocols. The DICOM headers of these
series often have incorrect information due to the sheer diversity of protocols
and occasional technologist errors. To address this, we present a deep
learning-based classification model to classify 8 different body mpMRI series
types so that radiologists read the exams efficiently. Using mpMRI data from
various institutions, multiple deep learning-based classifiers of ResNet,
EfficientNet, and DenseNet are trained to classify 8 different MRI series, and
their performance is compared. Then, the best-performing classifier is
identified, and its classification capability under the setting of different
training data quantities is studied. Also, the model is evaluated on the
out-of-training-distribution datasets. Moreover, the model is trained using
mpMRI exams obtained from different scanners in two training strategies, and
its performance is tested. Experimental results show that the DenseNet-121
model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the
other classification models with p-value$<$0.05. The model shows greater than
0.95 accuracy when trained with over 729 studies of the training data, whose
performance improves as the training data quantities grew larger. On the
external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and
0.810 accuracy for each. These results indicate that in both the internal and
external datasets, the DenseNet-121 model attains high accuracy for the task of
classifying 8 body MRI series types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Liu, Chengcheng Zhao, Li Chai, Peng Cheng, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jointly addressing Byzantine attacks and privacy leakage in distributed
machine learning (DML) has become an important issue. A common strategy
involves integrating Byzantine-resilient aggregation rules with differential
privacy mechanisms. However, the incorporation of these techniques often
results in a significant degradation in model accuracy. To address this issue,
we propose a decentralized DML framework, named ImprovDML, that achieves high
model accuracy while simultaneously ensuring privacy preservation and
resilience to Byzantine attacks. The framework leverages a kind of resilient
vector consensus algorithms that can compute a point within the normal
(non-Byzantine) agents' convex hull for resilient aggregation at each
iteration. Then, multivariate Gaussian noises are introduced to the gradients
for privacy preservation. We provide convergence guarantees and derive
asymptotic learning error bounds under non-convex settings, which are tighter
than those reported in existing works. For the privacy analysis, we adopt the
notion of concentrated geo-privacy, which quantifies privacy preservation based
on the Euclidean distance between inputs. We demonstrate that it enables an
improved trade-off between privacy preservation and model accuracy compared to
differential privacy. Finally, numerical simulations validate our theoretical
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning for Gradient-Free Receiver Adaptation: Principles,
  Applications, and Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Zecchin, Tomer Raviv, Dileep Kalathil, Krishna Narayanan, Nir Shlezinger, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, deep learning has facilitated the creation of wireless
receivers capable of functioning effectively in conditions that challenge
traditional model-based designs. Leveraging programmable hardware
architectures, deep learning-based receivers offer the potential to dynamically
adapt to varying channel environments. However, current adaptation strategies,
including joint training, hypernetwork-based methods, and meta-learning, either
demonstrate limited flexibility or necessitate explicit optimization through
gradient descent. This paper presents gradient-free adaptation techniques
rooted in the emerging paradigm of in-context learning (ICL). We review
architectural frameworks for ICL based on Transformer models and structured
state-space models (SSMs), alongside theoretical insights into how sequence
models effectively learn adaptation from contextual information. Further, we
explore the application of ICL to cell-free massive MIMO networks, providing
both theoretical analyses and empirical evidence. Our findings indicate that
ICL represents a principled and efficient approach to real-time receiver
adaptation using pilot signals and auxiliary contextual information-without
requiring online retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Physics-Informed Neural Network Approach for Estimating
  Heterogeneous Elastic Properties from Noisy Displacement Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatthapong Srikitrungruang, Matthew Lemon, Sina Aghaee Dabaghan Fard, Jaesung Lee, Yuxiao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating spatially heterogeneous elasticity parameters,
particularly Young's modulus and Poisson's ratio, from noisy displacement
measurements remains significantly challenging in inverse elasticity problems.
Existing inverse estimation techniques are often limited by instability,
pronounced sensitivity to measurement noise, and difficulty in recovering
absolute-scale Young's modulus. This work presents a novel Inverse Elasticity
Physics-Informed Neural Network (IE-PINN) specifically designed to robustly
reconstruct heterogeneous distributions of elasticity parameters from noisy
displacement data based on linear elasticity physics. IE-PINN integrates three
distinct neural network architectures dedicated to separately modeling
displacement fields, strain fields, and elasticity distributions, thereby
significantly enhancing stability and accuracy against measurement noise.
Additionally, a two-phase estimation strategy is introduced: the first phase
recovers relative spatial distributions of Young's modulus and Poisson's ratio,
and the second phase calibrates the absolute scale of Young's modulus using
imposed loading boundary conditions. Additional methodological innovations,
including positional encoding, sine activation functions, and a sequential
pretraining protocol, further enhance the model's performance and robustness.
Extensive numerical experiments demonstrate that IE-PINN effectively overcomes
critical limitations encountered by existing methods, delivering accurate
absolute-scale elasticity estimations even under severe noise conditions. This
advancement holds substantial potential for clinical imaging diagnostics and
mechanical characterization, where measurements typically encounter substantial
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Out-of-Distribution Detection and Beyond in Vision Language
  Model Era: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Yueqian Lin, Qing Yu, Go Irie, Shafiq Joty, Yixuan Li, Hai Li, Ziwei Liu, Toshihiko Yamasaki, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) samples is crucial for ensuring the
safety of machine learning systems and has shaped the field of OOD detection.
Meanwhile, several other problems are closely related to OOD detection,
including anomaly detection (AD), novelty detection (ND), open set recognition
(OSR), and outlier detection (OD). To unify these problems, a generalized OOD
detection framework was proposed, taxonomically categorizing these five
problems. However, Vision Language Models (VLMs) such as CLIP have
significantly changed the paradigm and blurred the boundaries between these
fields, again confusing researchers. In this survey, we first present a
generalized OOD detection v2, encapsulating the evolution of these fields in
the VLM era. Our framework reveals that, with some field inactivity and
integration, the demanding challenges have become OOD detection and AD. Then,
we highlight the significant shift in the definition, problem settings, and
benchmarks; we thus feature a comprehensive review of the methodology for OOD
detection and related tasks to clarify their relationship to OOD detection.
Finally, we explore the advancements in the emerging Large Vision Language
Model (LVLM) era, such as GPT-4V. We conclude with open challenges and future
directions. The resource is available at
https://github.com/AtsuMiyai/Awesome-OOD-VLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR2025. Survey paper. We welcome questions, issues, and
  paper requests via https://github.com/AtsuMiyai/Awesome-OOD-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coherent Local Explanations for Mathematical Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daan Otto, Jannis Kurtz, S. Ilker Birbil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge of explainable artificial intelligence methods seeks to enhance
transparency and explainability in machine learning models. At the same time,
there is a growing demand for explaining decisions taken through complex
algorithms used in mathematical optimization. However, current explanation
methods do not take into account the structure of the underlying optimization
problem, leading to unreliable outcomes. In response to this need, we introduce
Coherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO
provides explanations for multiple components of optimization models, the
objective value and decision variables, which are coherent with the underlying
model structure. Our sampling-based procedure can provide explanations for the
behavior of exact and heuristic solution algorithms. The effectiveness of CLEMO
is illustrated by experiments for the shortest path problem, the knapsack
problem, and the vehicle routing problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhen Zhang, Tao Feng, Jiaxuan You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To facilitate learning, we employ a lightweight
rule-based reward comprising format rewards, final outcome rewards, and a novel
cost reward for optimizing the balance between performance and cost, opening a
pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also
conditions only on simple model descriptors such as pricing, latency, and
example performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/ulab-uiuc/Router-R1. Models
  and Datasets are available at
  https://huggingface.co/collections/ulab-ai/router-r1-6851bbe099c7a56914b5db03</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Perturb-ability Score to Mitigate Evasion Adversarial Attacks on
  Flow-Based ML-NIDS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07448v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07448v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed elShehaby, Ashraf Matrawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As network security threats evolve, safeguarding flow-based Machine Learning
(ML)-based Network Intrusion Detection Systems (NIDS) from evasion adversarial
attacks is crucial. This paper introduces the notion of feature perturb-ability
and presents a novel Perturb-ability Score (PS), which quantifies how
susceptible NIDS features are to manipulation in the problem-space by an
attacker. PS thereby identifies features structurally resistant to evasion
attacks in flow-based ML-NIDS due to the semantics of network traffic fields,
as these features are constrained by domain-specific limitations and
correlations. Consequently, attempts to manipulate such features would likely
either compromise the attack's malicious functionality, render the traffic
invalid for processing, or potentially both outcomes simultaneously.
  We introduce and demonstrate the effectiveness of our PS-enabled defenses,
PS-guided feature selection and PS-guided feature masking, in enhancing
flow-based NIDS resilience. Experimental results across various ML-based NIDS
models and public datasets show that discarding or masking highly manipulatable
features (high-PS features) can maintain solid detection performance while
significantly reducing vulnerability to evasion adversarial attacks. Our
findings confirm that PS effectively identifies flow-based NIDS features
susceptible to problem-space perturbations. This novel approach leverages
problem-space NIDS domain constraints as lightweight universal defense
mechanisms against evasion adversarial attacks targeting flow-based ML-NIDS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">dataset</span> of high-resolution plantar pressures for gait analysis across
  varying footwear and walking speeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robyn Larracy, Angkoon Phinyomark, Ala Salehi, Eve MacDonald, Saeed Kazemi, Shikder Shafiul Bashar, Aaron Tabor, Erik Scheme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait refers to the patterns of limb movement generated during walking, which
are unique to each individual due to both physical and behavioral traits.
Walking patterns have been widely studied in biometrics, biomechanics, sports,
and rehabilitation. While traditional methods rely on video and motion capture,
advances in plantar pressure sensing technology now offer deeper insights into
gait. However, underfoot pressures during walking remain underexplored due to
the lack of large, publicly accessible datasets. To address this, we introduce
the UNB StepUP-P150 dataset: a footStep database for gait analysis and
recognition using Underfoot Pressure, including data from 150 individuals. This
dataset comprises high-resolution plantar pressure data (4 sensors per
cm-squared) collected using a 1.2m by 3.6m pressure-sensing walkway. It
contains over 200,000 footsteps from participants walking with various speeds
(preferred, slow-to-stop, fast, and slow) and footwear conditions (barefoot,
standard shoes, and two personal shoes), supporting advancements in biometric
gait recognition and presenting new research opportunities in biomechanics and
deep learning. UNB StepUP-P150 establishes a new benchmark for plantar
pressure-based gait analysis and recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on
  Synthetic Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01481v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01481v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic video generation has gained significant attention for its realism
and broad applications, but remains prone to violations of common sense and
physical laws. This highlights the need for reliable abnormality detectors that
understand such principles and are robust to hallucinations. To address this,
we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from
synthetic videos generated by models like Veo2, Sora, and Kling, paired with
expert-crafted counterintuitive QA to evaluate the critical thinking abilities
of Multi-modal Large Language Models (MLLMs) on abnormalities that are
perceptually obvious to humans but often hallucinated due to language priors.
VideoHallu evaluates MLLMs' abnormality detection abilities with examples
across alignment, consistency, commonsense, and physics. We benchmark SOTA
MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and
VideoChat-R1. We observe that these models perform well on many real-world
benchmarks like MVBench and MovieChat, but still struggle with basic
physics-based and commonsense reasoning in synthetic videos. We further show
that post-training with Group Relative Policy Optimization (GRPO), using
curriculum learning on datasets combining video QA with counterintuitive
commonsense and physics reasoning over real and synthetic videos, improves
MLLMs' abnormality detection and critical thinking, demonstrating the value of
targeted training for improving their understanding of commonsense and physical
laws. Our code is available at https://github.com/zli12321/VideoHallu.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alternating Regret for Online Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumita Hait, Ping Li, Haipeng Luo, Mengxiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by alternating learning dynamics in two-player games, a recent work
by Cevher et al.(2024) shows that $o(\sqrt{T})$ alternating regret is possible
for any $T$-round adversarial Online Linear Optimization (OLO) problem, and
left as an open question whether the same is true for general Online Convex
Optimization (OCO). We answer this question in the affirmative by showing that
the continuous Hedge algorithm achieves
$\tilde{\mathcal{O}}(d^{\frac{2}{3}}T^{\frac{1}{3}})$ alternating regret for
any adversarial $d$-dimensional OCO problems. We show that this implies an
alternating learning dynamic that finds a Nash equilibrium for any
convex-concave zero-sum games or a coarse correlated equilibrium for any convex
two-player general-sum games at a rate of
$\tilde{\mathcal{O}}(d^{\frac{2}{3}}/T^{\frac{2}{3}})$. To further improve the
time complexity and/or the dimension dependence, we propose another simple
algorithm, Follow-the-Regularized-Leader with a regularizer whose convex
conjugate is 3rd-order smooth, for OCO with smooth and self-concordant loss
functions (such as linear or quadratic losses). We instantiate our algorithm
with different regularizers and show that, for example, when the decision set
is the $\ell_2$ ball, our algorithm achieves
$\tilde{\mathcal{O}}(T^{\frac{2}{5}})$ alternating regret with no dimension
dependence (and a better $\tilde{\mathcal{O}}(T^{\frac{1}{3}})$ bound for
quadratic losses). We complement our results by showing some algorithm-specific
alternating regret lower bounds, including a somewhat surprising
$\Omega(\sqrt{T})$ lower bound for a Regret Matching variant that is widely
used in alternating learning dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Gravity-informed Spatiotemporal Transformer for Human Activity
  Intensity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Zhenghong Wang, Fan Zhang, Chengling Tang, Chaogui Kang, Di Zhu, Zhongfu Ma, Sijie Ruan, Weiyu Zhang, Yu Zheng, Philip S. Yu, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activity intensity prediction is a crucial to many location-based
services. Although tremendous progress has been made to model dynamic
spatiotemporal patterns of human activity, most existing methods, including
spatiotemporal graph neural networks (ST-GNNs), overlook physical constraints
of spatial interactions and the over-smoothing phenomenon in spatial
correlation modeling. To address these limitations, this work proposes a
physics-informed deep learning framework, namely Gravity-informed
Spatiotemporal Transformer (Gravityformer) by refining transformer attention to
integrate the universal law of gravitation and explicitly incorporating
constraints from spatial interactions. Specifically, it (1) estimates two
spatially explicit mass parameters based on inflow and outflow, (2) models the
likelihood of cross-unit interaction using closed-form solutions of spatial
interactions to constrain spatial modeling randomness, and (3) utilizes the
learned spatial interaction to guide and mitigate the over-smoothing phenomenon
in transformer attention matrices. The underlying law of human activity can be
explicitly modeled by the proposed adaptive gravity model. Moreover, a parallel
spatiotemporal graph convolution transformer structure is proposed for
achieving a balance between coupled spatial and temporal learning. Systematic
experiments on six real-world large-scale activity datasets demonstrate the
quantitative and qualitative superiority of our approach over state-of-the-art
benchmarks. Additionally, the learned gravity attention matrix can be
disentangled and interpreted based on geographical laws. This work provides a
novel insight into integrating physical laws with deep learning for
spatiotemporal predictive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Near-Optimal Clustering in Mixture of Markov Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Lee, Yassir Jedra, Alexandre Proutière, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of clustering $T$ trajectories of length $H$, each
generated by one of $K$ unknown ergodic Markov chains over a finite state space
of size $S$. The goal is to accurately group trajectories according to their
underlying generative model. We begin by deriving an instance-dependent,
high-probability lower bound on the clustering error rate, governed by the
weighted KL divergence between the transition kernels of the chains. We then
present a novel two-stage clustering algorithm. In Stage~I, we apply spectral
clustering using a new injective Euclidean embedding for ergodic Markov chains
-- a contribution of independent interest that enables sharp concentration
results. Stage~II refines the initial clusters via a single step of
likelihood-based reassignment. Our method achieves a near-optimal clustering
error with high probability, under the conditions $H =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} (S^2 \vee \pi_{\min}^{-1}))$ and $TH =
\tilde{\Omega}(\gamma_{\mathrm{ps}}^{-1} S^2 )$, where $\pi_{\min}$ is the
minimum stationary probability of a state across the $K$ chains and
$\gamma_{\mathrm{ps}}$ is the minimum pseudo-spectral gap. These requirements
provide significant improvements, if not at least comparable, to the
state-of-the-art guarantee (Kausik et al., 2023), and moreover, our algorithm
offers a key practical advantage: unlike existing approach, it requires no
prior knowledge of model-specific quantities (e.g., separation between kernels
or visitation probabilities). We conclude by discussing the inherent gap
between our upper and lower bounds, providing insights into the unique
structure of this clustering problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages. Minor corrections in v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GL-LowPopArt: A Nearly Instance-Wise Minimax-Optimal Estimator for
  Generalized Low-Rank Trace Regression <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Lee, Kyoungseok Jang, Kwang-Sung Jun, Milan Vojnović, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 2 figures, 3 tables; Accepted as a Spotlight Poster to the
  42nd International Conference on Machine Learning (ICML 2025). Minor
  correction to the arXiv title in v2 ;). Added ToC in v3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fractured Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.12992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.12992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning. Code is available at
https://github.com/BaohaoLiao/frac-cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $k$-Submodular Interdiction Problems under Distributional
  Risk-Receptiveness and Robustness: Application to Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13023v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13023v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghun Park, Manish Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study submodular optimization in adversarial context, applicable to
machine learning problems such as feature selection using data susceptible to
uncertainties and attacks. We focus on Stackelberg games between an attacker
(or interdictor) and a defender where the attacker aims to minimize the
defender's objective of maximizing a $k$-submodular function. We allow
uncertainties arising from the success of attacks and inherent data noise, and
address challenges due to incomplete knowledge of the probability distribution
of random parameters. Specifically, we introduce Distributionally Robust
$k$-Submodular Interdiction Problem (DRO $k$-SIP) and Distributionally
Risk-Receptive $k$-Submodular Interdiction Problem (DRR $k$-SIP) along with
finitely convergent exact algorithms for solving them. When solving the DRO
$k$-SIP, the attacker optimizes their expected payoff with respect to the
worst-case probability distribution within the ambiguity set, and thereby have
robust attack strategies despite distributional ambiguity. In contrast, the DRR
$k$-SIP identifies attacker strategies with the best-case probability
distribution, and identifies critical vulnerabilities for the defender. The
optimal values derived from both DRO $k$-SIP and DRR $k$-SIP offer a confidence
interval-like range for the expected value of the defender's objective
function, capturing distributional ambiguity. We conduct computational
experiments on instances of feature selection and sensor placement problems,
using Wisconsin breast cancer data and synthetic data, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Continual Learning in Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of generative models has enabled modern AI systems to
comprehend and produce highly sophisticated content, even achieving human-level
performance in specific domains. However, these models remain fundamentally
constrained by catastrophic forgetting - a persistent challenge where adapting
to new tasks typically leads to significant degradation in performance on
previously learned tasks. To address this practical limitation, numerous
approaches have been proposed to enhance the adaptability and scalability of
generative models in real-world applications. In this work, we present a
comprehensive survey of continual learning methods for mainstream generative
models, including large language models, multimodal large language models,
vision language action models, and diffusion models. Drawing inspiration from
the memory mechanisms of the human brain, we systematically categorize these
approaches into three paradigms: architecture-based, regularization-based, and
replay-based methods, while elucidating their underlying methodologies and
motivations. We further analyze continual learning setups for different
generative models, including training objectives, benchmarks, and core
backbones, offering deeper insights into the field. The project page of this
paper is available at
https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and
  General Noise for Global Causal Discovery <span class="chip">UAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11759v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11759v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujai Hiremath, Promit Ghosal, Kyra Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal relationships from observational data is crucial when
experiments are costly or infeasible. Additive noise models (ANMs) enable
unique directed acyclic graph (DAG) identification, but existing
sample-efficient ANM methods often rely on restrictive assumptions on the data
generating process, limiting their applicability to real-world settings. We
propose local search in additive noise models, LoSAM, a topological ordering
method for learning a unique DAG in ANMs with mixed causal mechanisms and
general noise distributions. We introduce new causal substructures and criteria
for identifying roots and leaves, enabling efficient top-down learning. We
prove asymptotic consistency and polynomial runtime, ensuring scalability and
sample efficiency. We test LoSAM on synthetic and real-world data,
demonstrating state-of-the-art performance across all mixed mechanism settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Forty-First Annual Conference on Uncertainty in
  Artificial Intelligence (UAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RadioRAG: Online Retrieval-augmented Generation for Radiology Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15621v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15621v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Lisa Adams, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate outdated or inaccurate
information based on static training datasets. Retrieval-augmented generation
(RAG) mitigates this by integrating outside data sources. While previous RAG
systems used pre-assembled, fixed databases with limited flexibility, we have
developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data
from authoritative radiologic online sources in real-time. We evaluate the
diagnostic accuracy of various LLMs when answering radiology-specific questions
with and without access to additional online information via RAG. Using 80
questions from the RSNA Case Collection across radiologic subspecialties and 24
additional expert-curated questions with reference standard answers, LLMs
(GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were
prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG
retrieved context-specific information from Radiopaedia in real-time. Accuracy
was investigated. Statistical analyses were performed using bootstrapping. The
results were further compared with human performance. RadioRAG improved
diagnostic accuracy across most LLMs, with relative accuracy increases ranging
up to 54% for different LLMs. It matched or exceeded non-RAG models and the
human radiologist in question answering across radiologic subspecialties,
particularly in breast imaging and emergency radiology. However, the degree of
improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1
saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement,
highlighting variability in RadioRAG's effectiveness. LLMs benefit when
provided access to domain-specific data beyond their training data. RadioRAG
shows potential to improve LLM accuracy and factuality in radiology question
answering by integrating real-time domain-specific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Radiology: Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contributions to Representation Learning with Graph Autoencoders and
  Applications to Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph autoencoders (GAE) and variational graph autoencoders (VGAE) emerged as
two powerful groups of unsupervised node embedding methods, with various
applications to graph-based machine learning problems such as link prediction
and community detection. Nonetheless, at the beginning of this Ph.D. project,
GAE and VGAE models were also suffering from key limitations, preventing them
from being adopted in the industry. In this thesis, we present several
contributions to improve these models, with the general aim of facilitating
their use to address industrial-level problems involving graph representations.
Firstly, we propose two strategies to overcome the scalability issues of
previous GAE and VGAE models, permitting to effectively train these models on
large graphs with millions of nodes and edges. These strategies leverage graph
degeneracy and stochastic subgraph decoding techniques, respectively. Besides,
we introduce Gravity-Inspired GAE and VGAE, providing the first extensions of
these models for directed graphs, that are ubiquitous in industrial
applications. We also consider extensions of GAE and VGAE models for dynamic
graphs. Furthermore, we argue that GAE and VGAE models are often unnecessarily
complex, and we propose to simplify them by leveraging linear encoders. Lastly,
we introduce Modularity-Aware GAE and VGAE to improve community detection on
graphs, while jointly preserving good performances on link prediction. In the
last part of this thesis, we evaluate our methods on several graphs extracted
from the music streaming service Deezer. We put the emphasis on graph-based
music recommendation problems. In particular, we show that our methods can
improve the detection of communities of similar musical items to recommend to
users, that they can effectively rank similar artists in a cold start setting,
and that they permit modeling the music genre perception across cultures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ph.D. thesis defended at \'Ecole Polytechnique (IPP) in March 2022.
  As mentioned in this thesis, several chapters present results also published
  in scientific articles written with co-authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3-JEPA: Multimodal Alignment via Multi-gate MoE based on the
  Joint-Embedding Predictive Architecture <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05929v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05929v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Lei, Xiaolong Cheng, Qi Qin, Dan Wang, Kun Fan, Huazhen Huang, Qingqing Gu, Yetao Wu, Zhonglin Jiang, Yong Chen, Luo Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal learning strategies primarily optimize in the original
token space. Such a framework is easy to incorporate with the backbone of
pretrained language model, but might result in modality collapse. To alleviate
such issues, we leverage the Joint-Embedding Predictive Architecture (JEPA) on
the multimodal tasks, which converts the input embedding into the output
embedding space by a predictor and then conducts the cross-modal alignment on
the latent space. We implement this predictor by a Multi-Gate Mixture of
Experts (MMoE) and name the framework as M3-JEPA, accordingly. The gating
function disentangles the modality-specific and shared information and derives
information-theoretic optimality. The framework is implemented with both
contrastive and regularization loss, and solved by alternative gradient descent
(AGD) between different multimodal tasks. By thoroughly designed experiments,
we show that M3-JEPA can obtain state-of-the-art performance on different
modalities and tasks, generalize to unseen datasets and domains, and is
computationally efficient in both training and inference. Our observation
suggests that M3-JEPA might become a new basis to self-supervised learning in
the open world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures. ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KANITE: Kolmogorov-Arnold Networks for ITE estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshan Mehendale, Abhinav Thorat, Ravi Kolla, Niranjan Pedanekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)
for Individual Treatment Effect (ITE) estimation under multiple treatments
setting in causal inference. By utilizing KAN's unique abilities to learn
univariate activation functions as opposed to learning linear weights by
Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE
framework comprises two key architectures: 1.Integral Probability Metric (IPM)
architecture: This employs an IPM loss in a specialized manner to effectively
align towards ITE estimation across multiple treatments. 2. Entropy Balancing
(EB) architecture: This uses weights for samples that are learned by optimizing
entropy subject to balancing the covariates across treatment groups. Extensive
evaluations on benchmark datasets demonstrate that KANITE outperforms
state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and
$\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of
KANITE in achieving improved causal estimates, emphasizing the potential of
KANs to advance causal inference methodologies across diverse application
areas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interchangeable Token Embeddings for Extendable Vocabulary and
  Alpha-Equivalence <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models lack the notion of interchangeable tokens: symbols that are
semantically equivalent yet distinct, such as bound variables in formal logic.
This limitation prevents generalization to larger vocabularies and hinders the
model's ability to recognize alpha-equivalence, where renaming bound variables
preserves meaning. We formalize this machine learning problem and introduce
alpha-covariance, a metric for evaluating robustness to such transformations.
To tackle this task, we propose a dual-part token embedding strategy: a shared
component ensures semantic consistency, while a randomized component maintains
token distinguishability. Compared to a baseline that relies on alpha-renaming
for data augmentation, our approach demonstrates improved generalization to
unseen tokens in linear temporal logic solving, propositional logic assignment
prediction, and copying with an extendable vocabulary, while introducing a
favorable inductive bias for alpha-equivalence. Our findings establish a
foundation for designing language models that can learn interchangeable token
representations, a crucial step toward more flexible and systematic reasoning
in formal domains. Our code and project page are available at
https://necrashter.github.io/interchangeable-token-embeddings
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025 Poster Paper, Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Neurocognitive Disorders through Analyses of Topic Evolution
  and Cross-modal Consistency in Visual-Stimulated Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Li, Yuejiao Wang, Junan Li, Jiawen Kang, Bo Zheng, Simon Wong, Brian Mak, Helene Fung, Jean Woo, Man-Wai Mak, Timothy Kwok, Vincent Mok, Xianmin Gong, Xixin Wu, Xunying Liu, Patrick Wong, Helen Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of neurocognitive disorders (NCDs) is crucial for timely
intervention and disease management. Given that language impairments manifest
early in NCD progression, visual-stimulated narrative (VSN)-based analysis
offers a promising avenue for NCD detection. Current VSN-based NCD detection
methods primarily focus on linguistic microstructures (e.g., pauses, lexical
diversity), which are potentially linked to bottom-up (stimulus-driven)
cognitive processing. While these features illuminate basic language abilities,
the higher-order linguistic macrostructures (e.g., thematic or logical
development), which may reflect top-down (concept-driven) cognitive abilities,
remain underexplored. These patterns are crucial for NCD detection yet
challenging to quantify due to their abstract and complex nature. To bridge
this gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic
Topic Model (DTM) to track topic evolution over time, and (2) Text-Image
Temporal Alignment Network (TITAN) to measure cross-modal consistency between
speech and visual stimuli. Experimental results validated the efficiency of
proposed approaches in NCD detection, with TITAN achieving superior performance
both on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 =
0.8889). The feature contribution analysis revealed that macrostructural
features (e.g., topic variability, topic change rate, and topic consistency)
constituted the most significant contributors in the model's decision pathways,
outperforming investigated microstructural features. These findings underscore
the critical role of macrostructural patterns in understanding cognitive
impairment mechanisms in NCDs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to JSTSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Convergence for High-Order ODE Solvers in Diffusion Probabilistic
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Zhengyu Huang, Jiaoyang Huang, Zhengjiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models generate samples by learning to reverse a
noise-injection process that transforms data into noise. Reformulating this
reverse process as a deterministic probability flow ordinary differential
equation (ODE) enables efficient sampling using high-order solvers, often
requiring only $\mathcal{O}(10)$ steps. Since the score function is typically
approximated by a neural network, analyzing the interaction between its
regularity, approximation error, and numerical integration error is key to
understanding the overall sampling accuracy. In this work, we continue our
analysis of the convergence properties of the deterministic sampling methods
derived from probability flow ODEs [25], focusing on $p$-th order (exponential)
Runge-Kutta schemes for any integer $p \geq 1$. Under the assumption that the
first and second derivatives of the approximate score function are bounded, we
develop $p$-th order (exponential) Runge-Kutta schemes and demonstrate that the
total variation distance between the target distribution and the generated data
distribution can be bounded above by \begin{align*}
  O\bigl(d^{\frac{7}{4}}\varepsilon_{\text{score}}^{\frac{1}{2}}
+d(dH_{\max})^p\bigr), \end{align*} where $\varepsilon^2_{\text{score}}$
denotes the $L^2$ error in the score function approximation, $d$ is the data
dimension and $H_{\max}$ represents the maximum step size used in the solver.
We numerically verify the regularity assumption on benchmark datasets,
confirming that the first and second derivatives of the approximate score
function remain bounded in practice. Our theoretical guarantees hold for
general forward processes with arbitrary variance schedules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local minima of the empirical risk in high dimension: General theorems
  and convex examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiana Asgari, Andrea Montanari, Basil Saeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a general model for high-dimensional empirical risk minimization
whereby the data $\mathbf{x}_i$ are $d$-dimensional isotropic Gaussian vectors,
the model is parametrized by $\mathbf{\Theta}\in\mathbb{R}^{d\times k}$, and
the loss depends on the data via the projection
$\mathbf{\Theta}^\mathsf{T}\mathbf{x}_i$. This setting covers as special cases
classical statistics methods (e.g. multinomial regression and other generalized
linear models), but also two-layer fully connected neural networks with $k$
hidden neurons. We use the Kac-Rice formula from Gaussian process theory to
derive a bound on the expected number of local minima of this empirical risk,
under the proportional asymptotics in which $n,d\to\infty$, with $n\asymp d$.
Via Markov's inequality, this bound allows to determine the positions of these
minimizers (with exponential deviation bounds) and hence derive sharp
asymptotics on the estimation and prediction error. In this paper, we apply our
characterization to convex losses, where high-dimensional asymptotics were not
(in general) rigorously established for $k\ge 2$. We show that our approach is
tight and allows to prove previously conjectured results. In addition, we
characterize the spectrum of the Hessian at the minimizer. A companion paper
applies our general result to non-convex examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>101 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position Paper: Rethinking Privacy in RL for Sequential Decision-making
  in the Age of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flint Xiaofeng Fan, Cheston Tan, Roger Wattenhofer, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCNN 2025 Position Paper Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new type of federated clustering: A non-model-sharing approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Kawamata, Kaoru Kamijo, Masateru Kihira, Akihiro Toyoda, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the growing need to leverage sensitive data across
institutions has led to increased attention on federated learning (FL), a
decentralized machine learning paradigm that enables model training without
sharing raw data. However, existing FL-based clustering methods, known as
federated clustering, typically assume simple data partitioning scenarios such
as horizontal or vertical splits, and cannot handle more complex distributed
structures. This study proposes data collaboration clustering (DC-Clustering),
a novel federated clustering method that supports clustering over complex data
partitioning scenarios where horizontal and vertical splits coexist. In
DC-Clustering, each institution shares only intermediate representations
instead of raw data, ensuring privacy preservation while enabling collaborative
clustering. The method allows flexible selection between k-means and spectral
clustering, and achieves final results with a single round of communication
with the central server. We conducted extensive experiments using synthetic and
open benchmark datasets. The results show that our method achieves clustering
performance comparable to centralized clustering where all data are pooled.
DC-Clustering addresses an important gap in current FL research by enabling
effective knowledge discovery from distributed heterogeneous data. Its
practical properties -- privacy preservation, communication efficiency, and
flexibility -- make it a promising tool for privacy-sensitive domains such as
healthcare and finance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable representation learning of quantum data enabled by
  probabilistic variational autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paulin de Schoulepnikoff, Gorka Muñoz-Gil, Hendrik Poulsen Nautrup, Hans J. Briegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretable machine learning is rapidly becoming a crucial tool for
scientific discovery. Among existing approaches, variational autoencoders
(VAEs) have shown promise in extracting the hidden physical features of some
input data, with no supervision nor prior knowledge of the system at study.
Yet, the ability of VAEs to create meaningful, interpretable representations
relies on their accurate approximation of the underlying probability
distribution of their input. When dealing with quantum data, VAEs must hence
account for its intrinsic randomness and complex correlations. While VAEs have
been previously applied to quantum data, they have often neglected its
probabilistic nature, hindering the extraction of meaningful physical
descriptors. Here, we demonstrate that two key modifications enable VAEs to
learn physically meaningful latent representations: a decoder capable of
faithfully reproduce quantum states and a probabilistic loss tailored to this
task. Using benchmark quantum spin models, we identify regimes where standard
methods fail while the representations learned by our approach remain
meaningful and interpretable. Applied to experimental data from Rydberg atom
arrays, the model autonomously uncovers the phase structure without access to
prior labels, Hamiltonian details, or knowledge of relevant order parameters,
highlighting its potential as an unsupervised and interpretable tool for the
study of quantum systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text 10 pages, total document 16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning Under Adversarial Resource Constraints: A Spending
  Plan Is All You Need! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13244v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13244v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, Christian Kroer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study online decision making problems under resource constraints, where
both reward and cost functions are drawn from distributions that may change
adversarially over time. We focus on two canonical settings: $(i)$ online
resource allocation where rewards and costs are observed before action
selection, and $(ii)$ online learning with resource constraints where they are
observed after action selection, under full feedback or bandit feedback. It is
well known that achieving sublinear regret in these settings is impossible when
reward and cost distributions may change arbitrarily over time. To address this
challenge, we analyze a framework in which the learner is guided by a spending
plan--a sequence prescribing expected resource usage across rounds. We design
general (primal-)dual methods that achieve sublinear regret with respect to
baselines that follow the spending plan. Crucially, the performance of our
algorithms improves when the spending plan ensures a well-balanced distribution
of the budget across rounds. We additionally provide a robust variant of our
methods to handle worst-case scenarios where the spending plan is highly
imbalanced. To conclude, we study the regret of our algorithms when competing
against benchmarks that deviate from the prescribed spending plan.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially Robust Bloom Filters: Privacy, Reductions, and Open
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15751v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15751v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayder Tirmazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Bloom filter is a space-efficient probabilistic data structure that
represents a set $S$ of elements from a larger universe $U$. This efficiency
comes with a trade-off, namely, it allows for a small chance of false
positives. When you query the Bloom filter about an element x, the filter will
respond 'Yes' if $x \in S$. If $x \notin S$, it may still respond 'Yes' with
probability at most $\varepsilon$. We investigate the adversarial robustness
and privacy of Bloom filters, addressing open problems across three prominent
frameworks: the game-based model of Naor-Oved-Yogev (NOY), the simulator-based
model of Filic et. al., and learning-augmented variants. We prove the first
formal connection between the Filic and NOY models, showing that Filic
correctness implies AB-test resilience. We resolve a longstanding open question
by proving that PRF-backed Bloom filters fail the NOY model's stronger BP-test.
Finally, we introduce the first private Bloom filters with differential privacy
guarantees, including constructions applicable to learned Bloom filters. Our
taxonomy organizes the space of robustness and privacy guarantees, clarifying
relationships between models and constructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Graph Anomaly Detection: A <span class="highlight-title">Survey</span> and New Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hezhe Qiao, Hanghang Tong, Bo An, Irwin King, Charu Aggarwal, Guansong Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection (GAD), which aims to identify unusual graph instances
(nodes, edges, subgraphs, or graphs), has attracted increasing attention in
recent years due to its significance in a wide range of applications. Deep
learning approaches, graph neural networks (GNNs) in particular, have been
emerging as a promising paradigm for GAD, owing to its strong capability in
capturing complex structure and/or node attributes in graph data. Considering
the large number of methods proposed for GNN-based GAD, it is of paramount
importance to summarize the methodologies and findings in the existing GAD
studies, so that we can pinpoint effective model designs for tackling open GAD
problems. To this end, in this work we aim to present a comprehensive review of
deep learning approaches for GAD. Existing GAD surveys are focused on
task-specific discussions, making it difficult to understand the technical
insights of existing methods and their limitations in addressing some unique
challenges in GAD. To fill this gap, we first discuss the problem complexities
and their resulting challenges in GAD, and then provide a systematic review of
current deep GAD methods from three novel perspectives of methodology,
including GNN backbone design, proxy task design for GAD, and graph anomaly
measures. To deepen the discussions, we further propose a taxonomy of 13
fine-grained method categories under these three perspectives to provide more
in-depth insights into the model designs and their capabilities. To facilitate
the experiments and validation, we also summarize a collection of widely-used
GAD datasets and empirical comparison. We further discuss multiple open
problems to inspire more future high-quality research. A continuously updated
repository for datasets, links to the codes of algorithms, and empirical
comparison is available at
https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learners Should Acknowledge the Legal Implications of Large
  Language Models as Personal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrik Nolte, Michèle Finck, Kristof Meding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does GPT know you? The answer depends on your level of public recognition;
however, if your information was available on a website, the answer could be
yes. Most Large Language Models (LLMs) memorize training data to some extent.
Thus, even when an LLM memorizes only a small amount of personal data, it
typically falls within the scope of data protection laws. If a person is
identified or identifiable, the implications are far-reaching. The LLM is
subject to EU General Data Protection Regulation requirements even after the
training phase is concluded. To back our arguments: (1.) We reiterate that LLMs
output training data at inference time, be it verbatim or in generalized form.
(2.) We show that some LLMs can thus be considered personal data on their own.
This triggers a cascade of data protection implications such as data subject
rights, including rights to access, rectification, or erasure. These rights
extend to the information embedded within the AI model. (3.) This paper argues
that machine learning researchers must acknowledge the legal implications of
LLMs as personal data throughout the full ML development lifecycle, from data
collection and curation to model provision on e.g., GitHub or Hugging Face.
(4.) We propose different ways for the ML research community to deal with these
legal implications. Our paper serves as a starting point for improving the
alignment between data protection law and the technical capabilities of LLMs.
Our findings underscore the need for more interaction between the legal domain
and the ML community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers Learn Faster with Semantic Focus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parikshit Ram, Kenneth L. Clarkson, Tim Klinger, Shashanka Ubaru, Alexander G. Gray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various forms of sparse attention have been explored to mitigate the
quadratic computational and memory cost of the attention mechanism in
transformers. We study sparse transformers not through a lens of efficiency but
rather in terms of learnability and generalization. Empirically studying a
range of attention mechanisms, we find that input-dependent sparse attention
models appear to converge faster and generalize better than standard attention
models, while input-agnostic sparse attention models show no such benefits -- a
phenomenon that is robust across architectural and optimization hyperparameter
choices. This can be interpreted as demonstrating that concentrating a model's
"semantic focus" with respect to the tokens currently being considered (in the
form of input-dependent sparse attention) accelerates learning. We develop a
theoretical characterization of the conditions that explain this behavior. We
establish a connection between the stability of the standard softmax and the
loss function's Lipschitz properties, then show how sparsity affects the
stability of the softmax and the subsequent convergence and generalization
guarantees resulting from the attention mechanism. This allows us to
theoretically establish that input-agnostic sparse attention does not provide
any benefits. We also characterize conditions when semantic focus
(input-dependent sparse attention) can provide improved guarantees, and we
validate that these conditions are in fact met in our empirical evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blockchain-Enabled Variational Information Bottleneck for Data
  Extraction Based on Mutual Information in Internet of Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cui Zhang, Wenjun Zhang, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Vehicles (IoV) network can address the issue of limited
computing resources and data processing capabilities of individual vehicles,
but it also brings the risk of privacy leakage to vehicle users. Applying
blockchain technology can establish secure data links within the IoV, solving
the problems of insufficient computing resources for each vehicle and the
security of data transmission over the network. However, with the development
of the IoV, the amount of data interaction between multiple vehicles and
between vehicles and base stations, roadside units, etc., is continuously
increasing. There is a need to further reduce the interaction volume, and
intelligent data compression is key to solving this problem. The VIB technique
facilitates the training of encoding and decoding models, substantially
diminishing the volume of data that needs to be transmitted. This paper
introduces an innovative approach that integrates blockchain with VIB, referred
to as BVIB, designed to lighten computational workloads and reinforce the
security of the network. We first construct a new network framework by
separating the encoding and decoding networks to address the computational
burden issue, and then propose a new algorithm to enhance the security of IoV
networks. We also discuss the impact of the data extraction rate on system
latency to determine the most suitable data extraction rate. An experimental
framework combining Python and C++ has been established to substantiate the
efficacy of our BVIB approach. Comprehensive simulation studies indicate that
the BVIB consistently excels in comparison to alternative foundational
methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Journal. The source code has
  been released at:
  https://github.com/qiongwu86/BVIB-for-Data-Extraction-Based-on
  Mutual-Information-in-the-IoV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Relationships of Subjects and Shapelets for
  Semi-supervised Multivariate Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingsen Du, Meng Chen, Yongjian Li, Cun Ji, Shoushui Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series (MTS) classification is widely applied in fields
such as industry, healthcare, and finance, aiming to extract key features from
complex time series data for accurate decision-making and prediction. However,
existing methods for MTS often struggle due to the challenges of effectively
modeling high-dimensional data and the lack of labeled data, resulting in poor
classification performance. To address this issue, we propose a heterogeneous
relationships of subjects and shapelets method for semi-supervised MTS
classification. This method offers a novel perspective by integrating various
types of additional information while capturing the relationships between them.
Specifically, we first utilize a contrast temporal self-attention module to
obtain sparse MTS representations, and then model the similarities between
these representations using soft dynamic time warping to construct a similarity
graph. Secondly, we learn the shapelets for different subject types,
incorporating both the subject features and their shapelets as additional
information to further refine the similarity graph, ultimately generating a
heterogeneous graph. Finally, we use a dual level graph attention network to
get prediction. Through this method, we successfully transform dataset into a
heterogeneous graph, integrating multiple additional information and achieving
precise semi-supervised node classification. Experiments on the Human Activity
Recognition, sleep stage classification and University of East Anglia datasets
demonstrate that our method outperforms current state-of-the-art methods in MTS
classification tasks, validating its superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We would like to request the withdrawal of our manuscript due to
  logical errors in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time
  Series Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingsen Du, Meng Chen, Yongjian Li, Xiuxin Zhang, Jiahui Gao, Cun Ji, Shoushui Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series (MTS) data is generated through multiple sensors
across various domains such as engineering application, health monitoring, and
the internet of things, characterized by its temporal changes and high
dimensional characteristics. Over the past few years, many studies have
explored the long-range dependencies and similarities in MTS. However,
long-range dependencies are difficult to model due to their temporal changes
and high dimensionality makes it difficult to obtain similarities effectively
and efficiently. Thus, to address these issues, we propose contrast
similarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba).
Firstly, to obtain the dynamic similarity of each sample, we initially use
temporal contrast learning module to acquire MTS representations. And then we
construct a similarity matrix between MTS representations using Fast Dynamic
Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the
bidirectional nature of MTS, allowing us to better capture long-range and
short-range dependencies within the data. Finally, we utilize the
Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the
information interaction in the matrix and MTS node classification task. By
comprehensively considering the long-range dependencies and dynamic similarity
features, we achieved precise MTS node classification. We conducted experiments
on multiple University of East Anglia (UEA) MTS datasets, which encompass
diverse application scenarios. Our results demonstrate the superiority of our
method through both supervised and semi-supervised experiments on the MTS
classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We would like to request the withdrawal of our manuscript due to
  logical errors in the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implementation and Assessment of Machine Learning Models for Forecasting
  Suspected Opioid Overdoses in Emergency Medical Services Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron D. Mullen, Daniel R. Harris, Peter Rock, Katherine Thompson, Svetla Slavova, Jeffery Talbert, V. K. Cody Bumgardner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present efforts in the fields of machine learning and time series
forecasting to accurately predict counts of future suspected opioid overdoses
recorded by Emergency Medical Services (EMS) in the state of Kentucky.
Forecasts help government agencies properly prepare and distribute resources
related to opioid overdoses. Our approach uses county and district level
aggregations of suspected opioid overdose encounters and forecasts future
counts for different time intervals. Models with different levels of complexity
were evaluated to minimize forecasting error. A variety of additional
covariates relevant to opioid overdoses and public health were tested to
determine their impact on model performance. Our evaluation shows that useful
predictions can be generated with limited error for different types of regions,
and high performance can be achieved using commonly available covariates and
relatively simple forecasting models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Deep Reinforcement Learning Based Gradient Quantization for
  Federated Learning Enabled Vehicle Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cui Zhang, Wenjun Zhang, Qiong Wu, Pingyi Fan, Qiang Fan, Jiangzhou Wang, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) can protect the privacy of the vehicles in vehicle
edge computing (VEC) to a certain extent through sharing the gradients of
vehicles' local models instead of local data. The gradients of vehicles' local
models are usually large for the vehicular artificial intelligence (AI)
applications, thus transmitting such large gradients would cause large
per-round latency. Gradient quantization has been proposed as one effective
approach to reduce the per-round latency in FL enabled VEC through compressing
gradients and reducing the number of bits, i.e., the quantization level, to
transmit gradients. The selection of quantization level and thresholds
determines the quantization error, which further affects the model accuracy and
training time. To do so, the total training time and quantization error (QE)
become two key metrics for the FL enabled VEC. It is critical to jointly
optimize the total training time and QE for the FL enabled VEC. However, the
time-varying channel condition causes more challenges to solve this problem. In
this paper, we propose a distributed deep reinforcement learning (DRL)-based
quantization level allocation scheme to optimize the long-term reward in terms
of the total training time and QE. Extensive simulations identify the optimal
weighted factors between the total training time and QE, and demonstrate the
feasibility and effectiveness of the proposed scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Internet of Things Journal. The
  source code has been released at:
  https://github.com/qiongwu86/Distributed-Deep-Reinforcement-Learning-Based-Gradient
  Quantization-for-Federated-Learning-Enabled-Vehicle-Edge-Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bird Song Detector for improving bird identification through Deep
  Learning: a case study from Doñana 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.15576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.15576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alba Márquez-Rodríguez, Miguel Ángel Mohedano-Munoz, Manuel J. Marín-Jiménez, Eduardo Santamaría-García, Giulia Bastianelli, Pedro Jordano, Irene Mendoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive Acoustic Monitoring is a key tool for biodiversity conservation, but
the large volumes of unsupervised audio it generates present major challenges
for extracting meaningful information. Deep Learning offers promising
solutions. BirdNET, a widely used bird identification model, has shown success
in many study systems but is limited at local scale due to biases in its
training data, which focus on specific locations and target sounds rather than
entire soundscapes. A key challenge in bird species identification is that many
recordings either lack target species or contain overlapping vocalizations,
complicating automatic identification. To address these problems, we developed
a multi-stage pipeline for automatic bird vocalization identification in
Do\~nana National Park (SW Spain), a wetland of high conservation concern. We
deployed AudioMoth recorders in three main habitats across nine locations and
manually annotated 461 minutes of audio, resulting in 3749 labeled segments
spanning 34 classes. We first applied a Bird Song Detector to isolate bird
vocalizations using spectrogram-based image processing. Then, species were
classified using custom models trained at the local scale. Applying the Bird
Song Detector before classification improved species identification, as all
models performed better when analyzing only the segments where birds were
detected. Specifically, the combination of detector and fine-tuned BirdNET
outperformed the baseline without detection. This approach demonstrates the
effectiveness of integrating a Bird Song Detector with local classification
models. These findings highlight the need to adapt general-purpose tools to
specific ecological challenges. Automatically detecting bird species helps
track the health of this threatened ecosystem, given birds sensitivity to
environmental change, and supports conservation planning to reduce biodiversity
loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 14 images, for associated dataset see
  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for
  associated code see
  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and
  https://github.com/GrunCrow/Bird-Song-Detector</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Zero-Initialized Attention: Optimal Prompt and Gating Factor
  Estimation <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghiem T. Diep, Huy Nguyen, Chau Nguyen, Minh Le, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique
for LLaMA models, leveraging zero-initialized attention to stabilize training
and enhance performance. However, despite its empirical success, the
theoretical foundations of zero-initialized attention remain largely
unexplored. In this paper, we provide a rigorous theoretical analysis,
establishing a connection between zero-initialized attention and
mixture-of-expert models. We prove that both linear and non-linear prompts,
along with gating functions, can be optimally estimated, with non-linear
prompts offering greater flexibility for future applications. Empirically, we
validate our findings on the open LLM benchmarks, demonstrating that non-linear
prompts outperform linear ones. Notably, even with limited training data, both
prompt types consistently surpass vanilla attention, highlighting the
robustness and adaptability of zero-initialized attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable and accelerated spherical harmonic and Wigner transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14670v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14670v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew A. Price, Jason D. McEwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many areas of science and engineering encounter data defined on spherical
manifolds. Modelling and analysis of spherical data often necessitates
spherical harmonic transforms, at high degrees, and increasingly requires
efficient computation of gradients for machine learning or other differentiable
programming tasks. We develop novel algorithmic structures for accelerated and
differentiable computation of generalised Fourier transforms on the sphere
$\mathbb{S}^2$ and rotation group $\text{SO}(3)$, i.e. spherical harmonic and
Wigner transforms, respectively. We present a recursive algorithm for the
calculation of Wigner $d$-functions that is both stable to high harmonic
degrees and extremely parallelisable. By tightly coupling this with separable
spherical transforms, we obtain algorithms that exhibit an extremely
parallelisable structure that is well-suited for the high throughput computing
of modern hardware accelerators (e.g. GPUs). We also develop a hybrid automatic
and manual differentiation approach so that gradients can be computed
efficiently. Our algorithms are implemented within the JAX differentiable
programming framework in the S2FFT software code. Numerous samplings of the
sphere are supported, including equiangular and HEALPix sampling. Computational
errors are at the order of machine precision for spherical samplings that admit
a sampling theorem. When benchmarked against alternative C codes we observe up
to a 400-fold acceleration. Furthermore, when distributing over multiple GPUs
we achieve very close to optimal linear scaling with increasing number of GPUs
due to the highly parallelised and balanced nature of our algorithms. Provided
access to sufficiently many GPUs our transforms thus exhibit an unprecedented
effective linear time complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 7 figures, accepted by Journal of Computational Physics,
  code available at https://github.com/astro-informatics/s2fft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint
  Phase-shift Optimization and Multi-User Power Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangwei Qi, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicular edge computing (VEC) is an emerging technology with significant
potential in the field of internet of vehicles (IoV), enabling vehicles to
perform intensive computational tasks locally or offload them to nearby edge
devices. However, the quality of communication links may be severely
deteriorated due to obstacles such as buildings, impeding the offloading
process. To address this challenge, we introduce the use of Reconfigurable
Intelligent Surfaces (RIS), which provide alternative communication pathways to
assist vehicular communication. By dynamically adjusting the phase-shift of the
RIS, the performance of VEC systems can be substantially improved. In this
work, we consider a RIS-assisted VEC system, and design an optimal scheme for
local execution power, offloading power, and RIS phase-shift, where random task
arrivals and channel variations are taken into account. To address the scheme,
we propose an innovative deep reinforcement learning (DRL) framework that
combines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing
RIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy
Gradient (MADDPG) algorithm for optimizing the power allocation of vehicle user
(VU). Simulation results show that our proposed scheme outperforms the
traditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient
(TD3) and some typical stochastic schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Internet of Things Journal. The
  source code has been released at
  https://github.com/qiongwu86/DDPG-RIS-MADDPG-POWER. arXiv admin note: text
  overlap with arXiv:2406.11318</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Twin Vehicular Edge Computing Network: Task Offloading and
  Resource Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xie, Qiong Wu, Pingyi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing demand for multiple applications on internet of vehicles.
It requires vehicles to carry out multiple computing tasks in real time.
However, due to the insufficient computing capability of vehicles themselves,
offloading tasks to vehicular edge computing (VEC) servers and allocating
computing resources to tasks becomes a challenge. In this paper, a multi task
digital twin (DT) VEC network is established. By using DT to develop offloading
strategies and resource allocation strategies for multiple tasks of each
vehicle in a single slot, an optimization problem is constructed. To solve it,
we propose a multi-agent reinforcement learning method on the task offloading
and resource allocation. Numerous experiments demonstrate that our method is
effective compared to other benchmark algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICICSP 2024. The source code has been
  released
  at:https://github.com/qiongwu86/Digital-Twin-Vehicular-Edge-Computing-Network_Task-Offloading-and-Resource-Allocation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Semantic Communications in Internet of Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Ye, Qiong Wu, Pingyi Fan, Qiang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet of Vehicles (IoV), as the core of intelligent transportation system,
enables comprehensive interconnection between vehicles and their surroundings
through multiple communication modes, which is significant for autonomous
driving and intelligent traffic management. However, with the emergence of new
applications, traditional communication technologies face the problems of
scarce spectrum resources and high latency. Semantic communication, which
focuses on extracting, transmitting, and recovering some useful semantic
information from messages, can reduce redundant data transmission, improve
spectrum utilization, and provide innovative solutions to communication
challenges in the IoV. This paper systematically reviews state of art of
semantic communications in the IoV, elaborates the technical background of IoV
and semantic communications, and deeply discusses key technologies of semantic
communications in IoV, including semantic information extraction, semantic
communication architecture, resource allocation and management, and so on.
Through specific case studies, it demonstrates that semantic communications can
be effectively employed in the scenarios of traffic environment perception and
understanding, intelligent driving decision support, IoV service optimization,
and intelligent traffic management. Additionally, it analyzes the current
challenges and future research directions. This survey reveals that semantic
communications has broad application prospects in IoV, but it is necessary to
solve the real existing problems by combining advanced technologies to promote
its wide application in IoV and contributing to the development of intelligent
transportation system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to Entropy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORA: Coalitional Rational Advantage Decomposition for Multi-Agent
  Policy Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.04265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.04265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengda Ji, Genjiu Xu, Liying Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Jamming Source Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dania Herzalla, Willian T. Lunardi, Martin Andreoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based learning provides a powerful framework for modeling complex
relational structures; however, its application within the domain of wireless
security remains significantly underexplored. In this work, we introduce the
first application of graph-based learning for jamming source localization,
addressing the imminent threat of jamming attacks in wireless networks. Unlike
geometric optimization techniques that struggle under environmental
uncertainties and dense interference, we reformulate the localization as an
inductive graph regression task. Our approach integrates structured node
representations that encode local and global signal aggregation, ensuring
spatial coherence and adaptive signal fusion. To enhance robustness, we
incorporate an attention-based \ac{GNN} that adaptively refines neighborhood
influence and introduces a confidence-guided estimation mechanism that
dynamically balances learned predictions with domain-informed priors. We
evaluate our approach under complex \ac{RF} environments with various sampling
densities, network topologies, jammer characteristics, and signal propagation
conditions, conducting comprehensive ablation studies on graph construction,
feature selection, and pooling strategies. Results demonstrate that our novel
graph-based learning framework significantly outperforms established
localization baselines, particularly in challenging scenarios with sparse and
obfuscated signal information. Our code is available at
https://github.com/tiiuae/gnn-jamming-source-localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM
  <span class="highlight-title">Red Teaming</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.06253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.06253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Schoepf, Muhammad Zaid Hameed, Ambrish Rawat, Kieran Fraser, Giulio Zizzo, Giandomenico Cornacchia, Mark Purcell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With LLM usage rapidly increasing, their vulnerability to jailbreaks that
create harmful outputs are a major security risk. As new jailbreaking
strategies emerge and models are changed by fine-tuning, continuous testing for
security vulnerabilities is necessary. Existing Red Teaming methods fall short
in cost efficiency, attack success rate, attack diversity, or extensibility as
new attack types emerge. We address these challenges with Modular And Diverse
Malicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses
automatic assignment of attack strategies into relevant attack clusters,
chooses the most relevant clusters for a malicious goal, and then combines
strategies from the selected clusters to achieve diverse novel attacks with
high attack success rates. MAD-MAX further merges promising attacks together at
each iteration of Red Teaming to boost performance and introduces a similarity
filter to prune out similar attacks for increased cost efficiency. The MAD-MAX
approach is designed to be easily extensible with newly discovered attack
strategies and outperforms the prominent Red Teaming method Tree of Attacks
with Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and
queries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals
in our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX
does so with only 10.9 average queries to the target LLM compared to TAP with
23.3.
  WARNING: This paper contains contents which are offensive in nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data in Generative Models Workshop: The Bad, the Ugly, and the Greats
  (DIG-BUGS) at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Neural Network Training Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, Peter Mattson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training algorithms, broadly construed, are an essential part of every deep
learning pipeline. Training algorithm improvements that speed up training
across a wide variety of workloads (e.g., better update rules, tuning
protocols, learning rate schedules, or data selection schemes) could save time,
save computational resources, and lead to better, more accurate, models.
Unfortunately, as a community, we are currently unable to reliably identify
training algorithm improvements, or even determine the state-of-the-art
training algorithm. In this work, using concrete experiments, we argue that
real progress in speeding up training requires new benchmarks that resolve
three basic challenges faced by empirical comparisons of training algorithms:
(1) how to decide when training is complete and precisely measure training
time, (2) how to handle the sensitivity of measurements to exact workload
details, and (3) how to fairly compare algorithms that require hyperparameter
tuning. In order to address these challenges, we introduce a new, competitive,
time-to-result benchmark using multiple workloads running on fixed hardware,
the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of
workload variants that make it possible to detect benchmark submissions that
are more robust to workload changes than current widely-used methods. Finally,
we evaluate baseline submissions constructed using various optimizers that
represent current practice, as well as other optimizers that have recently
received attention in the literature. These baseline results collectively
demonstrate the feasibility of our benchmark, show that non-trivial gaps
between methods exist, and set a provisional state-of-the-art for future
benchmark submissions to try and surpass.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>102 pages, 8 figures, 41 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imagine Beyond! Distributionally Robust Auto-Encoding for State Space
  Coverage in Online Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.17830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.17830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Castanet, Olivier Sigaud, Sylvain Lamprier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously
acquire diverse behaviors, but faces major challenges in visual environments
due to high-dimensional, semantically sparse observations. In the online
setting, where agents learn representations while exploring, the latent space
evolves with the agent's policy, to capture newly discovered areas of the
environment. However, without incentivization to maximize state coverage in the
representation, classical approaches based on auto-encoders may converge to
latent spaces that over-represent a restricted set of states frequently visited
by the agent. This is exacerbated in an intrinsic motivation setting, where the
agent uses the distribution encoded in the latent space to sample the goals it
learns to master. To address this issue, we propose to progressively enforce
distributional shifts towards a uniform distribution over the full state space,
to ensure a full coverage of skills that can be learned in the environment. We
introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that
combines the $\beta$-VAE framework with Distributionally Robust Optimization.
DRAG leverages an adversarial neural weighter of training states of the VAE, to
account for the mismatch between the current data distribution and unseen parts
of the environment. This allows the agent to construct semantically meaningful
latent spaces beyond its immediate experience. Our approach improves state
space coverage and downstream control performance on hard exploration
environments such as mazes and robotic control involving walls to bypass,
without pre-training nor prior environment knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agile Orchestration at Will: An Entire Smart Service-Based Security
  Architecture Towards 6G 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.22963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.22963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Duan, Guoshun Nan, Rushan Li, Zijun Wang, Lihua Xiong, Chaoying Yuan, Guorong Liu, Hui Xu, Qimei Cui, Xiaofeng Tao, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The upcoming 6G will fundamentally reshape mobile networks beyond
communications, unlocking a multitude of applications that were once considered
unimaginable. Meanwhile, security and resilience are especially highlighted in
the 6G design principles. However, safeguarding 6G networks will be quite
challenging due to various known and unknown threats from highly heterogeneous
networks and diversified security requirements of distinct use cases, calling
for a comprehensive re-design of security architecture. This motivates us to
propose ES3A (Entire Smart Service-based Security Architecture), a novel
security architecture for 6G networks. Specifically, we first discuss six
high-level principles of our ES3A that include hierarchy, flexibility,
scalability, resilience, endogeny, and trust and privacy. With these goals in
mind, we then introduce three guidelines from a deployment perspective,
envisioning our ES3A that offers service-based security, end-to-end protection,
and smart security automation for 6G networks. Our architecture consists of
three layers and three domains. It relies on a two-stage orchestration
mechanism to tailor smart security strategies for customized protection in
high-dynamic 6G networks, thereby addressing the aforementioned challenges.
Finally, we prototype the proposed ES3A on a real-world radio system based on
Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.
We also provide a case to show the superiority of our architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Wireless Communications Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05754v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05754v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Chen, Hua-Lei Yin, Zeng-Bing Chen, Shengjun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose two hybrid quantum-inspired neural networks with
adaptive residual and dense connections respectively for pattern recognition.
We explain the frameworks of the symmetrical circuit models in the
quantum-inspired layers in our hybrid models. We also illustrate the potential
superiority of our hybrid models to prevent gradient explosion owing to the
sine and cosine functions in the quantum-inspired layers. Groups of numerical
experiments on generalization power showcase that our hybrid models are
comparable to the pure classical models with different noisy datasets utilized.
Furthermore, the comparison between our hybrid models and a state-of-the-art
hybrid quantum-classical convolutional network demonstrates 3%-4% higher
accuracy of our hybrid densely-connected model than the hybrid
quantum-classical network. Additionally, compared with other two hybrid
quantum-inspired residual networks, our hybrid models showcase a little higher
accuracy on image datasets with asymmetrical noises. Simultaneously, in terms
of groups of robustness experiments, the outcomes demonstrate that our two
hybrid models outperform pure classical models notably in resistance to
adversarial parameter attacks with various asymmetrical noises. They also
indicate the slight superiority of our densely-connected hybrid model over the
hybrid quantum-classical network to both symmetrical and asymmetrical attacks.
Meanwhile, the accuracy of our two hybrid models is a little bit higher than
that of the two hybrid quantum-inspired residual networks. In addition, an
ablation study indicate that the recognition accuracy of our two hybrid models
is 2%-3% higher than that of the traditional quantum-inspired neural network
without residual or dense connection. Eventually, we discuss the application
scenarios of our hybrid models by analyzing their computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages of main paper with two links of a 20-page supplementary
  material and the program codes below the acknowledgement in the main paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating
  Pre-trained Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Huasong Meng, Hao Guan, Liuhuo Wan, Sin Gee Teo, Guangdong Bai, Jin Song Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PAODING, a toolkit to debloat pretrained neural network models
through the lens of data-free pruning. To preserve the model fidelity, PAODING
adopts an iterative process, which dynamically measures the effect of deleting
a neuron to identify candidates that have the least impact to the output layer.
Our evaluation shows that PAODING can significantly reduce the model size,
generalize on different datasets and models, and meanwhile preserve the model
fidelity in terms of test accuracy and adversarial robustness. PAODING is
publicly available on PyPI via https://pypi.org/project/paoding-dl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The OCR Quest for Generalization: Learning to recognize low-resource
  alphabets with model editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrià Molina Rodríguez, Oriol Ramos Terrades, Josep Lladós
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint (under review) For Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Robustness-preserving Data-free Neural Network Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Jin Song Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying pre-trained neural network models in real-world applications,
model consumers often encounter resource-constraint platforms such as mobile
and smart devices. They typically use the pruning technique to reduce the size
and complexity of the model, generating a lighter one with less resource
consumption. Nonetheless, most existing pruning methods are proposed with the
premise that the model after being pruned has a chance to be fine-tuned or even
retrained based on the original training data. This may be unrealistic in
practice, as the data controllers are often reluctant to provide their model
consumers with the original data. In this work, we study the neural network
pruning in the data-free context, aiming to yield lightweight models that are
not only accurate in prediction but also robust against undesired inputs in
open-world deployments. Considering the absence of the fine-tuning and
retraining that can fix the mis-pruned units, we replace the traditional
aggressive one-shot strategy with a conservative one that treats the pruning as
a progressive process. We propose a pruning method based on stochastic
optimization that uses robustness-related metrics to guide the pruning process.
Our method is implemented as a Python program and evaluated with a series of
experiments on diverse neural network models. The experimental results show
that it significantly outperforms existing one-shot data-free pruning
approaches in terms of robustness preservation and accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEReBrO: Compact Encoder for Representations of Brain Oscillations Using
  Efficient Alternating Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10885v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10885v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandru Dimofte, Glenn Anta Bucagu, Thorir Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Luca Benini, Yawei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalograph (EEG) is a crucial tool for studying brain activity.
Recently, self-supervised learning methods leveraging large unlabeled datasets
have emerged as a potential solution to the scarcity of widely available
annotated EEG data. However, current methods suffer from at least one of the
following limitations: i) sub-optimal EEG signal modeling, ii) model sizes in
the hundreds of millions of trainable parameters, and iii) reliance on private
datasets and/or inconsistent public benchmarks, hindering reproducibility. To
address these challenges, we introduce a Compact Encoder for Representations of
Brain Oscillations using alternating attention (CEReBrO), a new small EEG
foundation model. Our tokenization scheme represents EEG signals at a
per-channel patch granularity. We propose an alternating attention mechanism
that jointly models intra-channel temporal dynamics and inter-channel spatial
correlations, achieving 2x speed improvement with 6x less memory required
compared to standard self-attention. We present several model sizes ranging
from 3.6 million to 85 million parameters. Pre-trained on over 20,000 hours of
publicly available scalp EEG recordings with diverse channel configurations,
our models set new benchmarks in emotion detection and seizure detection tasks,
with competitive performance in anomaly classification and gait prediction.
This validates our models' effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Serving Large Language Models on Huawei CloudMatrix384 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs), driven by growing
parameter scales, adoption of mixture-of-experts (MoE) architectures, and
expanding context lengths, imposes unprecedented demands on AI infrastructure.
Traditional AI clusters face limitations in compute intensity, memory
bandwidth, inter-chip communication, and latency, compounded by variable
workloads and strict service-level objectives. Addressing these issues requires
fundamentally redesigned hardware-software integration. This paper introduces
Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in
the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C
NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified
Bus (UB) network, enabling direct all-to-all communication and dynamic pooling
of resources. These features optimize performance for communication-intensive
operations, such as large-scale MoE expert parallelism and distributed
key-value cache access. To fully leverage CloudMatrix384, we propose
CloudMatrix-Infer, an advanced LLM serving solution incorporating three core
innovations: a peer-to-peer serving architecture that independently scales
prefill, decode, and caching; a large-scale expert parallelism strategy
supporting EP320 via efficient UB-based token dispatch; and hardware-aware
optimizations including specialized operators, microbatch-based pipelining, and
INT8 quantization. Evaluation with the DeepSeek-R1 model shows
CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of
6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms
TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s
per NPU even under stringent 15 ms latency constraints, while INT8 quantization
maintains model accuracy across benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>59 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Free Privacy Protection for Wireless Federated Learning: Enjoy It or
  Suffer from It? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicai Li, Tiejun Lv, Xiyu Zhao, Xin Yuan, Wei Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inherent communication noises have the potential to preserve privacy for
wireless federated learning (WFL) but have been overlooked in digital
communication systems predominantly using floating-point number standards,
e.g., IEEE 754, for data storage and transmission. This is due to the
potentially catastrophic consequences of bit errors in floating-point numbers,
e.g., on the sign or exponent bits. This paper presents a novel channel-native
bit-flipping differential privacy (DP) mechanism tailored for WFL, where
transmit bits are randomly flipped and communication noises are leveraged, to
collectively preserve the privacy of WFL in digital communication systems. The
key idea is to interpret the bit perturbation at the transmitter and bit errors
caused by communication noises as a bit-flipping DP process. This is achieved
by designing a new floating-point-to-fixed-point conversion method that only
transmits the bits in the fraction part of model parameters, hence eliminating
the need for transmitting the sign and exponent bits and preventing the
catastrophic consequence of bit errors. We analyze a new metric to measure the
bit-level distance of the model parameters and prove that the proposed
mechanism satisfies (\lambda,\epsilon)-R\'enyi DP and does not violate the WFL
convergence. Experiments validate privacy and convergence analysis of the
proposed mechanism and demonstrate its superiority to the state-of-the-art
Gaussian mechanisms that are channel-agnostic and add Gaussian noise for
privacy protection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures, accepted by IEEE Transactions on Information
  Forensics and Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13538v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13538v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Lin, Peter Palensky, Pedro P. Vergara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution time series data are crucial for the operation and planning
of energy systems such as electrical power systems and heating systems. Such
data often cannot be shared due to privacy concerns, necessitating the use of
synthetic data. However, high-resolution time series data is difficult to model
due to its inherent high dimensionality and complex temporal dependencies.
Leveraging the recent development of generative AI, especially diffusion
models, we propose EnergyDiff, a universal data generation framework for energy
time series data. EnergyDiff builds on state-of-the-art denoising diffusion
probabilistic models, utilizing a proposed denoising network dedicated to
high-resolution time series data and introducing a novel Marginal Calibration
technique. Our extensive experimental results demonstrate that EnergyDiff
achieves significant improvement in capturing the temporal dependencies and
marginal distributions compared to baselines, particularly at the 1-minute
resolution. EnergyDiff's universality is validated across diverse energy
domains (e.g., electricity demand, heat pump, PV, multiple time resolutions (1
minute, 15 minutes, 30 minutes and 1 hour), and at both customer and
transformer levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Non-Markovian Open Quantum Dynamics with Neural Quantum
  States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Cao, Liwei Ge, Daochi Zhang, Xiang Li, Yao Wang, Rui-Xue Xu, YiJing Yan, Xiao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing computational scaling for simulating non-Markovian dissipative
dynamics using artificial neural networks is both a major focus and formidable
challenge in open quantum systems. To enable neural quantum states (NQSs), we
encode environmental memory in dissipatons (quasiparticles with characteristic
lifetimes), yielding the dissipaton-embedded quantum master equation (DQME).
The resulting NQS-DQME framework achieves compact representation of many-body
correlations and non-Markovian memory. Benchmarking against numerically exact
hierarchical equations of motion confirms NQS-DQME maintains comparable
accuracy while enhancing scalability and interpretability. This methodology
opens new paths to explore non-Markovian open quantum dynamics in previously
intractable systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oscillatory State-Space Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03943v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03943v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Konstantin Rusch, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently
learning on long sequences. Inspired by cortical dynamics of biological neural
networks, we base our proposed LinOSS model on a system of forced harmonic
oscillators. A stable discretization, integrated over time using fast
associative parallel scans, yields the proposed state-space model. We prove
that LinOSS produces stable dynamics only requiring nonnegative diagonal state
matrix. This is in stark contrast to many previous state-space models relying
heavily on restrictive parameterizations. Moreover, we rigorously show that
LinOSS is universal, i.e., it can approximate any continuous and causal
operator mapping between time-varying functions, to desired accuracy. In
addition, we show that an implicit-explicit discretization of LinOSS perfectly
conserves the symmetry of time reversibility of the underlying dynamics.
Together, these properties enable efficient modeling of long-range
interactions, while ensuring stable and accurate long-horizon forecasting.
Finally, our empirical results, spanning a wide range of time-series tasks from
mid-range to very long-range classification and regression, as well as
long-horizon forecasting, demonstrate that our proposed LinOSS model
consistently outperforms state-of-the-art sequence models. Notably, LinOSS
outperforms Mamba and LRU by nearly 2x on a sequence modeling task with
sequences of length 50k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MENSA: A Multi-Event Network for Survival Analysis with Trajectory-based
  Likelihood Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Marius Lillelund, Ali Hossein Gharari Foomani, Weijie Sun, Shi-ang Qi, Russell Greiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MENSA, a novel deep learning model for multi-event survival
analysis, which predicts the time until an instance experiences multiple
distinct events based on its features. MENSA learns a shared representation of
the input features while capturing the complex dependence structures between
events. In practice, it optimizes the sum of the traditional negative
log-likelihood across events and a novel trajectory-based likelihood, which
encourages the model to learn the temporal order in which events occur.
Experiments on real-world clinical datasets demonstrate that MENSA improves
risk and time-to-event prediction compared to state-of-the-art models across
single-event, competing-risk, and multi-event settings. Moreover, MENSA
achieves this with fewer parameters and lower computational cost (FLOPs) than
several deep learning baselines, particularly in high-dimensional feature
spaces (more than 100 features).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence analysis of controlled particle systems arising in deep
  learning: from finite to infinite sample size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huafu Liao, Alpár R. Mészáros, Chenchen Mou, Chao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with a class of neural SDEs and studies the limiting
behavior of the associated sampled optimal control problems as the sample size
grows to infinity. The neural SDEs with $N$ samples can be linked to the
$N$-particle systems with centralized control. We analyze the
Hamilton-Jacobi-Bellman equation corresponding to the $N$-particle system and
establish regularity results which are uniform in $N$. The uniform regularity
estimates are obtained by the stochastic maximum principle and the analysis of
a backward stochastic Riccati equation. Using these uniform regularity results,
we show the convergence of the minima of the objective functionals and optimal
parameters of the neural SDEs as the sample size $N$ tends to infinity. The
limiting objects can be identified with suitable functions defined on the
Wasserstein space of Borel probability measures. Furthermore, quantitative
convergence rates are also obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRL-Based Resource Allocation for Motion Blur Resistant Federated
  Self-Supervised Learning in IoV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueying Gu, Qiong Wu, Pingyi Fan, Qiang Fan, Nan Cheng, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Internet of Vehicles (IoV), Federated Learning (FL) provides a
privacy-preserving solution by aggregating local models without sharing data.
Traditional supervised learning requires image data with labels, but data
labeling involves significant manual effort. Federated Self-Supervised Learning
(FSSL) utilizes Self-Supervised Learning (SSL) for local training in FL,
eliminating the need for labels while protecting privacy. Compared to other SSL
methods, Momentum Contrast (MoCo) reduces the demand for computing resources
and storage space by creating a dictionary. However, using MoCo in FSSL
requires uploading the local dictionary from vehicles to Base Station (BS),
which poses a risk of privacy leakage. Simplified Contrast (SimCo) addresses
the privacy leakage issue in MoCo-based FSSL by using dual temperature instead
of a dictionary to control sample distribution. Additionally, considering the
negative impact of motion blur on model aggregation, and based on SimCo, we
propose a motion blur-resistant FSSL method, referred to as BFSSL. Furthermore,
we address energy consumption and delay in the BFSSL process by proposing a
Deep Reinforcement Learning (DRL)-based resource allocation scheme, called
DRL-BFSSL. In this scheme, BS allocates the Central Processing Unit (CPU)
frequency and transmission power of vehicles to minimize energy consumption and
latency, while aggregating received models based on the motion blur level.
Simulation results validate the effectiveness of our proposed aggregation and
resource allocation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Internet of Things Journal. The
  source code has been released at: https://github.com/qiongwu86/DRL-BFSSL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRL-Based Optimization for AoI and Energy Consumption in C-V2X Enabled
  IoV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhang, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address communication latency issues, the Third Generation Partnership
Project (3GPP) has defined Cellular-Vehicle to Everything (C-V2X) technology,
which includes Vehicle-to-Vehicle (V2V) communication for direct
vehicle-to-vehicle communication. However, this method requires vehicles to
autonomously select communication resources based on the Semi-Persistent
Scheduling (SPS) protocol, which may lead to collisions due to different
vehicles sharing the same communication resources, thereby affecting
communication effectiveness. Non-Orthogonal Multiple Access (NOMA) is
considered a potential solution for handling large-scale vehicle communication,
as it can enhance the Signal-to-Interference-plus-Noise Ratio (SINR) by
employing Successive Interference Cancellation (SIC), thereby reducing the
negative impact of communication collisions. When evaluating vehicle
communication performance, traditional metrics such as reliability and
transmission delay present certain contradictions. Introducing the new metric
Age of Information (AoI) provides a more comprehensive evaluation of
communication system. Additionally, to ensure service quality, user terminals
need to possess high computational capabilities, which may lead to increased
energy consumption, necessitating a trade-off between communication energy
consumption and effectiveness. Given the complexity and dynamics of
communication systems, Deep Reinforcement Learning (DRL) serves as an
intelligent learning method capable of learning optimal strategies in dynamic
environments. Therefore, this paper analyzes the effects of multi-priority
queues and NOMA on AoI in the C-V2X vehicular communication system and proposes
an energy consumption and AoI optimization method based on DRL. Finally,
through comparative simulations with baseline methods, the proposed approach
demonstrates its advances in terms of energy consumption and AoI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Transactions on Green
  Communications and Networking. The source code has been released at:
  https://github.com/qiongwu86/DRL-Based-Optimization-for-Information-of-Age-and-Energy-Consumption-in-C-V2X-Enabled-IoV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trust Region Preference Approximation: A simple and stable reinforcement
  learning algorithm for LLM reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have rapidly evolved, approaching
Artificial General Intelligence (AGI) while benefiting from large-scale
reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent
reward-based optimization algorithms, such as Proximal Policy Optimization
(PPO) and Group Relative Policy Optimization (GRPO) have achieved significant
performance on reasoning tasks, whereas preference-based optimization
algorithms such as Direct Preference Optimization (DPO) significantly improve
the performance of LLMs on human alignment. However, despite the strong
performance of reward-based optimization methods in alignment tasks , they
remain vulnerable to reward hacking. Furthermore, preference-based algorithms
(such as Online DPO) haven't yet matched the performance of reward-based
optimization algorithms (like PPO) on reasoning tasks, making their exploration
in this specific area still a worthwhile pursuit. Motivated by these
challenges, we propose the Trust Region Preference Approximation (TRPA)
algorithm, which integrates rule-based optimization with preference-based
optimization for reasoning tasks. As a preference-based algorithm, TRPA
naturally eliminates the reward hacking issue. TRPA constructs preference
levels using predefined rules, forms corresponding preference pairs, and
leverages a novel optimization algorithm for RL training with a theoretical
monotonic improvement guarantee. Experimental results demonstrate that TRPA not
only achieves competitive performance on reasoning tasks but also exhibits
robust stability. The code of this paper are released and updating on
https://github.com/XueruiSu/Trust-Region-Preference-Approximation.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and scalable exchange-correlation with deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Luise, Chin-Wei Huang, Thijs Vogels, Derk P. Kooi, Sebastian Ehlert, Stephanie Lanius, Klaas J. H. Giesbertz, Amir Karton, Deniz Gunceler, Megan Stanley, Wessel P. Bruinsma, Lin Huang, Xinran Wei, José Garrido Torres, Abylay Katbashev, Bálint Máté, Sékou-Oumar Kaba, Roberto Sordillo, Yingrong Chen, David B. Williams-Young, Christopher M. Bishop, Jan Hermann, Rianne van den Berg, Paola Gori-Giorgi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Density Functional Theory (DFT) is the most widely used electronic structure
method for predicting the properties of molecules and materials. Although DFT
is, in principle, an exact reformulation of the Schr\"odinger equation,
practical applications rely on approximations to the unknown
exchange-correlation (XC) functional. Most existing XC functionals are
constructed using a limited set of increasingly complex, hand-crafted features
that improve accuracy at the expense of computational efficiency. Yet, no
current approximation achieves the accuracy and generality for predictive
modeling of laboratory experiments at chemical accuracy -- typically defined as
errors below 1 kcal/mol. In this work, we present Skala, a modern deep
learning-based XC functional that bypasses expensive hand-designed features by
learning representations directly from data. Skala achieves chemical accuracy
for atomization energies of small molecules while retaining the computational
efficiency typical of semi-local DFT. This performance is enabled by training
on an unprecedented volume of high-accuracy reference data generated using
computationally intensive wavefunction-based methods. Notably, Skala
systematically improves with additional training data covering diverse
chemistry. By incorporating a modest amount of additional high-accuracy data
tailored to chemistry beyond atomization energies, Skala achieves accuracy
competitive with the best-performing hybrid functionals across general main
group chemistry, at the cost of semi-local DFT. As the training dataset
continues to expand, Skala is poised to further enhance the predictive power of
first-principles simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main: 13 pages plus references, 11 figures and tables. Supplementary
  information: 19 pages, 12 figures and tables. v2 update: fix rendering of
  figure 1 and part of figure 5 in Safari PDF viewer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLARE: Towards Universal <span class="highlight-title">Dataset</span> Purification against Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linshan Hou, Wei Luo, Zhongyun Hua, Songhua Chen, Leo Yu Zhang, Yiming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are susceptible to backdoor attacks, where
adversaries poison datasets with adversary-specified triggers to implant hidden
backdoors, enabling malicious manipulation of model predictions. Dataset
purification serves as a proactive defense by removing malicious training
samples to prevent backdoor injection at its source. We first reveal that the
current advanced purification methods rely on a latent assumption that the
backdoor connections between triggers and target labels in backdoor attacks are
simpler to learn than the benign features. We demonstrate that this assumption,
however, does not always hold, especially in all-to-all (A2A) and untargeted
(UT) attacks. As a result, purification methods that analyze the separation
between the poisoned and benign samples in the input-output space or the final
hidden layer space are less effective. We observe that this separability is not
confined to a single layer but varies across different hidden layers. Motivated
by this understanding, we propose FLARE, a universal purification method to
counter various backdoor attacks. FLARE aggregates abnormal activations from
all hidden layers to construct representations for clustering. To enhance
separation, FLARE develops an adaptive subspace selection algorithm to isolate
the optimal space for dividing an entire dataset into two clusters. FLARE
assesses the stability of each cluster and identifies the cluster with higher
stability as poisoned. Extensive evaluations on benchmark datasets demonstrate
the effectiveness of FLARE against 22 representative backdoor attacks,
including all-to-one (A2O), all-to-all (A2A), and untargeted (UT) attacks, and
its robustness to adaptive attacks. Codes are available at
\href{https://github.com/THUYimingLi/BackdoorBox}{BackdoorBox} and
\href{https://github.com/vtu81/backdoor-toolbox}{backdoor-toolbox}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, This paper is accepted and will appear in TIFS (CCF-A)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influential Bandits: Pulling an Arm May Change the Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.08200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.08200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato, Shinji Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While classical formulations of multi-armed bandit problems assume that each
arm's reward is independent and stationary, real-world applications often
involve non-stationary environments and interdependencies between arms. In
particular, selecting one arm may influence the future rewards of other arms, a
scenario not adequately captured by existing models such as rotting bandits or
restless bandits. To address this limitation, we propose the influential bandit
problem, which models inter-arm interactions through an unknown, symmetric,
positive semi-definite interaction matrix that governs the dynamics of arm
losses. We formally define this problem and establish two regret lower bounds,
including a superlinear $\Omega(T^2 / \log^2 T)$ bound for the standard LCB
algorithm (loss minimization version of UCB) and an algorithm-independent
$\Omega(T)$ bound, which highlight the inherent difficulty of the setting. We
then introduce a new algorithm based on a lower confidence bound (LCB)
estimator tailored to the structure of the loss dynamics. Under mild
assumptions, our algorithm achieves a regret of $O(KT \log T)$, which is nearly
optimal in terms of its dependence on the time horizon. The algorithm is simple
to implement and computationally efficient. Empirical evaluations on both
synthetic and real-world datasets demonstrate the presence of inter-arm
influence and confirm the superior performance of our method compared to
conventional bandit algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Data-Driven to Purpose-Driven Artificial Intelligence: Systems
  Thinking for Data-Analytic Automation of Patient Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anadria, Roel Dobbe, Anastasia Giachanou, Ruurd Kuiper, Richard Bartels, Wouter van Amsterdam, Íñigo Martínez de Rituerto de Troya, Carmen Zürcher, Daniel Oberski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we reflect on the data-driven modeling paradigm that is gaining
ground in AI-driven automation of patient care. We argue that the repurposing
of existing real-world patient datasets for machine learning may not always
represent an optimal approach to model development as it could lead to
undesirable outcomes in patient care. We reflect on the history of data
analysis to explain how the data-driven paradigm rose to popularity, and we
envision ways in which systems thinking and clinical domain theory could
complement the existing model development approaches in reaching human-centric
outcomes. We call for a purpose-driven machine learning paradigm that is
grounded in clinical theory and the sociotechnical realities of real-world
operational contexts. We argue that understanding the utility of existing
patient datasets requires looking in two directions: upstream towards the data
generation, and downstream towards the automation objectives. This
purpose-driven perspective to AI system development opens up new methodological
opportunities and holds promise for AI automation of patient care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work is under review at ACM Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-Agent vs. Multi-Agent LLM Strategies for Automated Student
  Reflection Assessment <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gen Li, Li Chen, Cheng Tang, Valdemar Švábenský, Daisuke Deguchi, Takayoshi Yamashita, Atsushi Shimada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the use of Large Language Models (LLMs) for automated assessment
of open-text student reflections and prediction of academic performance.
Traditional methods for evaluating reflections are time-consuming and may not
scale effectively in educational settings. In this work, we employ LLMs to
transform student reflections into quantitative scores using two assessment
strategies (single-agent and multi-agent) and two prompting techniques
(zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278
reflections from 377 students over three academic terms, demonstrate that the
single-agent with few-shot strategy achieves the highest match rate with human
evaluations. Furthermore, models utilizing LLM-assessed reflection scores
outperform baselines in both at-risk student identification and grade
prediction tasks. These findings suggest that LLMs can effectively automate
reflection assessment, reduce educators' workload, and enable timely support
for students who may need additional assistance. Our work emphasizes the
potential of integrating advanced generative AI technologies into educational
practices to enhance student engagement and academic success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 29th Pacific-Asia Conference on
  Knowledge Discovery and Data Mining (PAKDD 2025), see
  https://doi.org/10.1007/978-981-96-8186-0_24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing oncology with federated learning: transcending boundaries in
  breast, lung, and prostate cancer. A systematic <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshu Ankolekar, Sebastian Boie, Maryam Abdollahyan, Emanuela Gadaleta, Seyed Alireza Hasheminasab, Guang Yang, Charles Beauville, Nikolaos Dikaios, George Anthony Kastis, Michael Bussmann, Sara Khalid, Hagen Kruger, Philippe Lambin, Giorgos Papanastasiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a promising solution to address the
limitations of centralised machine learning (ML) in oncology, particularly in
overcoming privacy concerns and harnessing the power of diverse, multi-center
data. This systematic review synthesises current knowledge on the
state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer.
Distinct from previous surveys, our comprehensive review critically evaluates
the real-world implementation and impact of FL on cancer care, demonstrating
its effectiveness in enhancing ML generalisability, performance and data
privacy in clinical settings and data. We evaluated state-of-the-art advances
in FL, demonstrating its growing adoption amid tightening data privacy
regulations. FL outperformed centralised ML in 15 out of the 25 studies
reviewed, spanning diverse ML models and clinical applications, and
facilitating integration of multi-modal information for precision medicine.
Despite the current challenges identified in reproducibility, standardisation
and methodology across studies, the demonstrable benefits of FL in harnessing
real-world data and addressing clinical needs highlight its significant
potential for advancing cancer research. We propose that future research should
focus on addressing these limitations and investigating further advanced FL
methods, to fully harness data diversity and realise the transformative power
of cutting-edge FL in cancer care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 Figures, 3 Tables, 1 Supplementary Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Diffusion Bridges with Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Heng, Valentin De Bortoli, Arnaud Doucet, James Thornton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of simulating diffusion bridges, which are diffusion
processes that are conditioned to initialize and terminate at two given states.
The simulation of diffusion bridges has applications in diverse scientific
fields and plays a crucial role in the statistical inference of
discretely-observed diffusions. This is known to be a challenging problem that
has received much attention in the last two decades. This article contributes
to this rich body of literature by presenting a new avenue to obtain diffusion
bridge approximations. Our approach is based on a backward time representation
of a diffusion bridge, which may be simulated if one can time-reverse the
unconditioned diffusion. We introduce a variational formulation to learn this
time-reversal with function approximation and rely on a score matching method
to circumvent intractability. Another iteration of our proposed methodology
approximates the Doob's $h$-transform defining the forward time representation
of a diffusion bridge. We discuss algorithmic considerations and extensions,
and present numerical results on an Ornstein--Uhlenbeck process, a model from
financial econometrics for interest rates, and a model from genetics for cell
differentiation and development to illustrate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Map Space Belief Prediction for Manipulation-Enhanced Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20606v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20606v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Marcos Correia Marques, Nils Dengler, Tobias Zaenker, Jesper Mucke, Shenlong Wang, Maren Bennewitz, Kris Hauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for objects in cluttered environments requires selecting efficient
viewpoints and manipulation actions to remove occlusions and reduce uncertainty
in object locations, shapes, and categories. In this work, we address the
problem of manipulation-enhanced semantic mapping, where a robot has to
efficiently identify all objects in a cluttered shelf. Although Partially
Observable Markov Decision Processes~(POMDPs) are standard for decision-making
under uncertainty, representing unstructured interactive worlds remains
challenging in this formalism. To tackle this, we define a POMDP whose belief
is summarized by a metric-semantic grid map and propose a novel framework that
uses neural networks to perform map-space belief updates to reason efficiently
and simultaneously about object geometries, locations, categories, occlusions,
and manipulation physics. Further, to enable accurate information gain
analysis, the learned belief updates should maintain calibrated estimates of
uncertainty. Therefore, we propose Calibrated Neural-Accelerated Belief Updates
(CNABUs) to learn a belief propagation model that generalizes to novel
scenarios and provides confidence-calibrated predictions for unknown areas. Our
experiments show that our novel POMDP planner improves map completeness and
accuracy over existing methods in challenging simulations and successfully
transfers to real-world cluttered shelves in zero-shot fashion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures; Published at RSS 2025 - this version contains a
  small fix to figure 6 which was missing a plot in the original submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second-Order Majorant Algorithm for Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17992v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17992v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai-Quyen Pham, Jérémy Cohen, Thierry Chonavel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative Matrix Factorization (NMF) is a fundamental tool in unsupervised
learning, widely used for tasks such as dimensionality reduction, feature
extraction, representation learning, and topic modeling. Many algorithms have
been developed for NMF, including the well-known Multiplicative Updates (MU)
algorithm, which belongs to a broader class of majorization-minimization
techniques. In this work, we introduce a general second-order optimization
framework for NMF under both quadratic and $\beta$-divergence loss functions.
This approach, called Second-Order Majorant (SOM), constructs a local quadratic
majorization of the loss function by majorizing its Hessian matrix. It includes
MU as a special case, while enabling faster variants. In particular, we propose
mSOM, a new algorithm within this class that leverages a tighter local
approximation to accelerate convergence. We provide a convergence analysis,
showing linear convergence for individual factor updates and global convergence
to a stationary point for the alternating version, AmSOM algorithm. Numerical
experiments on both synthetic and real data sets demonstrate that mSOM
consistently outperforms state-of-the-art algorithms across multiple loss
functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version in JMLR style. This version matches the manuscript
  currently under review at JMLR and includes substantial improvements over the
  original arXiv version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness
  Results and Improved Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00712v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00712v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lesi Chen, Jing Xu, Jingzhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization reveals the inner structure of otherwise oblique
optimization problems, such as hyperparameter tuning, neural architecture
search, and meta-learning. A common goal in bilevel optimization is to minimize
a hyper-objective that implicitly depends on the solution set of the
lower-level function. Although this hyper-objective approach is widely used,
its theoretical properties have not been thoroughly investigated in cases where
the lower-level functions lack strong convexity. In this work, we first provide
hardness results to show that the goal of finding stationary points of the
hyper-objective for nonconvex-convex bilevel optimization can be intractable
for zero-respecting algorithms. Then we study a class of tractable
nonconvex-nonconvex bilevel problems when the lower-level function satisfies
the Polyak-{\L}ojasiewicz (PL) condition. We show a simple first-order
algorithm can achieve better complexity bounds of
$\tilde{\mathcal{O}}(\epsilon^{-2})$, $\tilde{\mathcal{O}}(\epsilon^{-4})$ and
$\tilde{\mathcal{O}}(\epsilon^{-6})$ in the deterministic, partially
stochastic, and fully stochastic setting respectively. The complexities in the
first two cases are optimal up to logarithmic factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLT 2024. This arXiv version refines Assumption 4.1
  (d); adds discussions on related works in Appendix A; and corrects the kappa
  dependency in the upper bounds</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Aware Virtual Training: Enhancing Generalization in
  Meta-Reinforcement Learning for Out-of-Distribution Tasks <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02834v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02834v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongmo Kim, Yisak Park, Minung Kim, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning aims to develop policies that generalize to
unseen tasks sampled from a task distribution. While context-based meta-RL
methods improve task representation using task latents, they often struggle
with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware
Virtual Training (TAVT), a novel algorithm that accurately captures task
characteristics for both training and OOD scenarios using metric-based
representation learning. Our method successfully preserves task characteristics
in virtual tasks and employs a state regularization technique to mitigate
overestimation errors in state-varying environments. Numerical results
demonstrate that TAVT significantly enhances generalization to OOD tasks across
various MuJoCo and MetaWorld environments. Our code is available at
https://github.com/JM-Kim-94/tavt.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages main paper, 20 pages appendices with reference. Accepted to
  ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uniform Mean Estimation for Heavy-Tailed Distributions via
  Median-of-Means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikael Møller Høgsgaard, Andrea Paudice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Median of Means (MoM) is a mean estimator that has gained popularity in
the context of heavy-tailed data. In this work, we analyze its performance in
the task of simultaneously estimating the mean of each function in a class
$\mathcal{F}$ when the data distribution possesses only the first $p$ moments
for $p \in (1,2]$. We prove a new sample complexity bound using a novel
symmetrization technique that may be of independent interest. Additionally, we
present applications of our result to $k$-means clustering with unbounded
inputs and linear regression with general losses, improving upon existing
works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement
  Learning <span class="chip">ICML
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02844v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02844v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunwoo Lee, Jaebak Hwang, Yonghyeon Jo, Seungyul Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional robust methods in multi-agent reinforcement learning (MARL) often
struggle against coordinated adversarial attacks in cooperative scenarios. To
address this limitation, we propose the Wolfpack Adversarial Attack framework,
inspired by wolf hunting strategies, which targets an initial agent and its
assisting agents to disrupt cooperation. Additionally, we introduce the
Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust
MARL policies to defend against the proposed Wolfpack attack by fostering
systemwide collaboration. Experimental results underscore the devastating
impact of the Wolfpack attack and the significant robustness improvements
achieved by WALL. Our code is available at
https://github.com/sunwoolee0504/WALL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages main, 23 pages appendix with reference. Accepeted by ICML
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Nonlinear PDEs with Sparse Radial Basis Function Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Shao, Konstantin Pieper, Xiaochuan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework for solving nonlinear PDEs using sparse radial
basis function (RBF) networks. Sparsity-promoting regularization is employed to
prevent over-parameterization and reduce redundant features. This work is
motivated by longstanding challenges in traditional RBF collocation methods,
along with the limitations of physics-informed neural networks (PINNs) and
Gaussian process (GP) approaches, aiming to blend their respective strengths in
a unified framework. The theoretical foundation of our approach lies in the
function space of Reproducing Kernel Banach Spaces (RKBS) induced by
one-hidden-layer neural networks of possibly infinite width. We prove a
representer theorem showing that the sparse optimization problem in the RKBS
admits a finite solution and establishes error bounds that offer a foundation
for generalizing classical numerical analysis. The algorithmic framework is
based on a three-phase algorithm to maintain computational efficiency through
adaptive feature selection, second-order optimization, and pruning of inactive
neurons. Numerical experiments demonstrate the effectiveness of our method and
highlight cases where it offers notable advantages over GP approaches. This
work opens new directions for adaptive PDE solvers grounded in rigorous
analysis with efficient, learning-inspired implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic neuron approach to deep neural networks: Decoupling neurons for
  renormalization group analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghee Lee, Hye-Sung Lee, Jaeok Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network architectures often consist of repetitive structural
elements. We introduce an approach that reveals these patterns and can be
broadly applied to the study of deep learning. Similarly to how a power strip
helps untangle and organize complex cable connections, this approach treats
neurons as additional degrees of freedom in interactions, simplifying the
structure and enhancing the intuitive understanding of interactions within deep
neural networks. Furthermore, it reveals the translational symmetry of deep
neural networks, which simplifies the application of the renormalization group
transformation-a method that effectively analyzes the scaling behavior of the
system. By utilizing translational symmetry and renormalization group
transformations, we can analyze critical phenomena. This approach may open new
avenues for studying deep neural networks using statistical physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version matching the publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLäMmlein: Transparent, Compact and Competitive German-Only Language
  Models from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11171v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11171v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Pfister, Julia Wunderle, Andreas Hotho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We create two German-only decoder models, LL\"aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\"aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera ready @ACL25;
  https://www.informatik.uni-wuerzburg.de/datascience/projects/nlp/llammlein/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-17T00:00:00Z">2025-06-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HARMONY: A Scalable Distributed Vector Database for High-Throughput
  Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Xu, Feng Zhang, Chengxi Li, Lei Cao, Zheng Chen, Jidong Zhai, Xiaoyong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) is essential for various
data-intensive applications, including recommendation systems, image retrieval,
and machine learning. Scaling ANNS to handle billions of high-dimensional
vectors on a single machine presents significant challenges in memory capacity
and processing efficiency. To address these challenges, distributed vector
databases leverage multiple nodes for the parallel storage and processing of
vectors. However, existing solutions often suffer from load imbalance and high
communication overhead, primarily due to traditional partition strategies that
fail to effectively distribute the workload. In this paper, we introduce
Harmony, a distributed ANNS system that employs a novel multi-granularity
partition strategy, combining dimension-based and vector-based partition. This
strategy ensures a balanced distribution of computational load across all nodes
while effectively minimizing communication costs. Furthermore, Harmony
incorporates an early-stop pruning mechanism that leverages the monotonicity of
distance computations in dimension-based partition, resulting in significant
reductions in both computational and communication overhead. We conducted
extensive experiments on diverse real-world datasets, demonstrating that
Harmony outperforms leading distributed vector databases, achieving 4.63 times
throughput on average in four nodes and 58% performance improvement over
traditional distribution for skewed workloads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keigo: Co-designing Log-Structured Merge Key-Value Stores with a
  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version) <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Adão, Zhongjie Wu, Changjun Zhou, Oana Balmau, João Paulo, Ricardo Macedo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Keigo, a concurrency- and workload-aware storage middleware that
enhances the performance of log-structured merge key-value stores (LSM KVS)
when they are deployed on a hierarchy of storage devices. The key observation
behind Keigo is that there is no one-size-fits-all placement of data across the
storage hierarchy that optimizes for all workloads. Hence, to leverage the
benefits of combining different storage devices, Keigo places files across
different devices based on their parallelism, I/O bandwidth, and capacity. We
introduce three techniques - concurrency-aware data placement, persistent
read-only caching, and context-based I/O differentiation. Keigo is portable
across different LSMs, is adaptable to dynamic workloads, and does not require
extensive profiling. Our system enables established production KVS such as
RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We
evaluate Keigo using synthetic and realistic workloads, showing that it
improves the throughput of production-grade LSMs up to 4x for write- and 18x
for read-heavy workloads when compared to general-purpose storage systems and
specialized LSM KVS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the full paper to appear in VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PHast -- Perfect Hashing with fast evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.17918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.17918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Beling, Peter Sanders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perfect hash functions give unique "names" to arbitrary keys requiring only a
few bits per key. This is an essential building block in applications like
static hash tables, databases, or bioinformatics. This paper introduces the
PHast approach that has the currently fastest query time with competitive
construction time and space consumption. PHast improves bucket-placement which
first hashes each key k to a bucket, and then looks for the bucket seed s such
that a secondary hash function maps pairs (s,k) in a collision-free way. PHast
can use small-range primary hash functions with linear mapping, fixed-width
encoding of seeds, and parallel construction. This is achieved using small
overlapping slices of allowed values and bumping to handle unsuccessful seed
assignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abacus: A Cost-Based Optimizer for Semantic Operator Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.14661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.14661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Russo, Sivaprasad Sudhir, Gerardo Vitagliano, Chunwei Liu, Tim Kraska, Samuel Madden, Michael Cafarella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Liang, Pengcheng Zhang, Bin Yao, Zhongpu Chen, Yitong Song, Guangxu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient and scalable framework for Range Filtered
Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors
associated with attribute values. Given a query vector $q$ and a range $[l,
h]$, RF-ANNS aims to find the approximate $k$ nearest neighbors of $q$ among
data whose attribute values fall within $[l, h]$. Existing methods including
pre-, post-, and hybrid filtering strategies that perform attribute range
filtering before, after, or during the ANNS process, all suffer from
significant performance degradation when query ranges shift. Though building
dedicated indexes for each strategy and selecting the best one based on the
query range can address this problem, it leads to index consistency and
maintenance issues.
  Our framework, called UNIFY, constructs a unified Proximity Graph-based
(PG-based) index that seamlessly supports all three strategies. In UNIFY, we
introduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by
attribute values. It ensures the PG of objects from any segment combinations is
a sub-graph of SIG, thereby enabling efficient hybrid filtering by
reconstructing and searching a PG from relevant segments. Moreover, we present
Hierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which
incorporates a hierarchical structure inspired by HNSW to achieve logarithmic
hybrid filtering complexity. We also implement pre- and post-filtering for HSIG
by fusing skip list connections and compressed HNSW edges into the hierarchical
graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS
performance across small, mid, and large query ranges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Magneto: Combining Small and Large Language Models for Schema Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurong Liu, Eduardo Pena, Aecio Santos, Eden Wu, Juliana Freire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in language models opened new opportunities to address
complex schema matching tasks. Schema matching approaches have been proposed
that demonstrate the usefulness of language models, but they have also
uncovered important limitations: Small language models (SLMs) require training
data (which can be both expensive and challenging to obtain), and large
language models (LLMs) often incur high computational costs and must deal with
constraints imposed by context windows. We present Magneto, a cost-effective
and accurate solution for schema matching that combines the advantages of SLMs
and LLMs to address their limitations. By structuring the schema matching
pipeline in two phases, retrieval and reranking, Magneto can use
computationally efficient SLM-based strategies to derive candidate matches
which can then be reranked by LLMs, thus making it possible to reduce runtime
without compromising matching accuracy. We propose a self-supervised approach
to fine-tune SLMs which uses LLMs to generate syntactically diverse training
data, and prompting strategies that are effective for reranking. We also
introduce a new benchmark, developed in collaboration with domain experts,
which includes real biomedical datasets and presents new challenges to schema
matching methods. Through a detailed experimental evaluation, using both our
new and existing benchmarks, we show that Magneto is scalable and attains high
accuracy for datasets from different domains.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Intelligence: Designing Data Centers for Next-Gen Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.15006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.15006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8
trillion parameters - demands a radical rethinking of data center architecture
to ensure scalability, efficiency, and cost-effectiveness. Our work provides a
comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth
and capacity, multiple network topologies (two-tier vs. FullFlat optical), the
size of the scale-out domain, and popular parallelism/optimization strategies
used in LLMs. We introduce and evaluate FullFlat network architectures, which
provide uniform high-bandwidth, low-latency connectivity between all nodes, and
demonstrate their transformative impact on performance and scalability. Through
detailed sensitivity analyses, we quantify the benefits of overlapping compute
and communication, leveraging hardware-accelerated collectives, wider scale-out
domains, and larger memory capacity. Our study spans both sparse (mixture of
experts) and dense transformer-based LLMs, revealing how system design choices
affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens
per sec / Peak flops of the hardware) and overall throughput. For the co-design
study, we extended and validated a performance modeling tool capable of
predicting LLM runtime within 10% of real-world measurements. Our findings
offer actionable insights and a practical roadmap for designing AI data centers
that can efficiently support trillion-parameter models, reduce optimization
complexity, and sustain the rapid evolution of AI capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, submitted to SC25 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailiang Zhang, Dieu My T. Nguyen, Christine Smit, Mahabal Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data analysis on massive multi-dimensional data, such as high-resolution
large-region time averaging or area averaging for geospatial data, often
involves calculations over a significant number of data points. While
performing calculations in scalable and flexible distributed or cloud
environments is a viable option, a full scan of large data volumes still serves
as a computationally intensive bottleneck, leading to significant cost. This
paper introduces a generic and comprehensive method to address these
computational challenges. This method generates a small, size-tunable
supplementary dataset that stores the cumulative sums along specific subset
dimensions on top of the raw data. This minor addition unlocks rapid and cheap
high-resolution large-region data analysis, making calculations over large
numbers of data points feasible with small instances or even microservices in
the cloud. This method is general-purpose, but is particularly well-suited for
data stored in chunked, cloud-optimized formats and for services running in
distributed or cloud environments. We present a Zarr extension proposal to
integrate the specifications of this method and facilitate its straightforward
implementation in general-purpose software applications. Benchmark tests
demonstrate that this method, implemented in Amazon Web services (AWS),
significantly outperforms the brute-force approach used in on-premises
services. With just 5% supplemental storage, this method achieves a performance
that is 3-4 orders of magnitude (~10,000 times) faster than the brute-force
approach, while incurring significantly reduced computational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-Driven Online Vertical Federated Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganyu Wang, Boyu Wang, Bin Gu, Charles Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource Optimization with MPI Process Malleability for Dynamic
  Workloads in HPC Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Iserte, Iker Martín-Álvarez, Krzysztof Rojek, José I. Aliaga, Maribel Castillo, Weronika Folwarska, Antonio J. Peña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic resource management is essential for optimizing computational
efficiency in modern high-performance computing (HPC) environments,
particularly as systems scale. While research has demonstrated the benefits of
malleability in resource management systems (RMS), the adoption of such
techniques in production environments remains limited due to challenges in
standardization, interoperability, and usability. Addressing these gaps, this
paper extends our prior work on the Dynamic Management of Resources (DMR)
framework, which provides a modular and user-friendly approach to dynamic
resource allocation. Building upon the original DMRlib reconfiguration runtime,
this work integrates new methodology from the Malleability Module (MaM) of the
Proteo framework, further enhancing reconfiguration capabilities with new
spawning strategies and data redistribution methods. In this paper, we explore
new malleability strategies in HPC dynamic workloads, such as merging MPI
communicators and asynchronous reconfigurations, which offer new opportunities
for dramatically reducing memory overhead. The proposed enhancements are
rigorously evaluated on a world-class supercomputer, demonstrating improved
resource utilization and workload efficiency. Results show that dynamic
resource management can reduce the workload completion time by 40% and increase
the resource utilization by over 20%, compared to static resource allocation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SETI@home: Data Acquisition and Front-End Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric J. Korpela, David P. Anderson, Jeff Cobb, Matt Lebofsky, Wei Liu, Dan Werthimer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project,
looking for technosignatures in data recorded at multiple observatories from
1998 to 2020. Most radio SETI projects analyze data using dedicated processing
hardware. SETI@home uses a different approach: time-domain data is distributed
over the Internet to $\gt 10^{5}$ volunteered home computers, which analyze it.
The large amount of computing power this affords ($\sim 10^{15}$ floating-point
operations per second (FPOP/s)) allows us to increase the sensitivity and
generality of our search in three ways. We use coherent integration, a
technique in which data is transformed so that the power of drifting signals is
confined to a single discrete Fourier transform (DFT) bin. We perform this
coherent search over 123 000 Doppler drift rates in the range ($\pm$100 Hz
s$^{-1}$). Second, we search for a variety of signal types, such as pulsed
signals and arbitrary repeated waveforms. The analysis uses a range of DFT
sizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front
end of SETI@home produces a set of detections that exceed thresholds in power
and goodness of fit. We accumulated $\sim 1.2\times 10^{10}$ such detections.
The back end of SETI@home takes these detections, identifies and removes radio
frequency interference (RFI), and looks for groups of detections that are
consistent with extraterrestrial origin and that persist over long timescales.
This paper describes the front end of SETI@home and provides parameters for the
primary data source, the Arecibo Observatory; the back end and its results are
described in a companion paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures, 5 tables. Accepted to AJ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keigo: Co-designing Log-Structured Merge Key-Value Stores with a
  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version) <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Adão, Zhongjie Wu, Changjun Zhou, Oana Balmau, João Paulo, Ricardo Macedo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Keigo, a concurrency- and workload-aware storage middleware that
enhances the performance of log-structured merge key-value stores (LSM KVS)
when they are deployed on a hierarchy of storage devices. The key observation
behind Keigo is that there is no one-size-fits-all placement of data across the
storage hierarchy that optimizes for all workloads. Hence, to leverage the
benefits of combining different storage devices, Keigo places files across
different devices based on their parallelism, I/O bandwidth, and capacity. We
introduce three techniques - concurrency-aware data placement, persistent
read-only caching, and context-based I/O differentiation. Keigo is portable
across different LSMs, is adaptable to dynamic workloads, and does not require
extensive profiling. Our system enables established production KVS such as
RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We
evaluate Keigo using synthetic and realistic workloads, showing that it
improves the throughput of production-grade LSMs up to 4x for write- and 18x
for read-heavy workloads when compared to general-purpose storage systems and
specialized LSM KVS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extended version of the full paper to appear in VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concepts for designing modern C++ interfaces for MPI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. Nicole Avans, Alfredo A. Correa, Sayan Ghosh, Matthias Schimek, Joseph Schuchart, Anthony Skjellum, Evan D. Suggs, Tim Niklas Uhl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the C++ bindings were deleted in 2008, the Message Passing Interface
(MPI) community has revived efforts in building high-level modern C++
interfaces. Such interfaces are either built to serve specific scientific
application needs (with limited coverage to the underlying MPI
functionalities), or as an exercise in general-purpose programming model
building, with the hope that bespoke interfaces can be broadly adopted to
construct a variety of distributed-memory scientific applications. However,
with the advent of modern C++-based heterogeneous programming models, GPUs and
widespread Machine Learning (ML) usage in contemporary scientific computing,
the role of prospective community-standardized high-level C++ interfaces to MPI
is evolving. The success of such an interface clearly will depend on providing
robust abstractions and features adhering to the generic programming principles
that underpin the C++ programming language, without compromising on either
performance and portability, the core principles upon which MPI was founded.
However, there is a tension between idiomatic C++ handling of types and
lifetimes, and, MPI's loose interpretation of object lifetimes/ownership and
insistence on maintaining global states.
  Instead of proposing "yet another" high-level C++ interface to MPI,
overlooking or providing partial solutions to work around the key issues
concerning the dissonance between MPI semantics and idiomatic C++, this paper
focuses on the three fundamental aspects of a high-level interface: type
system, object lifetimes and communication buffers, also identifying
inconsistencies in the MPI specification. Presumptive solutions can be
unrefined, and we hope the broader MPI and C++ communities will engage with us
in productive exchange of ideas and concerns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consensus Power Inequality: A Comparative Study of Blockchain Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Tylinski, Abylay Satybaldy, Paolo Tasca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution of consensus power is a cornerstone of decentralisation,
influencing the security, resilience, and fairness of blockchain networks while
ensuring equitable impact among participants. This study provides a rigorous
evaluation of consensus power inequality across five prominent blockchain
networks - Bitcoin, Ethereum, Cardano, Hedera, and Algorand - using data
collected from January 2022 to July 2024. Leveraging established economic
metrics, including the Gini coefficient and Theil index, the research
quantitatively assesses how power is distributed among blockchain network
participants. A robust dataset, capturing network-specific characteristics such
as mining pools, staking patterns, and consensus nodes, forms the foundation of
the analysis, enabling meaningful comparisons across diverse architectures.
Through an in-depth comparative study, the paper identifies key disparities in
consensus power distribution. Hedera and Bitcoin demonstrate more balanced
power distribution, aligning closely with the principles of decentralisation.
Ethereum and Cardano demonstrate moderate levels of inequality. However,
contrary to expectations, Ethereum has become more concentrated following its
transition to Proof-of-Stake. Meanwhile, Algorand shows a pronounced
centralisation of power. Moreover, the findings highlight the structural and
operational drivers of inequality, including economic barriers, governance
models, and network effects, offering actionable insights for more equitable
network design. This study establishes a methodological framework for
evaluating blockchain consensus power inequality, emphasising the importance of
targeted strategies to ensure fairer power distribution and enhancing the
sustainability of decentralised systems. Future research will build on these
findings by integrating additional metrics and examining the influence of
emerging consensus mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence-Privacy-Fairness Trade-Off in Personalized Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Zhao, Qimei Cui, Weicai Li, Wei Ni, Ekram Hossain, Quan Z. Sheng, Xiaofeng Tao, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a
balance between personalization and generalization by conducting federated
learning (FL) to guide personalized learning (PL). While FL is unaffected by
personalized model training, in Ditto, PL depends on the outcome of the FL.
However, the clients' concern about their privacy and consequent perturbation
of their local models can affect the convergence and (performance) fairness of
PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension
of Ditto under the protection of differential privacy (DP), and analyzes the
trade-off among its privacy guarantee, model convergence, and performance
distribution fairness. We also analyze the convergence upper bound of the
personalized models under DP-Ditto and derive the optimal number of global
aggregations given a privacy budget. Further, we analyze the performance
fairness of the personalized models, and reveal the feasibility of optimizing
DP-Ditto jointly for convergence and fairness. Experiments validate our
analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of
the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by
over 32.71% in fairness and 9.66% in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Indicator for Quantifying and Minimizing Information Utility
  Loss of Robot Teams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyu Zhao, Qimei Cui, Wei Ni, Quan Z. Sheng, Abbas Jamalipour, Guoshun Nan, Xiaofeng Tao, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The timely exchange of information among robots within a team is vital, but
it can be constrained by limited wireless capacity. The inability to deliver
information promptly can result in estimation errors that impact collaborative
efforts among robots. In this paper, we propose a new metric termed Loss of
Information Utility (LoIU) to quantify the freshness and utility of information
critical for cooperation. The metric enables robots to prioritize information
transmissions within bandwidth constraints. We also propose the estimation of
LoIU using belief distributions and accordingly optimize both transmission
schedule and resource allocation strategy for device-to-device transmissions to
minimize the time-average LoIU within a robot team. A semi-decentralized
Multi-Agent Deep Deterministic Policy Gradient framework is developed, where
each robot functions as an actor responsible for scheduling transmissions among
its collaborators while a central critic periodically evaluates and refines the
actors in response to mobility and interference. Simulations validate the
effectiveness of our approach, demonstrating an enhancement of information
freshness and utility by 98%, compared to alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Redundancy of Full Nodes in Bitcoin: A Network-Theoretic
  Demonstration of Miner-Centric Propagation Topologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dr Craig S Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper formally examines the network structure of Bitcoin CORE (BTC) and
Bitcoin Satoshi Vision (BSV) using complex graph theory to demonstrate that
home-hosted full nodes are incapable of participating in or influencing the
propagation topology. Leveraging established models such as scale-free networks
and small-world connectivity, we demonstrate that the propagation graph is
dominated by a densely interconnected miner clique, while full nodes reside on
the periphery, excluded from all transaction-to-block inclusion paths. Using
simulation-backed metrics and eigenvalue centrality analysis, we confirm that
full nodes are neither critical nor operationally relevant for consensus
propagation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 1 figures. Comprehensive technical treatment of Bitcoin
  propagation topology. Submitted to arXiv for public dissemination and
  archival reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizheng Zhang, Michael Wornow, Kunle Olukotun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based agentic applications have shown increasingly remarkable
capabilities in complex workflows but incur substantial costs due to extensive
planning and reasoning requirements. Existing LLM caching techniques (like
context caching and semantic caching), primarily designed for serving chatbots,
are insufficient for agentic applications where outputs depend on external data
or environmental contexts. We propose agentic plan caching, a novel approach
that extracts, stores, adapts, and reuses structured plan templates from
planning stages of agentic applications across semantically similar tasks to
reduce the cost of serving. Unlike traditional semantic caching, our system
extracts plan templates from completed agent executions at test-time, employs
keyword extraction to match new requests against cached plans, and utilizes
lightweight models to adapt these templates to task-specific plans with
contexts. Evaluation across multiple real-world agentic applications shows that
our system can reduce costs by 46.62% on average while maintaining performance,
offering a more efficient solution for serving LLM-based agents that
complements existing LLM serving infrastructures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Serving of LLM Applications with Probabilistic Demand Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Liu, Zuo Gan, Zhenghao Gan, Weiye Wang, Chen Chen, Yizhou Shan, Xusheng Chen, Zhenhua Han, Yifei Zhu, Shixuan Sun, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applications based on Large Language Models (LLMs) contains a series of tasks
to address real-world problems with boosted capability, which have dynamic
demand volumes on diverse backends. Existing serving systems treat the resource
demands of LLM applications as a blackbox, compromising end-to-end efficiency
due to improper queuing order and backend warm up latency. We find that the
resource demands of LLM applications can be modeled in a general and accurate
manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which
leverages PDGraph for efficient serving of LLM applications. Confronting
probabilistic demand description, Hermes applies the Gittins policy to
determine the scheduling order that can minimize the average application
completion time. It also uses the PDGraph model to help prewarm cold backends
at proper moments. Experiments with diverse LLM applications confirm that
Hermes can effectively improve the application serving efficiency, reducing the
average completion time by over 70% and the P95 completion time by over 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Déjà Vu: Efficient Video-Language Query Engine with Learning-based
  Inter-Frame Computation Reuse <span class="chip">VLDB</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwoo Hwang, Daeun Kim, Sangyeop Lee, Yoonsung Kim, Guseul Heo, Hojoon Kim, Yunseok Jeong, Tadiwos Meaza, Eunhyeok Park, Jeongseob Ahn, Jongse Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Video-Language Models (VideoLMs) have demonstrated remarkable
capabilities, offering significant potential for flexible and powerful video
query systems. These models typically rely on Vision Transformers (ViTs), which
process video frames individually to extract visual embeddings. However,
generating embeddings for large-scale videos requires ViT inferencing across
numerous frames, posing a major hurdle to real-world deployment and
necessitating solutions for integration into scalable video data management
systems. This paper introduces D\'ej\`a Vu, a video-language query engine that
accelerates ViT-based VideoLMs by reusing computations across consecutive
frames. At its core is ReuseViT, a modified ViT model specifically designed for
VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking
an effective balance between accuracy and reuse. Although ReuseViT
significantly reduces computation, these savings do not directly translate into
performance gains on GPUs. To overcome this, D\'ej\`a Vu integrates
memory-compute joint compaction techniques that convert the FLOP savings into
tangible performance gains. Evaluations on three VideoLM tasks show that
D\'ej\`a Vu accelerates embedding generation by up to a 2.64x within a 2% error
bound, dramatically enhancing the practicality of VideoLMs for large-scale
video analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 VLDB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supporting the development of Machine Learning for fundamental science
  in a federated Cloud with the AI_INFN platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucio Anderlini, Matteo Barbetti, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Carmelo Pellegrino, Rosa Petrini, Alessandro Pascolini, Daniele Spiga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
("Artificial Intelligence at INFN") aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provision of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous, distributed computing resources, possibly
federated as Virtual Kubelets with the interLink provider.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in EPJ Web of Conferences (CHEP 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Dynamic Load Balancing Algorithms for Block-Structured
  Mesh-and-Particle Simulations in AMReX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.15122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.15122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitash Nanda, Md Kamal Hossain Chowdhury, Hannah Ross, Kevin Gott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Load balancing is critical for successful large-scale high-performance
computing (HPC) simulations. With modern supercomputers increasing in
complexity and variability, dynamic load balancing is becoming more critical to
use computational resources efficiently. In this study, performed during a
summer collaboration at Lawrence Berkeley National Laboratory, we investigate
various standard dynamic load-balancing algorithms. This includes the time
evaluation of a brute-force solve for application in algorithmic evaluation, as
well as quality and time evaluations of the Knapsack algorithm, an SFC
algorithm, and two novel algorithms: a painter's partition-based SFC algorithm
and a combination Knapsack+SFC methodology-based on hardware topology. The
results suggest Knapsack and painter's partition-based algorithms should be
among the first algorithms evaluated by HPC codes for cases with limited weight
deviation and will perform at least slightly better than AMReX's
percentage-tracking partitioning strategy across most simulations, although
effects diminish as weight variety increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, Accepted in the ACM Practice and Experience in
  Advanced Research Computing (PEARC) Conference Series 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupling Generation and Evaluation for Parallel Greedy Best-First
  Search(extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumi Shimoda, Alex Fukunaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to understand and control the search behavior of parallel search,
recent work has proposed a class of constrained parallel greedy best-first
search algorithms which only expands states that satisfy some
constraint.However, enforcing such constraints can be costly, as threads must
be waiting idly until a state that satisfies the expansion constraint is
available. We propose an improvement to constrained parallel search which
decouples state generation and state evaluation and significantly improves
state evaluation rate, resulting in better search performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of SoCS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HarMoEny: Efficient Multi-GPU Inference of MoE Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Doucet, Rishi Sharma, Martijn de Vos, Rafael Pires, Anne-Marie Kermarrec, Oana Balmau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) models offer computational efficiency during
inference by activating only a subset of specialized experts for a given input.
This enables efficient model scaling on multi-GPU systems that use expert
parallelism without compromising performance. However, load imbalance among
experts and GPUs introduces waiting times, which can significantly increase
inference latency. To address this challenge, we propose HarMoEny, a novel
solution to address MoE load imbalance through two simple techniques: (i)
dynamic token redistribution to underutilized GPUs and (ii) asynchronous
prefetching of experts from the system to GPU memory. These techniques achieve
a near-perfect load balance among experts and GPUs and mitigate delays caused
by overloaded GPUs. We implement HarMoEny and compare its latency and
throughput with four MoE baselines using real-world and synthetic datasets.
Under heavy load imbalance, HarMoEny increases throughput by 37%-70% and
reduces time-to-first-token by 34%-41%, compared to the next-best baseline.
Moreover, our ablation study demonstrates that HarMoEny's scheduling policy
reduces the GPU idling time by up to 84% compared to the baseline policies.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-16T00:00:00Z">2025-06-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sketched Sum-Product Networks for Joins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Tsan, Abylay Amanbayev, Asoke Datta, Florin Rusu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sketches have shown high accuracy in multi-way join cardinality estimation, a
critical problem in cost-based query optimization. Accurately estimating the
cardinality of a join operation -- analogous to its computational cost --
allows the optimization of query execution costs in relational database
systems. However, although sketches have shown high efficacy in query
optimization, they are typically constructed specifically for predefined
selections in queries that are assumed to be given a priori, hindering their
applicability to new queries. As a more general solution, we propose for
Sum-Product Networks to dynamically approximate sketches on-the-fly.
Sum-Product Networks can decompose and model multivariate distributions, such
as relations, as linear combinations of multiple univariate distributions. By
representing these univariate distributions as sketches, Sum-Product Networks
can combine them element-wise to efficiently approximate the sketch of any
query selection. These approximate sketches can then be applied to join
cardinality estimation. In particular, we implement the Fast-AGMS and Bound
Sketch methods, which have successfully been used in prior work, despite their
costly construction. By accurately approximating them instead, our work
provides a practical alternative to apply these sketches to query optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parachute: Single-Pass Bi-Directional Information Passing <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihail Stoian, Andreas Zimmerer, Skander Krid, Amadou Latyr Ngom, Jialin Ding, Tim Kraska, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sideways information passing is a well-known technique for mitigating the
impact of large build sides in a database query plan. As currently implemented
in production systems, sideways information passing enables only a
uni-directional information flow, as opposed to instance-optimal algorithms,
such as Yannakakis'. On the other hand, the latter require an additional pass
over the input, which hinders adoption in production systems.
  In this paper, we make a step towards enabling single-pass bi-directional
information passing during query execution. We achieve this by statically
analyzing between which tables the information flow is blocked and by
leveraging precomputed join-induced fingerprint columns on FK-tables. On the
JOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time
without and with semi-join filtering by 1.54x and 1.24x, respectively, when
allowed to use 15% extra space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnhanceGraph: A Continuously Enhanced Graph-based Index for
  High-dimensional Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Mingyu Yang, Lei Chen, Haoyang Li, Zhitao Shen, Xuemin Lin, Heng Tao Shen, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Approximate Nearest Neighbor Search in high-dimensional vector
spaces has garnered considerable attention due to the rapid advancement of deep
learning techniques. We observed that a substantial amount of search and
construction logs are generated throughout the lifespan of a graph-based index.
However, these two types of valuable logs are not fully exploited due to the
static nature of existing indexes. We present the EnhanceGraph framework, which
integrates two types of logs into a novel structure called a conjugate graph.
The conjugate graph is then used to improve search quality. Through theoretical
analyses and observations of the limitations of graph-based indexes, we propose
several optimization methods. For the search logs, the conjugate graph stores
the edges from local optima to global optima to enhance routing to the nearest
neighbor. For the construction logs, the conjugate graph stores the pruned
edges from the proximity graph to enhance retrieving of k nearest neighbors.
Our experimental results on several public and real-world industrial datasets
show that EnhanceGraph significantly improves search accuracy with the greatest
improvement on recall from 41.74% to 93.42%, but does not sacrifices search
efficiency. In addition, our EnhanceGraph algorithm has been integrated into
Ant Group's open-source vector library, VSAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Integer Linear Programming All You Need for Deletion Propagation? A
  Unified and Practical Approach for Generalized Deletion Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Makhija, Wolfgang Gatterbauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deletion Propagation (DP) refers to a family of database problems rooted in
the classical view-update problem: how to propagate intended deletions in a
view (query output) back to the source database while satisfying constraints
and minimizing side effects. Although studied for over 40 years, DP variants,
their complexities, and practical algorithms have been typically explored in
isolation.
  This work presents a unified and generalized framework for DP with several
key benefits: (1) It unifies and generalizes all previously known DP variants,
effectively subsuming them within a broader class of problems, including new,
well-motivated variants. (2) It comes with a practical and general-purpose
algorithm that is ``coarse-grained instance-optimal'': it runs in PTIME for all
known PTIME cases and can automatically exploit structural regularities in the
data, i.e. it does not rely on hints about such regularities as part of the
input. (3) It is complete: our framework handles all known DP variants in all
settings (including those involving self-joins, unions, and bag semantics), and
allows us to provide new complexity results. (4) It is easy to implement and,
in many cases, outperforms prior variant-specific solutions, sometimes by
orders of magnitude. We provide the first experimental results for several DP
variants previously studied only in theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Vector Database: Storage and Retrieval
  Technique, Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Ma, Ran Zhang, Yikun Han, Shirui Yu, Zaitian Wang, Zhiyuan Ning, Jinghan Zhang, Ping Xu, Pengjiang Li, Wei Ju, Chong Chen, Dongjie Wang, Kunpeng Liu, Pengyang Wang, Pengfei Wang, Yanjie Fu, Chunjiang Liu, Yuanchun Zhou, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector databases (VDBs) have emerged to manage high-dimensional data that
exceed the capabilities of traditional database management systems, and are now
tightly integrated with large language models as well as widely applied in
modern artificial intelligence systems. Although relatively few studies
describe existing or introduce new vector database architectures, the core
technologies underlying VDBs, such as approximate nearest neighbor search, have
been extensively studied and are well documented in the literature. In this
work, we present a comprehensive review of the relevant algorithms to provide a
general understanding of this booming research area. Specifically, we first
provide a review of storage and retrieval techniques in VDBs, with detailed
design principles and technological evolution. Then, we conduct an in-depth
comparison of several advanced VDB solutions with their strengths, limitations,
and typical application scenarios. Finally, we also outline emerging
opportunities for coupling VDBs with large language models, including open
research problems and trends, such as novel indexing strategies. This survey
aims to serve as a practical resource, enabling readers to quickly gain an
overall understanding of the current knowledge landscape in this rapidly
developing area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Heuristic Framework for Resource-Efficient Querying of
  Scientific Experiments Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Patel, Minal Bhise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in
  Precision Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishesh Kumar Tanwar, Soumik Sarkar, Asheesh K. Singh, Sajal K. Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To empower precision agriculture through distributed machine learning (DML),
split learning (SL) has emerged as a promising paradigm, partitioning deep
neural networks (DNNs) between edge devices and servers to reduce computational
burdens and preserve data privacy. However, conventional SL frameworks'
one-split-fits-all strategy is a critical limitation in agricultural ecosystems
where edge insect monitoring devices exhibit vast heterogeneity in
computational power, energy constraints, and connectivity. This leads to
straggler bottlenecks, inefficient resource utilization, and compromised model
performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement
learning (RL)-driven framework that dynamically tailors DNN split points for
each device, optimizing efficiency without sacrificing accuracy. Specifically,
a Q-learning agent acts as an adaptive orchestrator, balancing workloads and
latency thresholds across devices to mitigate computational starvation or
overload. By framing split layer selection as a finite-state Markov decision
process, ReinDSplit convergence ensures that highly constrained devices
contribute meaningfully to model training over time. Evaluated on three insect
classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit
achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit
pioneers a paradigm shift in SL by harmonizing RL for resource efficiency,
privacy, and scalability in heterogeneous environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BanditWare: A Contextual Bandit-based Framework for Hardware Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tainã Coleman, Hena Ahmed, Ravi Shende, Ismael Perez, Ïlkay Altintaş
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed computing systems are essential for meeting the demands of modern
applications, yet transitioning from single-system to distributed environments
presents significant challenges. Misallocating resources in shared systems can
lead to resource contention, system instability, degraded performance, priority
inversion, inefficient utilization, increased latency, and environmental
impact.
  We present BanditWare, an online recommendation system that dynamically
selects the most suitable hardware for applications using a contextual
multi-armed bandit algorithm. BanditWare balances exploration and exploitation,
gradually refining its hardware recommendations based on observed application
performance while continuing to explore potentially better options. Unlike
traditional statistical and machine learning approaches that rely heavily on
large historical datasets, BanditWare operates online, learning and adapting in
real-time as new workloads arrive.
  We evaluated BanditWare on three workflow applications: Cycles (an
agricultural science scientific workflow) BurnPro3D (a web-based platform for
fire science) and a matrix multiplication application. Designed for seamless
integration with the National Data Platform (NDP), BanditWare enables users of
all experience levels to optimize resource allocation efficiently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POPQC: Parallel Optimization for Quantum Circuits (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Liu, Jatin Arora, Mingkuan Xu, Umut A. Acar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization of quantum programs or circuits is a fundamental problem in
quantum computing and remains a major challenge. State-of-the-art quantum
circuit optimizers rely on heuristics and typically require superlinear, and
even exponential, time. Recent work proposed a new approach that pursues a
weaker form of optimality called local optimality. Parameterized by a natural
number $\Omega$, local optimality insists that each and every $\Omega$-segment
of the circuit is optimal with respect to an external optimizer, called the
oracle. Local optimization can be performed using only a linear number of calls
to the oracle but still incurs quadratic computational overheads in addition to
oracle calls. Perhaps most importantly, the algorithm is sequential.
  In this paper, we present a parallel algorithm for local optimization of
quantum circuits. To ensure efficiency, the algorithm operates by keeping a set
of fingers into the circuit and maintains the invariant that a $\Omega$-deep
circuit needs to be optimized only if it contains a finger. Operating in
rounds, the algorithm selects a set of fingers, optimizes in parallel the
segments containing the fingers, and updates the finger set to ensure the
invariant. For constant $\Omega$, we prove that the algorithm requires
$O(n\lg{n})$ work and $O(r\lg{n})$ span, where $n$ is the circuit size and $r$
is the number of rounds. We prove that the optimized circuit returned by the
algorithm is locally optimal in the sense that any $\Omega$-segment of the
circuit is optimal with respect to the oracle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SPAA25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Li, Haiyong Bao, Menghong Guan, Hao Pan, Cheng Huang, Hong-Ning Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite federated learning (FL)'s potential in collaborative learning, its
performance has deteriorated due to the data heterogeneity of distributed
users. Recently, clustered federated learning (CFL) has emerged to address this
challenge by partitioning users into clusters according to their similarity.
However, CFL faces difficulties in training when users are unwilling to share
their cluster identities due to privacy concerns. To address these issues, we
present an innovative Efficient and Robust Secure Aggregation scheme for CFL,
dubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while
maintaining users' cluster identity confidentially. Moreover, it detects
potential poisonous attacks without compromising individual client gradients by
discarding negatively correlated gradients and aggregating positively
correlated ones using a weighted approach. The server also authenticates
correct gradient encoding by clients. EBS-CFL has high efficiency with
client-side overhead O(ml + m^2) for communication and O(m^2l) for computation,
where m is the number of cluster identities, and l is the gradient size. When m
= 1, EBS-CFL's computational efficiency of client is at least O(log n) times
better than comparison schemes, where n is the number of clients.In addition,
we validate the scheme through extensive experiments. Finally, we theoretically
prove the scheme's security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xia, Christoph Hofmeister, Maximilian Egger, Rawad Bitar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) shows great promise in large-scale machine learning
but introduces new privacy and security challenges. We propose ByITFL and
LoByITFL, two novel FL schemes that enhance resilience against Byzantine users
while keeping the users' data private from eavesdroppers. To ensure privacy and
Byzantine resilience, our schemes build on having a small representative
dataset available to the federator and crafting a discriminator function
allowing the mitigation of corrupt users' contributions. ByITFL employs
Lagrange coded computing and re-randomization, making it the first
Byzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,
though at the cost of a significant communication overhead. LoByITFL, on the
other hand, achieves Byzantine resilience and IT privacy at a significantly
reduced communication cost, but requires a Trusted Third Party, used only in a
one-time initialization phase before training. We provide theoretical
guarantees on privacy and Byzantine resilience, along with convergence
guarantees and experimental results validating our findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDiT: Dynamic Resource Allocation for Diffusion Transformer Model
  Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyang Huang, Cunchen Hu, Jiaqi Zhu, Ziyuan Gao, Liangliang Xu, Yizhou Shan, Yungang Bao, Sun Ninghui, Tianwei Zhang, Sa Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Text-to-Video (T2V) model aims to generate dynamic and expressive videos
from textual prompts. The generation pipeline typically involves multiple
modules, such as language encoder, Diffusion Transformer (DiT), and Variational
Autoencoders (VAE). Existing serving systems often rely on monolithic model
deployment, while overlooking the distinct characteristics of each module,
leading to inefficient GPU utilization. In addition, DiT exhibits varying
performance gains across different resolutions and degrees of parallelism, and
significant optimization potential remains unexplored. To address these
problems, we present DDiT, a flexible system that integrates both inter-phase
and intra-phase optimizations. DDiT focuses on two key metrics: optimal degree
of parallelism, which prevents excessive parallelism for specific resolutions,
and starvation time, which quantifies the sacrifice of each request. To this
end, DDiT introduces a decoupled control mechanism to minimize the
computational inefficiency caused by imbalances in the degree of parallelism
between the DiT and VAE phases. It also designs a greedy resource allocation
algorithm with a novel scheduling mechanism that operates at the single-step
granularity, enabling dynamic and timely resource scaling. Our evaluation on
the T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets
reveals that DDiT significantly outperforms state-of-the-art baselines by up to
1.44x in p99 latency and 1.43x in average latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed
  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Steven Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a formalised architecture for synthetic agents designed
to retain immutable memory, verifiable reasoning, and constrained epistemic
growth. Traditional AI systems rely on mutable, opaque statistical models prone
to epistemic drift and historical revisionism. In contrast, we introduce the
concept of the Merkle Automaton, a cryptographically anchored, deterministic
computational framework that integrates formal automata theory with
blockchain-based commitments. Each agent transition, memory fragment, and
reasoning step is committed within a Merkle structure rooted on-chain,
rendering it non-repudiable and auditably permanent. To ensure selective access
and confidentiality, we derive symmetric encryption keys from ECDH exchanges
contextualised by hierarchical privilege lattices. This enforces cryptographic
access control over append-only DAG-structured knowledge graphs. Reasoning is
constrained by formal logic systems and verified through deterministic
traversal of policy-encoded structures. Updates are non-destructive and
historied, preserving epistemic lineage without catastrophic forgetting.
Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion
attestations. Collectively, this architecture reframes memory not as a cache
but as a ledger - one whose contents are enforced by protocol, bound by
cryptography, and constrained by formal logic. The result is not an intelligent
agent that mimics thought, but an epistemic entity whose outputs are provably
derived, temporally anchored, and impervious to post hoc revision. This design
lays foundational groundwork for legal, economic, and high-assurance
computational systems that require provable memory, unforgeable provenance, and
structural truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, includes formal automata specifications, cryptographic
  constructions, and epistemic architecture schema</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Terminology for Scientific Workflow Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07838v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07838v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frédéric Suter, Tainã Coleman, İlkay Altintaş, Rosa M. Badia, Bartosz Balis, Kyle Chard, Iacopo Colonnelli, Ewa Deelman, Paolo Di Tommaso, Thomas Fahringer, Carole Goble, Shantenu Jha, Daniel S. Katz, Johannes Köster, Ulf Leser, Kshitij Mehta, Hilary Oliver, J. -Luc Peterson, Giovanni Pizzi, Loïc Pottier, Raül Sirvent, Eric Suchyta, Douglas Thain, Sean R. Wilkinson, Justin M. Wozniak, Rafael Ferreira da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The term scientific workflow has evolved over the last two decades to
encompass a broad range of compositions of interdependent compute tasks and
data movements. It has also become an umbrella term for processing in modern
scientific applications. Today, many scientific applications can be considered
as workflows made of multiple dependent steps, and hundreds of workflow
management systems (WMSs) have been developed to manage and run these
workflows. However, no turnkey solution has emerged to address the diversity of
scientific processes and the infrastructure on which they are implemented.
Instead, new research problems requiring the execution of scientific workflows
with some novel feature often lead to the development of an entirely new WMS. A
direct consequence is that many existing WMSs share some salient features,
offer similar functionalities, and can manage the same categories of workflows
but also have some distinct capabilities. This situation makes researchers who
develop workflows face the complex question of selecting a WMS. This selection
can be driven by technical considerations, to find the system that is the most
appropriate for their application and for the resources available to them, or
other factors such as reputation, adoption, strong community support, or
long-term sustainability. To address this problem, a group of WMS developers
and practitioners joined their efforts to produce a community-based terminology
of WMSs. This paper summarizes their findings and introduces this new
terminology to characterize WMSs. This terminology is composed of fives axes:
workflow characteristics, composition, orchestration, data management, and
metadata capture. Each axis comprises several concepts that capture the
prominent features of WMSs. Based on this terminology, this paper also presents
a classification of 23 existing WMSs according to the proposed axes and terms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ILVES: Accurate and efficient bond length and angle constraints in
  molecular dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorién López-Villellas, Carl Christian Kjelgaard Mikkelsen, Juan José Galano-Frutos, Santiago Marco-Sola, Jesús Alastruey-Benedé, Pablo Ibáñez, Pablo Echenique, Miquel Moretó, Maria Cristina De Rosa, Pablo García-Risueño
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All-atom, force field-based molecular dynamics simulations are essential
tools in computational chemistry, enabling the prediction and analysis of
biomolecular systems with atomic-level resolution. However, as system sizes and
simulation timescales increase, so does the associated computational cost. To
extend simulated time using the same resources, a common strategy is to
constrain the fastest degrees of freedom, such as bond lengths, allowing for
larger integration time steps without compromising accuracy. The de facto
state-of-the-art algorithms for this purpose (SHAKE, LINCS, and P-LINCS) are
integrated into most molecular dynamics packages and widely adopted across the
field. Despite their impact, these methods exhibit limitations: all converge
slowly when high numerical accuracy is required, and the LINCS and P-LINCS
algorithms cannot handle general angular constraints, limiting further
increases in time step.
  In this article, we introduce ILVES, a family of parallel algorithms that
converge so rapidly that it is now practical to solve bond length and
associated angular constraint equations as accurately as the hardware will
allow. We have integrated ILVES into Gromacs and our analysis demonstrates that
it is superior to the state-of-the-art when constraining bond lengths. Due to
its better convergence properties, we also show that if the time step is
increased up to 3.5 fs by enforcing angular constraints, ILVES enables a 1.65x
increase in simulated time using the same computational resources and
wall-clock time, an outcome unattainable with current methods. This advance can
significantly reduce the computational cost of most all-atom molecular dynamics
simulations while improving their accuracy and extending access to larger
systems and longer timescales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EvalNet: A Practical Toolchain for Generation and Analysis of
  Extreme-Scale Interconnects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.12663v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.12663v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Patrick Iff, Marcel Schneider, Nils Blach, Alessandro Maissen, Salvatore Di Girolamo, Jens Domke, Jascha Krattenmacher, Ankit Singla, Kartik Lakhotia, Laura Monroe, Fabrizio Petrini, Robert Gerstenberger, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diversity of communication paths in a network - especially non-minimal
paths - is a key enabler of performance at extreme scales. We present EvalNet,
a toolchain for scalable generation and analysis over 25 important network
topologies, such as Slim Fly, PolarFly, and Orthogonal Fat Trees, with a strong
focus on path diversity metrics. EvalNet provides an extensive and fine-grained
analysis of shortest and non-shortest paths, including their multiplicities,
lengths, and interference. It supports exact measurement and visualization of
bandwidth and throughput between every router pair, enabling unprecedented
insight into routing potential. EvalNet also includes detailed models for
construction cost and power consumption, and interfaces seamlessly with
established simulators, which we tune to support large-scale evaluations on
low-cost hardware. Using EvalNet, we deliver the widest and most comprehensive
path diversity study to date, demonstrating how path diversity underpins
throughput and scalability, and facilitating progress towards new frontiers in
extreme-scale network design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-Tolerant Consensus in GPU-Inspired Shared Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.12788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.12788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chryssis Georgiou, Manaswini Piduguralla, Sathya Peri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we formalize a novel shared memory model inspired by the
popular GPU architecture. Within this model, we develop algorithmic solutions
to the Byzantine Consensus problem and analyze their fault-resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blockchain and Biometrics: <span class="highlight-title">Survey</span>, GDPR Elements, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Ghafourian, Ruben Vera-Rodriguez, Julian Fierrez, Bilgesu Sumer, Ruben Tolosana, Aythami Moralez, Els Kindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric recognition as an efficient and hard-to-forge way of identification
and verification has become an indispensable part of the current digital world.
The fast evolution of this technology has been a strong incentive for
integration into many applications. Meanwhile, blockchain, the decentralized
ledger technology, has been widely received by both research and industry in
the past few years, and it is being increasingly deployed today in many
different applications, such as money transfer, IoT, healthcare, or logistics.
Recently, researchers have started to speculate on the pros and cons and what
the best applications would be when these two technologies cross paths. This
paper provides a survey of the research literature on the combination of
blockchain and biometrics and includes a first legal analysis of this
integration based on GDPR to shed light on challenges and potentials. Although
the integration of blockchain technology into the biometric sector is still in
its infancy, with a growing body of literature discussing specific applications
and advanced technological setups, this paper aims to provide a holistic
understanding of blockchain applicability in biometrics. Based on published
studies, this article discusses, among others, practical examples combining
blockchain and biometrics for novel applications in PKI systems, distributed
trusted services, and identity management. Challenges and limitations when
combining blockchain and biometrics that motivate future work will also be
discussed; e.g., blockchain networks at their current stage may not be
efficient or economical for some real-time biometric applications. Finally, we
also discuss key legal aspects of the EU General Data Protection Regulation
(GDPR) related to this combination of technologies (blockchain and biometrics);
for example, accountability, immutability, anonymity, and data protection
elements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFPL: Decentralized Federated Prototype Learning Across Heterogeneous
  Data Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04947v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04947v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongliang Zhang, Fenghua Xu, Zhongyuan Yu, Shanchen Pang, Chunqiang Hu, Jiguo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed machine learning paradigm through
centralized model aggregation. However, standard federated learning relies on a
centralized server, making it vulnerable to server failures. While existing
solutions utilize blockchain technology to implement Decentralized Federated
Learning (DFL), the statistical heterogeneity of data distributions among
clients severely degrades the performance of DFL. Driven by this issue, this
paper proposes a decentralized federated prototype learning framework, named
DFPL, which significantly improves the performance of DFL across heterogeneous
data distributions. Specifically, DFPL introduces prototype learning into DFL
to mitigate the impact of statistical heterogeneity and reduces the amount of
parameters exchanged between clients. Additionally, blockchain is embedded into
our framework, enabling the training and mining processes to be implemented
locally on each client. From a theoretical perspective, we analyze the
convergence of DFPL by modeling the required computational resources during
both training and mining processes. The experiment results highlight the
superiority of our DFPL in model performance and communication efficiency
across four benchmark datasets with heterogeneous data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Heuristic Framework for Resource-Efficient Querying of
  Scientific Experiments Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayank Patel, Minal Bhise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-15T00:00:00Z">2025-06-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Humans, Machine Learning, and Language Models in Union: A Cognitive
  Study on Table Unionability <span class="chip">SIGMOD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreeram Marimuthu, Nina Klimenkova, Roee Shraga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data discovery and table unionability in particular became key tasks in
modern Data Science. However, the human perspective for these tasks is still
under-explored. Thus, this research investigates the human behavior in
determining table unionability within data discovery. We have designed an
experimental survey and conducted a comprehensive analysis, in which we assess
human decision-making for table unionability. We use the observations from the
analysis to develop a machine learning framework to boost the (raw) performance
of humans. Furthermore, we perform a preliminary study on how LLM performance
is compared to humans indicating that it is typically better to consider a
combination of both. We believe that this work lays the foundations for
developing future Human-in-the-Loop systems for efficient data discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 4 figures, ACM SIGMOD HILDA '25 (Status-Accepted)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Visualizing Electronic Medical Records via Natural Language
  Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodi Zhang, Siqi Ning, Qiyong Zheng, Jinyin Nie, Liangjie Zhang, Weicheng Wang, Yuanfeng Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic medical records (EMRs) contain essential data for patient care and
clinical research. With the diversity of structured and unstructured data in
EHR, data visualization is an invaluable tool for managing and explaining these
complexities. However, the scarcity of relevant medical visualization data and
the high cost of manual annotation required to develop such datasets pose
significant challenges to advancing medical visualization techniques. To
address this issue, we propose an innovative approach using large language
models (LLMs) for generating visualization data without labor-intensive manual
annotation. We introduce a new pipeline for building text-to-visualization
benchmarks suitable for EMRs, enabling users to visualize EMR statistics
through natural language queries (NLQs). The dataset presented in this paper
primarily consists of paired text medical records, NLQs, and corresponding
visualizations, forming the first large-scale text-to-visual dataset for
electronic medical record information called MedicalVis with 35,374 examples.
Additionally, we introduce an LLM-based approach called MedCodeT5, showcasing
its viability in generating EMR visualizations from NLQs, outperforming various
strong text-to-visualization baselines. Our work facilitates standardized
evaluation of EMR visualization methods while providing researchers with tools
to advance this influential field of application. In a nutshell, this study and
dataset have the potential to promote advancements in eliciting medical
insights through visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alpha-SQL: Zero-Shot Text-to-SQL using Monte Carlo Tree Search <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL, which enables natural language interaction with databases,
serves as a pivotal method across diverse industries. With new, more powerful
large language models (LLMs) emerging every few months, fine-tuning has become
incredibly costly, labor-intensive, and error-prone. As an alternative,
zero-shot Text-to-SQL, which leverages the growing knowledge and reasoning
capabilities encoded in LLMs without task-specific fine-tuning, presents a
promising and more challenging direction. To address this challenge, we propose
Alpha-SQL, a novel approach that leverages a Monte Carlo Tree Search (MCTS)
framework to iteratively infer SQL construction actions based on partial
reasoning states. To enhance the framework's reasoning capabilities, we
introduce LLM-as-Action-Model to dynamically generate SQL construction actions
during the MCTS process, steering the search toward more promising SQL queries.
Moreover, Alpha-SQL employs a self-supervised reward function to evaluate the
quality of candidate SQL queries, ensuring more accurate and efficient query
generation. Experimental results show that Alpha-SQL achieves 69.7% execution
accuracy on the BIRD development set, using a 32B open-source LLM without
fine-tuning. Alpha-SQL outperforms the best previous zero-shot approach based
on GPT-4o by 2.5% on the BIRD development set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A <span class="highlight-title">Survey</span> of Text-to-SQL in the Era of LLMs: Where are we, and where are
  we going? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05109v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05109v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, <span class="highlight-author">Guoliang Li</span>, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating users' natural language queries (NL) into SQL queries (i.e.,
Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing
relational databases and support various commercial applications. The
performance of Text-to-SQL has been greatly enhanced with the emergence of
Large Language Models (LLMs). In this survey, we provide a comprehensive review
of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from
the following four aspects: (1) Model: Text-to-SQL translation techniques that
tackle not only NL ambiguity and under-specification, but also properly map NL
with database schema and instances; (2) Data: From the collection of training
data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks;
(3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using
different metrics and granularities; and (4) Error Analysis: analyzing
Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to
evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL
solutions. Finally, we discuss the research challenges and open problems of
Text-to-SQL in the LLMs era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL
  Translation <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language to SQL (i.e., NL2SQL) translation is crucial for
democratizing database access, but even state-of-the-art models frequently
generate semantically incorrect SQL queries, hindering the widespread adoption
of these techniques by database vendors. While existing NL2SQL benchmarks
primarily focus on correct query translation, we argue that a benchmark
dedicated to identifying common errors in NL2SQL translations is equally
important, as accurately detecting these errors is a prerequisite for any
subsequent correction-whether performed by humans or models. To address this
gap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and
categorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a
two-level taxonomy to systematically classify semantic errors, covering 9 main
categories and 31 subcategories. The benchmark consists of 2,018
expert-annotated instances, each containing a natural language query, database
schema, and SQL query, with detailed error annotations for semantically
incorrect queries. Through comprehensive experiments, we demonstrate that
current large language models exhibit significant limitations in semantic error
detection, achieving an average detection accuracy of 75.16%. Specifically, our
method successfully detected 106 errors (accounting for 6.91%) in BIRD, a
widely-used NL2SQL dataset, which were previously undetected annotation errors.
This highlights the importance of semantic error detection in NL2SQL systems.
The benchmark is publicly available at https://nl2sql-bugs.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 4 tables, KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Computing From First Principles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Odoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This book on Distributed Computing aims to benefit a diverse audience,
ranging from aspiring engineers, and seasoned researchers, to a wide range of
professionals. Driven by my passion for making the core concepts of distributed
computing accessible, this work is a significant undertaking designed to
empower individuals from all backgrounds to gain valuable insight. Have you
ever wondered how a typical distributed system works under the hood? Are you
looking for a pedagogical guide with complete implementations? In this work, we
have implemented several foundational algorithms in Distributed Computing.
Whether your expertise lies in the theoretical foundations or the practical
applications of the principles of Distributed Systems, this book is for you.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Stabilizing Replicated State Machine Coping with Byzantine and
  Recurring Transient Faults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shlomi Dolev, Amit Hendin, Maurice Herlihy, Maria Potop Butucaru, Elad Michael Schiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to perform repeated Byzantine agreement lies at the heart of
important applications such as blockchain price oracles or replicated state
machines. Any such protocol requires the following properties: (1)
\textit{Byzantine fault-tolerance}, because not all participants can be assumed
to be honest, (2) r\textit{ecurrent transient fault-tolerance}, because even
honest participants may be subject to transient ``glitches'', (3)
\textit{accuracy}, because the results of quantitative queries (such as price
quotes) must lie within the interval of honest participants' inputs, and (4)
\textit{self-stabilization}, because it is infeasible to reboot a distributed
system following a fault.
  This paper presents the first protocol for repeated Byzantine agreement that
satisfies the properties listed above. Specifically, starting in an arbitrary
system configuration, our protocol establishes consistency. It preserves
consistency in the face of up to $\lceil n/3 \rceil -1$ Byzantine participants
{\em and} constant recurring (``noise'') transient faults, of up to $\lceil n/6
\rceil-1$ additional malicious transient faults, or even more than $\lceil n/6
\rceil-1$ (uniformly distributed) random transient faults, in each repeated
Byzantine agreement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-architecture universal feature coding via distribution alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsheng Gao, Shan Liu, Feng Wu, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature coding has become increasingly important in scenarios where semantic
representations rather than raw pixels are transmitted and stored. However,
most existing methods are architecture-specific, targeting either CNNs or
Transformers. This design limits their applicability in real-world scenarios
where features from both architectures coexist. To address this gap, we
introduce a new research problem: cross-architecture universal feature coding
(CAUFC), which seeks to build a unified codec that can effectively compress
features from heterogeneous architectures. To tackle this challenge, we propose
a two-step distribution alignment method. First, we design the format alignment
method that unifies CNN and Transformer features into a consistent 2D token
format. Second, we propose the feature value alignment method that harmonizes
statistical distributions via truncation and normalization. As a first attempt
to study CAUFC, we evaluate our method on the image classification task.
Experimental results demonstrate that our method achieves superior
rate-accuracy trade-offs compared to the architecture-specific baseline. This
work marks an initial step toward universal feature compression across
heterogeneous model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Real-Time Job Mapping and Resource Management in
  Mobile-Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanchao Gao, Niraj Kumar, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile-edge computing (MEC) has emerged as a promising paradigm for enabling
Internet of Things (IoT) devices to handle computation-intensive jobs. Due to
the imperfect parallelization of algorithms for job processing on servers and
the impact of IoT device mobility on data communication quality in wireless
networks, it is crucial to jointly consider server resource allocation and IoT
device mobility during job scheduling to fully benefit from MEC, which is often
overlooked in existing studies. By jointly considering job scheduling, server
resource allocation, and IoT device mobility, we investigate the
deadline-constrained job offloading and resource management problem in MEC with
both communication and computation contentions, aiming to maximize the total
energy saved for IoT devices. For the offline version of the problem, where job
information is known in advance, we formulate it as an Integer Linear
Programming problem and propose an approximation algorithm, $\mathtt{LHJS}$,
with a constant performance guarantee. For the online version, where job
information is only known upon release, we propose a heuristic algorithm,
$\mathtt{LBS}$, that is invoked whenever a job is released. Finally, we conduct
experiments with parameters from real-world applications to evaluate their
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17246v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17246v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model autoscaling is the key mechanism to achieve serverless
model-as-a-service, but it faces a fundamental trade-off between scaling speed
and storage/memory usage to cache parameters, and cannot meet frequent scaling
requirements across multiple hosts. The key problem is that data plane
performance is slow, and scaled instances remain stopped while parameters are
loading. In this paper, we first show that the data plane can be made fast with
no or O(1) caching by loading parameters through the compute network between
GPUs because: (1) its speed is comparable to host cache and is underutilized,
and (2) scaling multiple instances requires no or O(1) caching with
network-optimized multicast. Second, autoscaling can be made live by breaking
the scaling abstraction for inference from a coarse-grained instance-level to a
fine-grained layer-level. This allows us to offload the layer computation from
the overloaded serving instances to the scaled ones without waiting for the
parameters to be fully loaded. Under real-world workloads, our system
BLITZSCALE achieves up to 94 % lower tail latency reductions compared to
state-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU
time used for serving by 49 % when compared with serving systems that do not
support autoscaling like DistServe and vLLM with the same
service-level-agreement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of OSDI'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ zkMixer: A Configurable Zero-Knowledge Mixer with Anti-Money Laundering
  Consensus Protocols 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Constantinides, John Cartlidge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a zero-knowledge cryptocurrency mixer framework that allows
groups of users to set up a mixing pool with configurable governance
conditions, configurable deposit delays, and the ability to refund or
confiscate deposits if it is suspected that funds originate from crime. Using a
consensus process, group participants can monitor inputs to the mixer and
determine whether the inputs satisfy the mixer conditions. If a deposit is
accepted by the group, it will enter the mixer and become untraceable. If it is
not accepted, the verifiers can freeze the deposit and collectively vote to
either refund the deposit back to the user, or confiscate the deposit and send
it to a different user. This behaviour can be used to examine deposits,
determine if they originate from a legitimate source, and if not, return
deposits to victims of crime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, accepted author manuscript</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-14T00:00:00Z">2025-06-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redbench: A Benchmark Reflecting Real Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skander Krid, Mihail Stoian, Andreas Kipf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance-optimized components have made their way into production systems. To
some extent, this adoption is due to the characteristics of customer workloads,
which can be individually leveraged during the model training phase. However,
there is a gap between research and industry that impedes the development of
realistic learned components: the lack of suitable workloads. Existing ones,
such as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail
to exhibit real workload patterns, particularly distribution shifts.
  In this paper, we introduce Redbench, a collection of 30 workloads that
reflect query patterns observed in the real world. The workloads were obtained
by sampling queries from support benchmarks and aligning them with workload
characteristics observed in Redset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eighth International Workshop on Exploiting Artificial Intelligence
  Techniques for Data Management (aiDM 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and
  Ethics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asifullah khan, Muhammad Zaeem Khan, Saleha Jamshed, Sadia Ahmad, Aleesha Zainab, Kaynat Khatib, Faria Bibi, Abdul Rehman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey paper outlines the key developments in the field of Large
Language Models (LLMs), such as enhancing their reasoning skills, adaptability
to various tasks, increased computational efficiency, and ability to make
ethical decisions. The techniques that have been most effective in bridging the
gap between human and machine communications include the Chain-of-Thought
prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.
The improvements in multimodal learning and few-shot or zero-shot techniques
have further empowered LLMs to handle complex jobs with minor input. They also
manage to do more with less by applying scaling and optimization tricks for
computing power conservation. This survey also offers a broader perspective on
recent advancements in LLMs going beyond isolated aspects such as model
architecture or ethical concerns. It categorizes emerging methods that enhance
LLM reasoning, efficiency, and ethical alignment. It also identifies
underexplored areas such as interpretability, cross-modal integration and
sustainability. With recent progress, challenges like huge computational costs,
biases, and ethical risks remain constant. Addressing these requires bias
mitigation, transparent decision-making, and clear ethical guidelines. Future
research will focus on enhancing models ability to handle multiple input,
thereby making them more intelligent, safe, and reliable.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Cloud-Based Transcriptomics: Performance Analysis and
  Optimization of the STAR Aligner Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Kica, Sabina Lichołai, Michał Orzechowski, Maciej Malawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore the Transcriptomics Atlas pipeline adapted for
cost-efficient and high-throughput computing in the cloud. We propose a
scalable, cloud-native architecture designed for running a resource-intensive
aligner -- STAR -- and processing tens or hundreds of terabytes of
RNA-sequencing data. We implement multiple optimization techniques that give
significant execution time and cost reduction. The impact of particular
optimizations is measured in medium-scale experiments followed by a large-scale
experiment that leverages all of them and validates the current design. Early
stopping optimization allows a reduction in total alignment time by 23%. We
analyze the scalability and efficiency of one of the most widely used sequence
aligners. For the cloud environment, we identify one of the most suitable EC2
instance types and verify the applicability of spot instances usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Flow: Perspectives, Scenarios, and Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun An, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pioneered by the foundational information theory by Claude Shannon and the
visionary framework of machine intelligence by Alan Turing, the convergent
evolution of information and communication technologies (IT/CT) has created an
unbroken wave of connectivity and computation. This synergy has sparked a
technological revolution, now reaching its peak with large artificial
intelligence (AI) models that are reshaping industries and redefining
human-machine collaboration. However, the realization of ubiquitous
intelligence faces considerable challenges due to substantial resource
consumption in large models and high communication bandwidth demands. To
address these challenges, AI Flow has been introduced as a multidisciplinary
framework that integrates cutting-edge IT and CT advancements, with a
particular emphasis on the following three key points. First, device-edge-cloud
framework serves as the foundation, which integrates end devices, edge servers,
and cloud clusters to optimize scalability and efficiency for low-latency model
inference. Second, we introduce the concept of familial models, which refers to
a series of different-sized models with aligned hidden features, enabling
effective collaboration and the flexibility to adapt to varying resource
constraints and dynamic scenarios. Third, connectivity- and interaction-based
intelligence emergence is a novel paradigm of AI Flow. By leveraging
communication networks to enhance connectivity, the collaboration among AI
models across heterogeneous nodes achieves emergent intelligence that surpasses
the capability of any single model. The innovations of AI Flow provide enhanced
intelligence, timely responsiveness, and ubiquitous accessibility to AI
services, paving the way for the tighter fusion of AI techniques and
communication systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are with Institute of Artificial Intelligence (TeleAI), China
  Telecom, China. Author names are listed alphabetically by surname. This work
  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:
  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The
  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the
  CTO and Chief Scientist of China Telecom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Federated Learning using Remote Embeddings for Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Naman, Yogesh Simmhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. Federated Learning (FL) has emerged as a viable machine learning
approach for training a shared model on decentralized data, addressing privacy
concerns while leveraging parallelism. Existing methods that address the unique
requirements of federated GNN training using remote embeddings to enhance
convergence accuracy are limited by their diminished performance due to large
communication costs with a shared embedding server. In this paper, we present
OpES, an optimized federated GNN training framework that uses remote
neighbourhood pruning, and overlaps pushing of embeddings to the server with
local training to reduce the network costs and training time. The modest drop
in per-round accuracy due to pre-emptive push of embeddings is out-stripped by
the reduction in per-round training time for large and dense graphs like Reddit
and Products, converging up to $\approx2\times$ faster than the
state-of-the-art technique using an embedding server and giving up to $20\%$
better accuracy than vanilla federated GNN learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of paper in the proceedings of the 30th International
  European Conference on Parallel and Distributed Computing (Euro-Par)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QoS-aware Scheduling of Periodic Real-time Task Graphs on Heterogeneous
  Pre-occupied MECs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Shankar, Astha Kumari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In latency-sensitive applications, efficient task scheduling is crucial for
maintaining Quality of Service (QoS) while meeting strict timing constraints.
This paper addresses the challenge of scheduling periodic tasks structured as
directed acyclic graphs (DAGs) within heterogeneous, pre-occupied Mobile Edge
Computing (MEC) networks. We propose a modified version of the Heterogeneous
Earliest Finish Time (HEFT) algorithm designed to exploit residual processing
capacity in preoccupied MEC environments. Our approach dynamically identifies
idle intervals on processors to create a feasible hyperperiodic schedule that
specifies an allocated virtual machine (VM), task version, and start time for
each task. This scheduling strategy maximizes the aggregate QoS by optimizing
task execution without disrupting the existing periodic workload, while also
adhering to periodicity, precedence, and resource constraints.Experimental
results demonstrate that our method achieves enhanced load balancing and
resource utilization, highlighting its potential to improve performance in
heterogeneous MEC infrastructures supporting real-time, periodic applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 1 table,2 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Unified Caching for Accelerating Heterogeneous AI Workloads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianze Wang, Yifei Liu, Chen Chen, Pengfei Zuo, Jiawei Zhang, Qizhen Weng, Yin Chen, Zhenhua Han, Jieru Zhao, Quan Chen, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern AI clusters, which host diverse workloads like data pre-processing,
training and inference, often store the large-volume data in cloud storage and
employ caching frameworks to facilitate remote data access. To avoid
code-intrusion complexity and minimize cache space wastage, it is desirable to
maintain a unified cache shared by all the workloads. However, existing cache
management strategies, designed for specific workloads, struggle to handle the
heterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous
access patterns and item storage granularities. In this paper, we propose
IGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache
leverages a hierarchical access abstraction, AccessStreamTree, to organize the
recent data accesses in a tree structure, facilitating access pattern detection
at various granularities. Using this abstraction, IGTCache applies hypothesis
testing to categorize data access patterns as sequential, random, or skewed.
Based on these detected access patterns and granularities, IGTCache tailors
optimal cache management strategies including prefetching, eviction, and space
allocation accordingly. Experimental results show that IGTCache increases the
cache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the
overall job completion time by 52.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GroupNL: Low-Resource and Robust CNN Design over Cloud and Device 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuntao Ding, Jianhang Xie, Junna Zhang, Salman Raza, Shangguang Wang, Jiannong Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has become mainstream to deploy Convolutional Neural Network (CNN) models
on ubiquitous Internet of Things (IoT) devices with the help of the cloud to
provide users with a variety of high-quality services. Most existing methods
have two limitations: (i) low robustness in handling corrupted image data
collected by IoT devices; and (ii) high consumption of computational and
transmission resources. To this end, we propose the Grouped NonLinear
transformation generation method (GroupNL), which generates diversified feature
maps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to
improve the robustness of the CNN model. Specifically, partial convolution
filters are designated as seed filters in a convolutional layer, and a small
set of feature maps, i.e., seed feature maps, are first generated based on
vanilla convolution operation. Then, we split seed feature maps into several
groups, each with a set of different NLFs, to generate corresponding diverse
feature maps with in-place nonlinear processing. Moreover, GroupNL effectively
reduces the parameter transmission between multiple nodes during model training
by setting the hyperparameters of NLFs to random initialization and not
updating them during model training, and reduces the computing resources by
using NLFs to generate feature maps instead of most feature maps generated
based on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,
Icons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the
proposed GroupNL outperforms other state-of-the-art methods in model robust and
training acceleration. Specifically, on the Icons-50 dataset, the accuracy of
GroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla
ResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN
when trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Energy-Efficient Distributed Agreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Mirault, Peter Robinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study fault-tolerant consensus in a variant of the synchronous message
passing model, where, in each round, every node can choose to be awake or
asleep. This is known as the sleeping model (Chatterjee, Gmyr, Pandurangan PODC
2020) and defines the awake complexity (also called \emph{energy complexity}),
which measures the maximum number of rounds that any node is awake throughout
the execution. Only awake nodes can send and receive messages in a given round
and all messages sent to sleeping nodes are lost. We present new deterministic
consensus algorithms that tolerate up to $f<n$ crash failures, where $n$ is the
number of nodes. Our algorithms match the optimal time complexity lower bound
of $f+1$ rounds. For multi-value consensus, where the input values are chosen
from some possibly large set, we achieve an energy complexity of ${O}(\lceil
f^2 / n \rceil)$ rounds, whereas for binary consensus, we show that ${O}(\lceil
f / \sqrt{n} \rceil)$ rounds are possible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at PODC 2025 as brief announcement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance optimization of BLAS algorithms with band matrices for
  RISC-V processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Pirova, Anastasia Vodeneeva, Konstantin Kovalev, Alexander Ustinov, Evgeny Kozinov, Alexey Liniov, Valentin Volokitin, Iosif Meyerov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of RISC-V instruction set architecture presents new
opportunities and challenges for software developers. Is it sufficient to
simply recompile high-performance software optimized for x86-64 onto RISC-V
CPUs? Are current compilers capable of effectively optimizing C and C++ codes
or is it necessary to use intrinsics or assembler? Can we analyze and improve
performance without well-developed profiling tools? Do standard optimization
techniques work? Are there specific RISC-V features that need to be considered?
These and other questions require careful consideration. In this paper, we
present our experience optimizing four BLAS algorithms for band matrix
operations on RISC-V processors. We demonstrate how RISC-V-optimized
implementations of OpenBLAS algorithms can be significantly accelerated through
improved vectorization of computationally intensive loops. Experiments on
Lichee Pi 4A and Banana Pi BPI-F3 devices using RVV 0.7.1 and RVV 1.0 vector
instruction sets respectively, show speedups of 1.5x to 10x depending on the
operation compared to the OpenBLAS baseline. In particular, the successful use
of vector register grouping with RVV can lead to significant performance
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decentralized Distributed Graph Coloring: Cluster Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Flin, Magnus M. Halldorsson, Alexandre Nolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph coloring is fundamental to distributed computing. We give the first
sub-logarithmic distributed algorithm for coloring cluster graphs. These graphs
are obtained from the underlying communication network by contracting nodes and
edges, and they appear frequently as components in the study of distributed
algorithms. In particular, we give a $O(\log^* n)$-round algorithm to
$(\Delta+1)$-color cluster graphs of at least polylogarithmic degree. The
previous best bound known was $\operatorname{poly}(\log n)$ [Flin et al.,
SODA'24]. This properly generalizes results in the CONGEST model and shows that
distributed graph problems can be solved quickly even when the node itself is
decentralized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>81 pages, accepted to PODC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Resource Allocation and Energy Efficiency in Federated Fog
  Computing for IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00791v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00791v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Sarmad Shah, Anas Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fog computing significantly enhances the efficiency of IoT applications by
providing computation, storage, and networking resources at the edge of the
network. In this paper, we propose a federated fog computing framework designed
to optimize resource management, minimize latency, and reduce energy
consumption across distributed IoT environments. Our framework incorporates
predictive scheduling, energy-aware resource allocation, and adaptive mobility
management strategies. Experimental results obtained from extensive simulations
using the OMNeT++ environment demonstrate that our federated approach
outperforms traditional non-federated architectures in terms of resource
utilization, latency, energy efficiency, task execution time, and scalability.
These findings underline the suitability and effectiveness of the proposed
framework for supporting sustainable and high-performance IoT services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Resource-Constrained Federated Learning Systems with Guessed
  Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Yassine Boukhari, Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Othmane Safsafi, Rishi Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables a set of client devices to collaboratively
train a model without sharing raw data. This process, though, operates under
the constrained computation and communication resources of edge devices. These
constraints combined with systems heterogeneity force some participating
clients to perform fewer local updates than expected by the server, thus
slowing down convergence. Exhaustive tuning of hyperparameters in FL,
furthermore, can be resource-intensive, without which the convergence is
adversely affected. In this work, we propose GEL, the guess and learn
algorithm. GEL enables constrained edge devices to perform additional learning
through guessed updates on top of gradient-based steps. These guesses are
gradientless, i.e., participating clients leverage them for free. Our generic
guessing algorithm (i) can be flexibly combined with several state-of-the-art
algorithms including FEDPROX, FEDNOVA, FEDYOGI or SCALEFL; and (ii) achieves
significantly improved performance when the learning rates are not best tuned.
We conduct extensive experiments and show that GEL can boost empirical
convergence by up to 40% in resource constrained networks while relieving the
need for exhaustive learning rate tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decouple and Decompose: Scaling Resource Allocation with DeDe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiying Xu, Minlan Yu, Francis Y. Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient resource allocation is essential in cloud systems to facilitate
resource sharing among tenants. However, the growing scale of these
optimization problems have outpaced commercial solvers commonly employed in
production. To accelerate resource allocation, prior approaches either
customize solutions for narrow domains or impose workload-specific assumptions.
In this work, we revisit real-world resource allocation problems and uncover a
common underlying structure: the vast majority of these problems are inherently
separable, i.e., they optimize the aggregate utility of individual resource and
demand allocations, under separate constraints for each resource and each
demand. Building on this observation, we develop DeDe, a scalable and
theoretically rooted optimization framework for large-scale resource
allocation. At the core of DeDe is a decouple-and-decompose approach: it
decouples entangled resource and demand constraints and thereby decomposes the
overall optimization into alternating per-resource and per-demand subproblems
that can be solved efficiently and in parallel. We have implemented and
released DeDe as a Python package with a familiar modeling interface. Our
experiments on three representative resource allocation tasks -- cluster
scheduling, traffic engineering, and load balancing -- demonstrate that DeDe
delivers significant speedups while generating higher-quality allocations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge
  Using Analytical Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.09061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.09061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alyssa Pinnock, Shakya Jayakody, Kawsher A Roxy, Md Rubel Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 7 pages, IEEE conference template</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by USENIX ATC'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-06-13T00:00:00Z">2025-06-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Databases <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schema-R1: A reasoning training approach for schema linking in
  Text-to-SQL Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuzhenghong Wen, Su Pan, yuwei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema linking is a critical step in Text-to-SQL task, aiming to accurately
predict the table names and column names required for the SQL query based on
the given question. However, current fine-tuning approaches for schema linking
models employ a rote-learning paradigm, excessively optimizing for ground truth
schema linking outcomes while compromising reasoning ability. This limitation
arises because of the difficulty in acquiring a high-quality reasoning sample
for downstream tasks. To address this, we propose Schema-R1, a reasoning schema
linking model trained using reinforcement learning. Specifically, Schema-R1
consists of three key steps: constructing small batches of high-quality
reasoning samples, supervised fine-tuning for cold-start initialization, and
rule-based reinforcement learning training. The final results demonstrate that
our method effectively enhances the reasoning ability of the schema linking
model, achieving a 10\% improvement in filter accuracy compared to the existing
method. Our code is available at https://github.com/hongWin/Schema-R1/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-based Dynamic Differential Testing for Database Connectors with
  Reinforcement Learning-Guided Prompt Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Lyu, Minghao Zhao, Yanhao Wang, Liang Jie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database connectors are critical components enabling applications to interact
with underlying database management systems (DBMS), yet their security
vulnerabilities often remain overlooked. Unlike traditional software defects,
connector vulnerabilities exhibit subtle behavioral patterns and are inherently
challenging to detect. Besides, nonstandardized implementation of connectors
leaves potential risks (a.k.a. unsafe implementations) but is more elusive. As
a result, traditional fuzzing methods are incapable of finding such
vulnerabilities. Even for LLM-enable test case generation, due to a lack of
domain knowledge, they are also incapable of generating test cases that invoke
all interface and internal logic of connectors. In this paper, we propose
reinforcement learning (RL)-guided LLM test-case generation for database
connector testing. Specifically, to equip the LLM with sufficient and
appropriate domain knowledge, a parameterized prompt template is composed which
can be utilized to generate numerous prompts. Test cases are generated via LLM
with a prompt, and are dynamically evaluated through differential testing
across multiple connectors. The testing is iteratively conducted, with each
round RL is adopted to select optimal prompt based on prior-round behavioral
feedback, so as to maximize control flow coverage. We implement aforementioned
methodology in a practical tool and evaluate it on two widely used JDBC
connectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported
16 bugs, among them 10 are officially confirmed and the rest are acknowledged
as unsafe implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCPQ: Object-Centric Process Querying & Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Küsters, Wil M. P. van der Aalst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process querying is used to extract information and insights from process
execution data. Similarly, process constraints can be checked against input
data, yielding information on which process instances violate them.
Traditionally, such process mining techniques use case-centric event data as
input. However, with the uptake of Object-Centric Process Mining (OCPM),
existing querying and constraint checking techniques are no longer applicable.
Object-Centric Event Data (OCED) removes the requirement to pick a single case
notion (i.e., requiring that events belong to exactly one case) and can thus
represent many real-life processes much more accurately. In this paper, we
present a novel highly-expressive approach for object-centric process querying,
called OCPQ. It supports a wide variety of applications, including OCED-based
constraint checking and filtering. The visual representation of nested queries
in OCPQ allows users to intuitively read and create queries and constraints. We
implemented our approach using (1) a high-performance execution engine backend
and (2) an easy-to-use editor frontend. Additionally, we evaluated our approach
on a real-life dataset, showing the lack in expressiveness of prior work and
runtime performance significantly better than the general querying solutions
SQLite and Neo4j, as well as comparable to the performance-focused DuckDB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conjunctive Queries with Free Access Patterns under Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09032v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09032v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Kara, Milos Nikolic, Dan Olteanu, Haozhe Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of answering conjunctive queries with free access
patterns (CQAPs) under updates. A free access pattern is a partition of the
free variables of the query into input and output. The query returns tuples
over the output variables given a tuple of values over the input variables.
  We introduce a fully dynamic evaluation approach that works for all CQAPs and
is optimal for two classes of CQAPs. This approach recovers prior work on the
dynamic evaluation of conjunctive queries without access patterns.
  We first give a syntactic characterisation of all CQAPs that admit constant
time per single-tuple update and whose output tuples can be enumerated with
constant delay given a tuple of values over the input variables.
  We further chart the complexity trade-off between the preprocessing time,
update time and enumeration delay for a class of CQAPs. For some of these
CQAPs, our approach achieves optimal, albeit non-constant, update time and
delay. This optimality is predicated on the Online Matrix-Vector Multiplication
conjecture.
  We finally adapt our approach to the dynamic evaluation of tractable CQAPs
over probabilistic databases under updates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Algorithms for Fair Max-Min Diversification in $\mathbb{R}^d$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Kurkure, Miles Shamo, Joseph Wiseman, Sainyam Galhotra, Stavros Sintos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of extracting a diverse subset from a dataset, often referred to as
maximum diversification, plays a pivotal role in various real-world
applications that have far-reaching consequences. In this work, we delve into
the realm of fairness-aware data subset selection, specifically focusing on the
problem of selecting a diverse set of size $k$ from a large collection of $n$
data points (FairDiv).
  The FairDiv problem is well-studied in the data management and theory
community. In this work, we develop the first constant approximation algorithm
for FairDiv that runs in near-linear time using only linear space. In contrast,
all previously known constant approximation algorithms run in super-linear time
(with respect to $n$ or $k$) and use super-linear space. Our approach achieves
this efficiency by employing a novel combination of the Multiplicative Weight
Update method and advanced geometric data structures to implicitly and
approximately solve a linear program. Furthermore, we improve the efficiency of
our techniques by constructing a coreset. Using our coreset, we also propose
the first efficient streaming algorithm for the FairDiv problem whose
efficiency does not depend on the distribution of data points. Empirical
evaluation on million-sized datasets demonstrates that our algorithm achieves
the best diversity within a minute. All prior techniques are either highly
inefficient or do not generate a good solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable
  with DBOS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.10886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.10886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Vasquez-Grinnell, Alex Poliakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To meet the needs of a large pharmaceutical organization, we set out to
create S3Mirror - an application for transferring large genomic sequencing
datasets between S3 buckets quickly, reliably, and observably. We used the DBOS
Transact durable execution framework to achieve these goals and benchmarked the
performance and cost of the application. S3Mirror is an open source DBOS Python
application that can run in a variety of environments, including DBOS Cloud
Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the
cost. Moreover, S3Mirror is resilient to failures and allows for real-time
filewise observability of ongoing and past transfers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with
  Heterogeneous LoRA Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Zhang, Ping Liu, Jiahao Xu, Rui Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning has recently been utilized to collaboratively fine-tune
foundation models across multiple clients. Notably, federated low-rank
adaptation LoRA-based fine-tuning methods have recently gained attention, which
allows clients to fine-tune FMs with a small portion of trainable parameters
locally. However, most existing methods do not account for the heterogeneous
resources of clients or lack an effective local training strategy to maximize
global fine-tuning performance under limited resources. In this work, we
propose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that
enables clients to collaboratively fine-tune an FM with different local
trainable LoRA layers. To ensure its effectiveness, we develop several
heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local
trainable LoRA layers based on clients' resource capabilities and the layer
importance. Specifically, based on the dynamic layer importance, we design a
Fisher Information Matrix score-based HLA that leverages dynamic gradient norm
information. To better stabilize the training process, we consider the
intrinsic importance of LoRA layers and design a Geometrically-Defined HLA
strategy. It shapes the collective distribution of trainable LoRA layers into
specific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck,
and Uniform. Moreover, we extend GD-HLA into a randomized version, named
Randomized Geometrically-Defined HLA, for enhanced model accuracy with
randomness. By co-designing the proposed HLA strategies, we incorporate both
the dynamic and intrinsic layer importance into the design of our HLA strategy.
We evaluate our approach on five datasets under diverse federated LoRA
fine-tuning settings, covering three levels of data distribution from IID to
extreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both
effective and efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TNNLS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure API-Driven Research Automation to Accelerate Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler J. Skluzacek, Paul Bryant, A. J. Ruckman, Daniel Rosendo, Suzanne Prentice, Michael J. Brim, Ryan Adamson, Sarp Oral, Mallikarjun Shankar, Rafael Ferreira da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Secure Scientific Service Mesh (S3M) provides API-driven infrastructure
to accelerate scientific discovery through automated research workflows. By
integrating near real-time streaming capabilities, intelligent workflow
orchestration, and fine-grained authorization within a service mesh
architecture, S3M revolutionizes programmatic access to high performance
computing (HPC) while maintaining uncompromising security. This framework
allows intelligent agents and experimental facilities to dynamically provision
resources and execute complex workflows, accelerating experimental lifecycles,
and unlocking the full potential of AI-augmented autonomous science. S3M
signals a new era in scientific computing infrastructure that eliminates
traditional barriers between researchers, computational resources, and
experimental facilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PEARC 2025, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A retrospective on DISPEED -- Leveraging heterogeneity in a drone swarm
  for IDS execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Lannurien, Camélia Slimani, Louis Morge-Rollet, Laurent Lemarchand, David Espes, Frédéric Le Roy, Jalil Boukhobza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarms of drones are gaining more and more autonomy and efficiency during
their missions. However, security threats can disrupt their missions'
progression. To overcome this problem, Network Intrusion Detection Systems
((N)IDS) are promising solutions to detect malicious behavior on network
traffic. However, modern NIDS rely on resource-hungry machine learning
techniques, that can be difficult to deploy on a swarm of drones. The goal of
the DISPEED project is to leverage the heterogeneity (execution platforms,
memory) of the drones composing a swarm to deploy NIDS. It is decomposed in two
phases: (1) a characterization phase that consists in characterizing various
IDS implementations on diverse embedded platforms, and (2) an IDS
implementation mapping phase that seeks to develop selection strategies to
choose the most relevant NIDS depending on the context. On the one hand, the
characterization phase allowed us to identify 36 relevant IDS implementations
on three different embedded platforms: a Raspberry Pi 4B, a Jetson Xavier, and
a Pynq-Z2. On the other hand, the IDS implementation mapping phase allowed us
to design both standalone and distributed strategies to choose the best NIDSs
to deploy depending on the context. The results of the project have led to
three publications in international conferences, and one publication in a
journal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bounded Memory in Distributed Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Ben Basat, Keren Censor-Hillel, Yi-Jun Chang, Wenchen Han, Dean Leitersdorf, Gregory Schwartzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advent of programmable switches makes distributed algorithms
readily deployable in real-world datacenter networks. However, there are still
gaps between theory and practice that prevent the smooth adaptation of CONGEST
algorithms to these environments. In this paper, we focus on the memory
restrictions that arise in real-world deployments. We introduce the
$\mu$-CONGEST model where on top of the bandwidth restriction, the memory of
nodes is also limited to $\mu$ words, in line with real-world systems. We
provide fast algorithms of two main flavors.
  First, we observe that many algorithms in the CONGEST model are
memory-intensive and do not work in $\mu$-CONGEST. A prime example of a family
of algorithms that use large memory is clique-listing algorithms. We show that
the memory issue that arises here cannot be resolved without incurring a cost
in the round complexity, by establishing a lower bound on the round complexity
of listing cliques in $\mu$-CONGEST. We introduce novel techniques to overcome
these issues and generalize the algorithms to work within a given memory bound.
Combined with our lower bound, these provide tight tradeoffs between the
running time and memory of nodes.
  Second, we show that it is possible to efficiently simulate various families
of streaming algorithms in $\mu$-CONGEST. These include fast simulations of
$p$-pass algorithms, random order streams, and various types of mergeable
streaming algorithms.
  Combining our contributions, we show that we can use streaming algorithms to
efficiently generate statistics regarding combinatorial structures in the
network. An example of an end result of this type is that we can efficiently
identify and provide the per-color frequencies of the frequent monochromatic
triangles in $\mu$-CONGEST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The 37th ACM Symposium on Parallelism in Algorithms and
  Architectures (SPAA '25). 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capsule: Efficient Player Isolation for Datacenters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouheng Du, Nima Davari, Li Li, Nodir Kodirov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud gaming is increasingly popular. A challenge for cloud provider is to
keep datacenter utilization high: a non-trivial task due to application
variety. These applications come in different shapes and sizes. So do cloud
datacenter resources, e.g., CPUs, GPUs, NPUs.
  Part of the challenge stems from game engines being predominantly designed to
run only one player. One player in a lightweight game might utilize only a
fraction of the cloud server GPU. The remaining GPU capacity will be left
underutilized, an undesired outcome for the cloud provider. We introduce
Capsule, a mechanism that allows multiple players to seamlessly share one GPU.
  We implemented Capsule in O3DE, a popular open source game engine. Our
evaluations show that Capsule can increase datacenter resource utilization by
accommodating up to 2.25x more players, without degrading player gaming
experience. Capsule is also application agnostic. We ran four applications on
Capsule-based O3DE with no application changes. Our experiences show that
Capsule design can be adopted by other game engines to increase datacenter
utilization across cloud providers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-Aware Virtualization over Inter-Core Connected Neural
  Processing Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahu Feng, Erhu Feng, Dong Du, Pinjie Xu, Yubin Xia, Haibo Chen, Rong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of artificial intelligence (AI) applications, an
emerging class of AI accelerators, termed Inter-core Connected Neural
Processing Units (NPU), has been adopted in both cloud and edge computing
environments, like Graphcore IPU, Tenstorrent, etc. Despite their innovative
design, these NPUs often demand substantial hardware resources, leading to
suboptimal resource utilization due to the imbalance of hardware requirements
across various tasks. To address this issue, prior research has explored
virtualization techniques for monolithic NPUs, but has neglected inter-core
connected NPUs with the hardware topology.
  This paper introduces vNPU, the first comprehensive virtualization design for
inter-core connected NPUs, integrating three novel techniques: (1) NPU route
virtualization, which redirects instruction and data flow from virtual NPU
cores to physical ones, creating a virtual topology; (2) NPU memory
virtualization, designed to minimize translation stalls for SRAM-centric and
NoC-equipped NPU cores, thereby maximizing the memory bandwidth; and (3)
Best-effort topology mapping, which determines the optimal mapping from all
candidate virtual topologies, balancing resource utilization with end-to-end
performance. We have developed a prototype of vNPU on both an FPGA platform
(Chipyard+FireSim) and a simulator (DCRA). Evaluation results indicate that,
compared to other virtualization approaches such as unified virtual memory and
MIG, vNPU achieves up to a 2x performance improvement across various ML models,
with only 2% hardware cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MindFlayer SGD: Efficient Parallel SGD in the Presence of Heterogeneous
  and Random Worker Compute Times 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artavazd Maranjyan, Omar Shaikh Omar, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of minimizing the expectation of smooth nonconvex
functions in a distributed setting with multiple parallel workers that are able
to compute stochastic gradients. A significant challenge in this context is the
presence of arbitrarily heterogeneous and stochastic compute times among
workers, which can severely degrade the performance of existing parallel
stochastic gradient descent (SGD) methods. While some parallel SGD algorithms
achieve optimal performance under deterministic but heterogeneous delays, their
effectiveness diminishes when compute times are random - a scenario not
explicitly addressed in their design. To bridge this gap, we introduce
MindFlayer SGD, a novel parallel SGD method specifically designed to handle
stochastic and heterogeneous compute times. Through theoretical analysis and
empirical evaluation, we demonstrate that MindFlayer SGD consistently
outperforms existing baselines, particularly in environments with heavy-tailed
noise. Our results highlight its robustness and scalability, making it a
compelling choice for large-scale distributed learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyijian Liu, Jun Li, Wu-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high memory and computation demand of large language models (LLMs) makes
them challenging to be deployed on consumer devices due to limited GPU memory.
Offloading can mitigate the memory constraint but often suffers from low GPU
utilization, leading to low inference efficiency. In this work, we propose a
novel framework, called pipelined offloading (PIPO), for efficient inference on
consumer devices. PIPO designs a fine-grained offloading pipeline, complemented
with optimized data transfer and computation, to achieve high concurrency and
efficient scheduling for inference. Experimental results show that compared
with state-of-the-art baseline, PIPO increases GPU utilization from below 40%
to over 90% and achieves up to 3.1$\times$ higher throughput, running on a
laptop equipped with a RTX3060 GPU of 6GB memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based
  Proof-of-Stake Blockchains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeta Avarikioti, Eleftherios Kokoris Kogias, Ray Neiheiser, Christos Stefo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a formal analysis of quorum-based State Machine Replication (SMR)
protocols in Proof-of-Stake (PoS) systems under a hybrid threat model
comprising honest, Byzantine, and rational validators. Our analysis of
traditional quorum-based protocols establishes two fundamental impossibility
results: (1) in partially synchronous networks, no quorum-based protocol can
achieve SMR when rational and Byzantine validators comprise more than $1/3$ of
participants, and (2) in synchronous networks, SMR remains impossible when
rational and Byzantine validators comprise $2/3$ or more of participants.
  To overcome these limitations, we propose two complementary solutions in our
hybrid model. First, we introduce a protocol that enforces a bound on the
volume of the total transacted amount that is finalized within any time window
$\Delta$ and prove that this bound is necessary for secure SMR protocols in our
model. Second, we present the \emph{strongest chain rule}, which enables
efficient finalization of transactions when the majority of honest participants
provably support the SMR execution. Through empirical analysis of Ethereum and
Cosmos networks, we demonstrate that validator participation consistently
exceeds the required ${5}/{6}$ threshold, establishing the practical
feasibility of our solution in production PoS systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auctions with Tokens: Monetary Policy as a Mechanism Design Choice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13794v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13794v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Canidio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I study a repeated auction in which payments are made with a blockchain token
created and initially owned by the auction designer. Unlike the ``virtual
money'' previously examined in mechanism design, such tokens can be saved and
traded outside the mechanism. I show that the present-discounted value of
expected revenues equals that of a conventional dollar auction, but revenues
accrue earlier and are less volatile. The optimal monetary policy burns the
tokens used for payment, a practice common in blockchain-based protocols. I
also show that the same outcome can be reproduced in a dollar auction if the
auctioneer issues a suitable dollar-denominated security. This equivalence
breaks down with moral hazard and contracting frictions: with severe
contracting frictions the token auction dominates, whereas with mild
contracting frictions the dollar auction combined with a dollar-denominated
financial instrument is preferred.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Mechanism design, Auctions, Blockchain, Cryptocurrencies, Tokens,
  Private Money</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Level set-based inverse homogenisation of three-dimensional
  piezoelectric materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03148v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03148v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary J. Wegert, Anthony P. Roberts, Vivien J. Challis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we use memory-distributed level set-based topology optimisation
to design three-dimensional periodic piezoelectric materials with enhanced
properties. We compare and assess several existing iterative solvers with
respect to their weak scalability and find that an approximate Schur complement
preconditioned generalized minimal residual method method demonstrates the best
performance and scalability for solving the piezoelectric homogenisation
equations. We use the developed techniques to computationally design
high-resolution piezoelectric metamaterials with enhanced stiffness and
piezoelectric properties that yield new insights into material design for
sensor, hydrophone, and actuator applications. We suggest two robust structures
with no fine-scale features that exhibit enhanced piezoelectric properties
several times larger than those of the base material. We find that level
set-based topology optimisation is well suited to problems involving
piezoelectricity and has the advantage of avoiding large regions of
intermediate density material. Our memory-distributed level-set implementation
is open source and provided for practitioners in the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Hybrid Defense for Byzantine Attacks in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables multiple clients to collaboratively train a
global model without sharing their local data. Recent studies have highlighted
the vulnerability of FL to Byzantine attacks, where malicious clients send
poisoned updates to degrade model performance. In particular, many attacks have
been developed targeting specific aggregation rules, whereas various defense
mechanisms have been designed for dedicated threat models. This paper studies
the resilience of attack-agnostic FL scenarios, where the server lacks prior
knowledge of both the attackers' strategies and the number of malicious clients
involved. We first introduce hybrid defenses against state-of-the-art attacks.
Our goal is to identify a general-purpose aggregation rule that performs well
on average while also avoiding worst-case vulnerabilities. By adaptively
selecting from available defenses, we demonstrate that the server remains
robust even when confronted with a substantial proportion of poisoned updates.
We also emphasize that existing FL defenses should not automatically be
regarded as secure, as demonstrated by the newly proposed Trapsetter attack.
The proposed attack outperforms other state-of-the-art attacks by further
increasing the impact of the attack by 5-15%. Our findings highlight the
ongoing need for the development of Byzantine-resilient aggregation algorithms
in FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You can lie but not deny: SWMR registers with signature properties in
  systems with Byzantine processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09805v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09805v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Hu, Sam Toueg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We define and show how to implement SWMR registers that provide properties of
unforgeable digital signatures - without actually using such signatures - in
systems with Byzantine processes. Intuitively, processes can use these
registers to write values as if they are ``signed'', such that these ``signed
values'' can be ``verified'' by any process and ``relayed'' to any process. All
our register implementations are from SWMR registers, and they work in systems
with $n > 3f$ processes, $f$ of which can be Byzantine. We show that these
implementations are optimal in the number of Byzantine processes they can
tolerate: more precisely, we prove that if $3 \le n \le 3f$, the registers that
we propose cannot be implemented from SWMR registers without using signatures.
The registers that we introduce in this paper can also be implemented without
signatures in message-passing systems with $n > 3f$ processes, $f$ of which can
be Byzantine: this is because SWMR registers can be implemented in such systems
(Most\'efaoui, Petrolia, Raynal, and Jard 2017).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WindVE: Collaborative CPU-NPU Vector Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.14941v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.14941v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Huang, Xuebing Yu, Yi Xiong, Wenjie Huang, Entong Li, Li Zeng, Xin chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation is a technology that enhances large language
models by integrating information retrieval. In the industry, inference
services based on LLMs are highly sensitive to cost-performance ratio,
prompting the need for improving hardware resource utilization in the inference
service. Specifically, vector embedding and retrieval processes take up to 20%
of the total latency. Therefore, optimizing the utilization of computational
resources in vector embeddings is crucial for enhancing the cost-performance
ratio of inference processes, which in turn boosts their product
competitiveness.In this paper, we analyze the deployment costs of vector
embedding technology in inference services, propose a theoretical formula, and
determine through the mathematical expression that increasing the capacity to
process concurrent queries is the key to reducing the deployment costs of
vector embeddings. Therefore, in this paper, we focus on improving the
product's capability to process concurrent queries. To optimize concurrency
without sacrificing performance, we have designed a queue manager that adeptly
offloads CPU peak queries. This manager utilizes a linear regression model to
ascertain the optimal queue depths, a critical parameter that significantly
influences the efficacy of the system. We further develop a system named WindVE
that uses a CPU-NPU heterogeneous architecture to offload peak concurrent
queries, which leverages the performance differences between the two processors
to effectively manage traffic surges. Through experiments, we compare WindVE to
the state-of-the-art vector embedding framework FlagEmbedding, and achieve a
concurrency level up to 22.3% higher than the scheme without offloading.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-06-21T05:30:55.920903998Z">
            2025-06-21 05:30:55 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
